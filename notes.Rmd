---
title: "Class NOTES"
date: "September 1, 2020"
output: github_document

---
# Class 1

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```
## Introduction

- Major
- Prior experiences with stats/computing/data analysis
- Any specific plan to conduct data analysis for RA/senior thesis, etc.

# Motivation

## Data Science

- **Data science**: Using quantitative data to understand the social world  
  + Data collection (e.g., public records, surveys, texts)
  + Data cleaning
  + Statistical modeling
  + Presentation (visualization)
  

## Why take this course?

- You will develop various skills necessary to extract statistical patterns from quantitative data

- Quantitative skills will allow you to answer important questions better
    + Will help you with your final projects or senior thesis project, etc.
    + Will give you a critical eye to evaluate the quality of scientific evidence

- These skills are becoming standard in many industries, academic disciplines, etc. 


# Course Details

## Learning approach: practice, practice!

- Impossible to learn statistical methods without analyzing dataset on your own **over and over again**

- Typical class will consist of...
  + Brief lecture to introduce a concept (e.g., multiple regression)
  + Demonstrate how to apply the concept in practice with actual dataset 
  + In-class exercises
- I will ask you to submit your code at the end of class
  + Check if everyone is following
  + Due before next class
  + Graded on submission (participation score)


  
## Final project (new experiment)

- Enough already with "proposals"
- We will run an **actual** original experiment as part of class
  + Topic: TBD 
  + Will recruit 500-700 participants from Amazon's Mechanical Turk (online labor market)

- You will write a short research note reporting the findings of the experiment


## Computing

- We will use R to analyze data

- R
  + Extremely popular programming language in the data science world
  + Huge plus to your resume
  + Large online community
    + Googling will give you an answer most of the time
  + **FREE**


- How to get a good participation score
  + Submit in-class exercise codes (i.e., come to class) 
  + Do minor homeworks
  + Ask and answer questions during class
  + Be active in the Canvans discussion board

# Install R and RStudio

## Download 

- R: https://cloud.r-project.org/
- RStudio: https://rstudio.com/products/rstudio/download/


--- 

# Class 2


## Today's Agenda

- Introduction to R (Imai 1.3)

# Intro to R

## Open R


## Objects

- Information or Data stored in memory
- Shown in the "Environment" window in RStudio
- Create them with the assignment operator `<-`
```{r}
course<-290 # You can put comments in R script after # sign
course # R will ignore anything that comes after '#'
```


## numeric object
```{r}
corona.sep1<-44027 # define a numeric object for NYC's population
corona.sep2<-40757 # define a numeric object for Philadelphia's population 
corona.sep2+corona.sep1 # sum 
corona.sep2-corona.sep1 # difference
corona.sep2/corona.sep1 # ratio 
```

## character object
```{r}
myname<-"Jin"
myname
```
- The `class()` function will tell you the class of your object
```{r}
class(myname)
class(corona.sep1)
```

# LeBron James' Stats

## LeBron James since 2011
Season |Games Played| Points Scored|Team
-------|-----|------|--------------------
2010-11|	79 |  3063|Miami
2011-12|	62 |	1683|Miami
2012-13|	76 |	2036|Miami
2013-14|	77 |	2089|Miami
2014-15|	69 |	1743|Cleveland
2015-16|	76 |	1920|Cleveland
2016-17|	74 |	1954|Cleveland
2017-18|	82 |	2251|Cleveland
2018-19|	55 |	1505|LA
2019-20|	67 |	1698|LA

## Vector
- Sequence of information stored in a specific order
- Use the `c()` function (concatenate) to create one

```{r}
# How many games LeBron played each season between 2011 and 2020
lebron.gp <- c(79,62,76,77,69,76,74,82,55,67) 
lebron.gp
```
- Combine multiple vectors
```{r}
lebron.mia <- c(79,62,76,77)
lebron.cle <- c(69,76,74,82)
lebron.lal <- c(55,67)
lebron.gp<-c(lebron.mia,lebron.cle,lebron.lal)
lebron.gp
```

## Indexing: Pull specific elements from a vector

```{r}
lebron.gp[2] # how many games LeBron played in 2012
lebron.gp[c(1,3)] # how many games LeBron played in 2011 and 2013
lebron.gp[1:3] # how many games LeBron played in 2011 through 2013
lebron.gp[2]-lebron.gp[1] # calculate change in games LeBron played each season from 2011 to 2012
lebron.gp[-10] # omit the current season
```

## Arithmetic operations
- Apply arithmetic operations to all elements in a vector
```{r}
lebron.gp - 10
lebron.gp/2

```


## Functions to summarize LeBron's GP
```{r}
min(lebron.gp)
max(lebron.gp)
range(lebron.gp)
summary(lebron.gp)
```


## Functions
```{r}
length(lebron.gp)
sum(lebron.gp)
sum(lebron.gp)/length(lebron.gp)
mean(lebron.gp)
mean(lebron.gp[-10]) # omit the current season
```



# Loading a dataset

##
- Create a working directory
- Save your data file in the directory
- Set working directory in R

```{r}
#setwd ("~/desktop/comm 290")
getwd ()
```
- Load the file using the `read.csv()` function
```{r}
lebron<-read.csv("lebron.csv")
```
- Display the data by clicking on the data in the Environment window


## Inspecting a dataset (or "data frame")

- rows: observations
- columns: variables

```{r}
names(lebron)
nrow(lebron)
ncol(lebron)
```

## Inspecting a data file

```{r}
dim(lebron)
head(lebron)
```


## Inspecting a data file

```{r}
summary(lebron)
```

## Accessing parts of the data


- Access rows and columns by specifying row/column in square brackets `[rows, columns]`
- Rows/Columns unspecified $\rightsquigarrow$ Return everything

```{r}
lebron[1,] # Display LeBron's records in his rookie season
lebron[c(1,3,5),] # Display LeBron's records in 2004, 2006, 2008
```

## Accessing parts of the data

- Access rows and columns by specifying row/column in square brackets [rows, columns]
- Rows/Columns unspecified $\rightsquigarrow$ Return everything

```{r}
lebron[,3] # Display LeBron's games played throughout his NBA career
```

## LeBron's points per game and rebounds per game in each year
- use the `$` operator to access a variable in a data
```{r}
lebron$G # GPs in each season
lebron$PTS # total points scored in each season
lebron$ORB+lebron$DRB # total rebounds grabbed in each season
```


## LeBron's points per game and rebounds per game in each year
- use the $ operator to access a variable in a data
```{r}
lebron$PTS/lebron$G # points per game
(lebron$ORB+lebron$DRB)/lebron$G # rebounds per game
```


## LeBron's career points per game and shooting percentage

```{r}
sum(lebron$PTS)
sum(lebron$G)
sum(lebron$PTS)/sum(lebron$G)
sum(lebron$FG)/sum(lebron$FGA)
```


# In-class Exercises


## Michael Jordan vs. LeBron James

- Download and load mj.csv into your environment
- Calculate Michael Jordan's career points per game and compare it to LeBron's
- Calculate Michael Jordan's 3-pt shooting percentage and compare it to LeBron's
- Omit the last two seasons from Jordan's career and calculate points per game during Jordan's career with the Bulls


## Michael Jordan vs. LeBron James
```{r}
mj<-read.csv("mj.csv")
sum(mj$PTS)/sum(mj$G)
sum(mj$Three)/sum(mj$ThreeA)
sum(mj$PTS[1:13])/sum(mj$G[1:13])
```

## COMM290 Students

- Download and load comm290.csv into your environment
- Provide the number of variables and number of observations
- Provide the range and mean of the number of stat courses that COMM 290 students have taken


---
# Class 3


## Today's Agenda

- A brief look at the trust data (Assignment 1)
- Continue to work with the LeBron data
- In-class exercises
- Introduction to RMarkdown

## Data

-  Set working directory 
    - Mac: `setwd ("~/desktop/comm 290")`
    - Windows: `setwd ("c:/comm 290")`
- you probably don't need to do this because in last class you set your class folder as your permanent WD 
  - but it's good idea to start your R scrip by setting the right WD, especially if you're switching between different projects
```{r}
#setwd ("~/desktop/comm 290")
trust<-read.csv("trust.csv")
lebron<-read.csv("lebron.csv")
mj<-read.csv("mj.csv")
```

# Homework data

## Trust in government from ANES
```{r}
head(trust)
```

# Where we left off: Lebron's career


## Accessing parts of the data


- Access rows and columns by specifying row/column in square brackets `[rows, columns]`
- Rows/Columns unspecified $\rightsquigarrow$ Return everything
```{r results='hide'}
lebron
```

```{r}
lebron[1,] # Display LeBron's records in his rookie season
lebron[c(1,3,5),] # Display LeBron's records in 2004, 2006, 2008
```

## Accessing parts of the data

- Access rows and columns by specifying row/column in square brackets [rows, columns]
- Rows/Columns unspecified $\rightsquigarrow$ Return everything

```{r}
lebron[,3] # Display LeBron's games played throughout his NBA career
```

## LeBron's points per game and rebounds per game in each year
- use the `$` operator to access a variable in a data
```{r}
lebron$G # games in each season
lebron$PTS # total points scored in each season
lebron$ORB+lebron$DRB # total rebounds grabbed in each season
```


## LeBron's points per game and rebounds per game in each year
- use the $ operator to access a variable in a data
```{r}
lebron$PTS/lebron$G # points per game
(lebron$ORB+lebron$DRB)/lebron$G # rebounds per game
```


## LeBron's career points per game and shooting percentage

```{r}
sum(lebron$PTS)
sum(lebron$G)
sum(lebron$PTS)/sum(lebron$G)
sum(lebron$FG)/sum(lebron$FGA)
```


# In-class Exercises

## COMM 290 data
- Provide the number of variables and number of observations
- Provide the range and mean of stat courses

## Michael Jordan vs. LeBron James

- Download and load mj.csv into your environment
- Calculate Michael Jordan's career points per game and compare it to LeBron's
- Calculate Michael Jordan's 3-pt shooting percentage and compare it to LeBron's
- Omit the last two seasons from Jordan's career and calculate points per game during Jordan's career with the Bulls


# RMarkdown

## What is a RMarkdown file?

- You will write your HW in a file called an â€œR markdownâ€ file.
- Mix of text answers and your code to analyze data/produce graph.
- I write my slides in R markdown and have been posting the .rmd files with the slides.
- Resources
  + Markdown reference in RStudio
  + Cheat sheet on Canvas

## Quick demo

![](Screen Shot 2020-01-23 at 12.34.22 AM.png){width=500px}

## Quick demo

![](Screen Shot 2020-01-23 at 12.34.45 AM.png){width=500px}

## Quick demo

- Output options: Word, HTML, PDF
  - For homework, use Word
  - Submit both rmd and word documents
  - FYI: To create a PDF output file directly from R, install MacTex (Mac OS X users) or TikTek (Windows users)

- Tips
  - Put both your data and rmd file in your WD before knitting
  - Don't use `View()` function
  - Your HW isn't done until you successfully knit your rmd file
    - Many knitting issues at the last minute resulting in late submissions!

---
# Class 4

## Today's Agenda

### Where are we?

- What we have been doing
  + Create new objects
  + Load new dataset into R
  + Apply basic functions to variables in data

- Assignment 1 due in one week


### Where are we going?

- Finalize introduction to R/Rstudio/RMarkdown
- Brief intro to causality 
- Racial discrimination experiment
- Why correlation isn't causation (and when correlation *is* causation)
- In-class exercise: MMR Vaccine Promotion Experiment

## Datasets
- Download and save the following files in your WD
```{r}
resume<-read.csv("resume.csv")
mmr<-read.csv("mmr.csv")
```


# R/Rstudio/RMarkdown

## RMarkdown

- Output options: Word, HTML, PDF
  - For homework, use Word
  - Submit both rmd and word documents
  - FYI: To create a PDF output file directly from R, install MacTex (Mac OS X users) or TikTek (Windows users)

- Tips
  - Put both your data and rmd file in your WD before knitting
  - Don't use `View()` function
  - Your HW isn't done until you successfully knit your rmd file
    - Many knitting issues at the last minute resulting in late submissions!

## Open the .rmd demo file

## R files

- R script file (`.R`) 
  + Plain text file with R codes
- Markdown file (`.rmd`)
  + Homework assignments
  + Mix of text answers and R codes that analyze data/display results within a document
  + Efficient tool to write a report

- R data file (`.RData`)
  + Saves workspace (collection of objects loaded in Global Environment)
  + We don't use it in this class
    - R can load any data regardless of their format (`.csv`, `.dat`, `.sav`, etc.)

# Causality

## Causality and Experimentation

### Correlation is not causation?

### Experiment vs. Non-Experiments?


# Racial Discrimination Experiment


## Background

- Does race affect oneâ€™s job prospect?
  + Correlation between race and job: non white people have higher unemployment rates, lower average salaries, etc.
  + Correlation isn't causation because other **confounding factors** (e.g., education) may explain the difference

- Marianne Bertrand and Sendhil Mullainathan (2004) â€œAre Emily and Greg more employable than Lakisha and Jamal? A ï¬eld experiment on labor market discrimination.â€ *American Economic Review*

## Study design

- Sent out fake resumes to potential employers
- Randomized the names of job applicants 
  + African-American-sounding or White-sounding names
  + E.g., Jamal Jones or Greg Baker
- Leaving other information unchanged (all confounding factors are held constant)
- Compared the callback rates

## First look at the resume data

```{r}
resume <- read.csv("resume.csv")
dim(resume)
head(resume)
summary(resume)
```

## Inspect variables using table() function

- Frequency table
```{r}
table(resume$call)
prop.table(table(resume$call))
table(resume$race)
```

## Inspect variables using table() function

- Crosstab
```{r}
table (resume$race, resume$call)
table (race = resume$race, callback = resume$call)
```


## Callback rates by race
- Subset the data by race
- Use the `subset()` function
    + `subset(DATA, CONDITION)`
- Use logical operators to specify conditions
  + `==`, `<`, `>=`, `<`, `<=`, `!=`, `&` (and), `|` (or)
```{r}
resume.b<-subset(resume,race=="black")
resume.w<-subset(resume,race=="white")
dim(resume.b)
dim(resume.w)
```

## Callback rates by race
```{r}
mean(resume.b$call)
mean(resume.w$call)
mean(resume.w$call)-mean(resume.b$call) 
```

- Applicatans with an African-American-sounding name were less likely to get a call back by 3.2 percentage points


## Callback rates by race
```{r}
mean(resume.b$call)/mean(resume.w$call) 
```

- The callback rate of black applicants was only 67% of the callback rate of white applicants
- Having an African-American-sounding name lowers your call back probability by 33%
- Huge effect

## Callback rates by race and gender
- use logical operator `&`
```{r}
resume.bf<-subset(resume,race=="black" & sex == "female")
resume.wf<-subset(resume,race=="white" & sex == "female")
resume.bm<-subset(resume,race=="black" & sex == "male")
resume.wm<-subset(resume,race=="white" & sex == "male")
```
## Callback rates by race and gender
```{r}
mean(resume.bf$call)
mean(resume.wf$call)
mean(resume.bm$call)
mean(resume.wm$call)
```
## Racial discrimination by gender
```{r}
mean(resume.bf$call)-mean(resume.wf$call)
mean(resume.bm$call)-mean(resume.wm$call)
```
- Racial discrimination doesn't appear to vary across gender

## White male versus other groups
- use logical operator `|` (or)
```{r}
resume.ot<-subset(resume, race=="black"|sex=="female")
mean(resume.ot$call)-mean(resume.wm$call)
```
  

## Factor variables
- Let's categorize applicants into race by gender types, and create a factor variable for it
- A factor variable is like a character variable, except that it has ordered levels
```{r}
resume$type<- NA # creating a new variable with missing  values (optional)
resume$type[resume$race == "black" & resume$sex == "female"] <- "BlackFemale"
resume$type[resume$race == "black" & resume$sex == "male"] <- "BlackMale" 
resume$type[resume$race == "white" & resume$sex == "female"] <- "WhiteFemale" 
resume$type[resume$race == "white" & resume$sex == "male"] <- "WhiteMale"
resume$type<-as.factor(resume$type)
table(resume$type)
```

## `tapply()` function
- Applies a function repeatedly to an object within each levels of a factor variable
- `tapply (OBJECT, LEVELS, FUNCTION)`

```{r}
tapply(resume$call, resume$type, mean) # apply the mean function to resume$call within each level of resume$type
round(tapply(resume$call, resume$type, mean), digits=2) # use the round() function to round up the numbers
```

- Let's reorder the type variable and recreate the table
```{r}
resume$type<-factor(resume$type, levels = c("WhiteMale", "WhiteFemale", "BlackMale", "BlackFemale"))
tapply(resume$call, resume$type, mean) 
```

## Callback rates by first name
```{r}
tapply(resume$call, resume$firstname, mean)
```

## Callback rates by first name
- Sort the table by callback rates instead of alphabet using the sort() function
```{r}
sort(tapply(resume$call, resume$firstname, mean))
```


---

![](Screen Shot 2020-01-22 at 1.41.54 PM.png){width=500px}


## Correlation is not causation

- People who go to museums live longer: correlation
- Going to museums increases life expectancy: causation
- People often conflate the two
- Different how? 
- Correlation = Causation + Selection Bias
  + Selection Bias: Any difference that would have existed irrespective of museum visits
  + People who can afford to go to museums are more likely to afford decent health care, to exercise, etc.
  + Correlation can arise even if there is **no** effect of museum visits on health

## When correlation *is* causation

- Experimental research
  + Randomly assign treatments to different observations
  + Then measure the outcome variables
  + Randomization &rarr; No systematic difference &rarr; No selection bias
- Difference between experimental vs. non-experimental research
  + Random assignment
- When there's no random assignment
  + Causal inference becomes *extremely* difficult
- When you read an article that makes causal claims without randomization
  + Be very skeptical
  + It's the author's job to convince you!
  + Correlation dressed up as causation: more common than you'd think

## Remember!

- Correlation is not causation when there's a selection bias
- Experiment solves the selection bias problem through randomization
- Rule of thumb
  + Not an experiment &rarr; no causal evidence
  + Unless proven otherwise

# Exercise: MMR Vaccine Experiment

## Background 
- Question: How to reduce vaccine misperceptions and increase vaccination rates?
- A nationally representative experiment with about 1700 parents conducted in 2011
- Parents were randomly assigned to receive 1 of 4 treatment messages or a "placebo" message
  1. Placebo: costs and benefits of bird feeding (Unrelated)
  2. Correction: No evidence that vaccination causes autism
  3. Disease Image: Pictures of a child who has each of the diseases 
  4. Disease Narrative: A narrative of an infant boy hospitalized because of measles
  5. Disease Risk: Explain the dangers of the diseases prevented by MMR vaccine
- Nyhan et al. (2014) Effective Messages in Vaccine Promotion: A Randomized Trial. *Pediatrics*

---

### Details

```{r}
mmr<-read.csv("mmr.csv") # download and save the mmr.csv file in your WD and open
```

- cond: experimental conditions
- prior: attitude toward vaccination measured **BEFORE** treatment
- autism: 1 = MMR vaccine doesn't cause autism (0 = else)
- intent: 1 = Very likely to give MMR vaccine to another child (0 = else)

### Exercise
1. Obtain the number of observations for each condition.
2. Compare people's perception about the link between MMR vaccine and autism across conditions. Which treatment(s) worked?
3. Compare people's intention to vaccinate their children across conditions. Which treatment(s) worked? Is there anything odd?
4. Subset the data by people's prior attitude toward vaccination. And repeat Question 3 for each group. Who drove the pattern observed in Question 2?


---

# Class 5


## Today's Agenda
- Causality and Experimental Research
- MMR Vaccine Experiment
- Social Pressure Experiment 
- Same-sex Marriage Experiment (probably next class)

## Datasets
```{r}
mmr<-read.csv("mmr.csv")
social<-read.csv("social.csv")
gay<-read.csv("gay.csv")
```

# Causality and Experimentation

## Correlation is not causation

![](Screen Shot 2020-01-22 at 1.41.54 PM.png){width=500px}

## Correlation is not causation


- People who go to museums live longer: correlation
- Going to museums increases life expectancy: causation
- People often conflate the two
- Different how? 
- Correlation = Causation + Selection Bias
  + Selection Bias: Any difference that would have existed irrespective of museum visits
  + People who can afford to go to museums are more likely to afford decent health care, to exercise, etc.
  + Correlation can arise even if there is **no** effect of museum visits on health

## When correlation *is* causation

- Experimental research
  + Randomly assign treatments to different observations
  + Then measure the outcome variables
  + Randomization &rarr; No systematic difference &rarr; No selection bias
- Difference between experimental vs. non-experimental research
  + Random assignment
- When there's no random assignment
  + Causal inference becomes *extremely* difficult
- When you read an article that makes causal claims without randomization
  + Be very skeptical
  + It's the author's job to convince you!
  + Correlation dressed up as causation: more common than you'd think


## Basics of understanding (analyzing) experiments

- What are the experimental conditions? 
- What is the outcome variable (i.e., "dependent variable)? 
- The most basic approach to analyzing experimental data
  - Compare means between control vs treatment groups on the outcome of interest
  - Does the treatment group's mean differ from the control group's mean in the expected direction?
  - Is that difference statistically and substantively meaningful?
    - We will deal with this question later in this course
    

# Exercise: MMR Vaccine Experiment

## Background 
- Question: How to reduce vaccine misperceptions and increase vaccination rates?
- A nationally representative experiment with about 1700 parents conducted in 2011
- Parents were randomly assigned to receive 1 of 4 treatment messages or a "placebo" message
  1. Placebo: costs and benefits of bird feeding (Unrelated)
  2. Correction: No evidence that vaccination causes autism
  3. Disease Image: Pictures of a child who has each of the diseases 
  4. Disease Narrative: A narrative of an infant boy hospitalized because of measles
  5. Disease Risk: Explain the dangers of the diseases prevented by MMR vaccine
- Nyhan et al. (2014) Effective Messages in Vaccine Promotion: A Randomized Trial. *Pediatrics*



## Data

```{r}
names(mmr)
```

- cond: experimental conditions
- prior: attitude toward vaccination measured **BEFORE** treatment
- autism: 1 = MMR vaccine doesn't cause autism (0 = else)
- intent: 1 = Very likely to give MMR vaccine to another child (0 = else)

## Exercise
1. Obtain the number of observations for each condition.
2. Compare people's perception about the link between MMR vaccine and autism across conditions. Which treatment(s) worked?
3. Compare people's intention to vaccinate their children across conditions. Which treatment(s) worked? Is there anything odd?
4. Subset the data by people's prior attitude toward vaccination. And repeat Question 3 for each group. Who drove the pattern observed in Question 2?

## Question 1
-  Question1: Obtain the number of observations for each condition.


## Question 2
- Question 2: Compare people's perception about the link between MMR vaccine and autism across conditions. Which treatment(s) worked?
```{r}
tapply(mmr$autism,mmr$cond,mean) # comparing proportions people holding accurate perceptions by condition
tapply(mmr$autism,mmr$cond,mean)[2]-tapply(mmr$autism,mmr$cond,mean)[1] # Effect of correction treatment
tapply(mmr$autism,mmr$cond,mean)-tapply(mmr$autism,mmr$cond,mean)[1] # Effect of four treatments
```

## Question 3

- Question 3: Compare people's intention to vaccinate their children across conditions. Which treatment(s) worked? Is there anything odd?

```{r}
tapply(mmr$intent,mmr$cond,mean) # comparing proportions people intending to vaccinate
tapply(mmr$intent,mmr$cond,mean)[2]-tapply(mmr$intent,mmr$cond,mean)[1] # Effect of correction treatment
tapply(mmr$intent,mmr$cond,mean)-tapply(mmr$intent,mmr$cond,mean)[1] # Effect of four treatments
```

## Question 4

- Question 4:  Subset the data by people's prior attitude toward vaccination. And repeat Question 3 for each group. Who drove the pattern observed in Question 2?
```{r}
mmr.u=subset(mmr,prior=="unfavorable")
tapply(mmr.u$intent,mmr.u$cond,mean) # comparing proportions people intending to vaccinate
tapply(mmr.u$intent,mmr.u$cond,mean)[2]-tapply(mmr.u$intent,mmr.u$cond,mean)[1] # Effect of correction treatment
tapply(mmr.u$intent,mmr.u$cond,mean)-tapply(mmr.u$intent,mmr.u$cond,mean)[1] # Effect of four treatments
```


--- 

# Class 5


## Today's Agenda
- Causality and Experimental Research
- MMR Vaccine Experiment
- Social Pressure Experiment 
- Same-sex Marriage Experiment (probably next class)

## Datasets
```{r}
mmr<-read.csv("mmr.csv")
social<-read.csv("social.csv")
gay<-read.csv("gay.csv")
```

# Causality and Experimentation

## Correlation is not causation

![](Screen Shot 2020-01-22 at 1.41.54 PM.png){width=500px}

## Correlation is not causation


- People who go to museums live longer: correlation
- Going to museums increases life expectancy: causation
- People often conflate the two
- Different how? 
- Correlation = Causation + Selection Bias
  + Selection Bias: Any difference that would have existed irrespective of museum visits
  + People who can afford to go to museums are more likely to afford decent health care, to exercise, etc.
  + Correlation can arise even if there is **no** effect of museum visits on health

## When correlation *is* causation

- Experimental research
  + Randomly assign treatments to different observations
  + Then measure the outcome variables
  + Randomization &rarr; No systematic difference &rarr; No selection bias
- Difference between experimental vs. non-experimental research
  + Random assignment
- When there's no random assignment
  + Causal inference becomes *extremely* difficult
- When you read an article that makes causal claims without randomization
  + Be very skeptical
  + It's the author's job to convince you!
  + Correlation dressed up as causation: more common than you'd think


## Basics of understanding (analyzing) experiments

- What are the experimental conditions? 
- What is the outcome variable (i.e., "dependent variable)? 
- The most basic approach to analyzing experimental data
  - Compare means between control vs treatment groups on the outcome of interest
  - Does the treatment group's mean differ from the control group's mean in the expected direction?
  - Is that difference statistically and substantively meaningful?
    - We will deal with this question later in this course
    

# Exercise: MMR Vaccine Experiment

## Background 
- Question: How to reduce vaccine misperceptions and increase vaccination rates?
- A nationally representative experiment with about 1700 parents conducted in 2011
- Parents were randomly assigned to receive 1 of 4 treatment messages or a "placebo" message
  1. Placebo: costs and benefits of bird feeding (Unrelated)
  2. Correction: No evidence that vaccination causes autism
  3. Disease Image: Pictures of a child who has each of the diseases 
  4. Disease Narrative: A narrative of an infant boy hospitalized because of measles
  5. Disease Risk: Explain the dangers of the diseases prevented by MMR vaccine
- Nyhan et al. (2014) Effective Messages in Vaccine Promotion: A Randomized Trial. *Pediatrics*



## Data

```{r}
names(mmr)
```

- cond: experimental conditions
- prior: attitude toward vaccination measured **BEFORE** treatment
- autism: 1 = MMR vaccine doesn't cause autism (0 = else)
- intent: 1 = Very likely to give MMR vaccine to another child (0 = else)

## Exercise
1. Obtain the number of observations for each condition.
2. Compare people's perception about the link between MMR vaccine and autism across conditions. Which treatment(s) worked?
3. Compare people's intention to vaccinate their children across conditions. Which treatment(s) worked? Is there anything odd?
4. Subset the data by people's prior attitude toward vaccination. And repeat Question 3 for each group. Who drove the pattern observed in Question 2?

## Question 1
-  Question1: Obtain the number of observations for each condition.

## Question 2
- Question 2: Compare people's perception about the link between MMR vaccine and autism across conditions. Which treatment(s) worked?
```{r}
```

## Question 3

- Question 3: Compare people's intention to vaccinate their children across conditions. Which treatment(s) worked? Is there anything odd?

```{r}
```
## Question 4

- Question 4:  Subset the data by people's prior attitude toward vaccination. And repeat Question 3 for each group. Who drove the pattern observed in Question 2?
```{r}
```


# Social Pressure and Voting

## Background

- Question: why do people vote at all, when there's no chance that your vote will change the election outcome? Perhaps, people are pressured/shamed into voting
- The effect of social pressure on voting
- A large scale randomized experiment in Michigan (2006)
- Gerber et al. (2008). "Social Pressure and Voter Turnout: Evidence from a Large-Scale Field Experiment." American Political Science Review.

## Design

- Randomly assigned Michigan households to receive nothing (control) or one of "get-out-the-vote" messages (treatment)
- Sent out mails days before the 2006 primary election
- Treatments
  - Civic Duty: DO YOUR CIVIC DUTY â€“ VOTE!
  - Hawthorne: Civic duty + YOU ARE BEING STUDIED!
    + Hawthorne effect: people act differently when they know they're being studied
  - Neighbors: Civic duty + Threaten to publicize whether you voted to your neighbors
- Compared turnout rates across conditions using public voting records

## "Civic Duty" Treatment

![](civic.png){width=500px}


## "Hawthorne" Traetment

![](hawthorne.png){width=500px}

## "Neighbor" Treatment

![](neighbor.png){width=500px}



## Variables
```{r}
names(social)
```
  - primary2004: voted in 2004 primary (1) or didn't vote (0)
  - primary2006: voted in 2006 primary (1) or didn't vote (0)
  - messages: experimental conditions
  - hhsize: household size
  - sex
  - yearofbirth: year of birth

## In-class Exercise

1. Inspect the data and the variables that it contains. Provide the number of observations per conditions. 
2. Change the messages variable to a factor variable and re-order it so that the control group comes first, and the neighbors condition comes last.
3. Before looking at the treatment effects, make your own predictions about which message would increase turnout rates, and by how much.
4. Compare the 2006 turnout rates across conditions. Calculate the effects of each treatment message.Did the finding confirm your expectations?
5. Compare the 2004 turnout rates across conditions. Think about what this finding means.
6. Compare gender proportions and age across conditions


## Question 1

Inspect the data and the variables that it contains. Provide the number of observations per conditions. 

```{r}
```

## Question 2

Change the messages variable to a factor variable and re-order it so that the control group comes first, and the neighbors condition comes last.

```{r}
```

## Questions 4 

- Compare the 2006 turnout rates across conditions. Did the finding confirm your expectations? 
- Calculate the effects of each treatment message.

```{r}
```


## Questions 5 
- Compare the 2004 turnout rates across conditions. Think about what this finding means.
```{r}
```

  
  
## Questions 5 
- Compare the 2004 turnout rates across conditions. Think about what this finding means.
```{r}
```

- Pre-treatment Variables
  + Anything that happened/determined before treatment
  + 2004 Turnout, gender, year of birth, household size, etc
  + Should be well-balanced, especially with a large N
  + Large imbalance in pre-treatment variables: sign that something went wrong!

## Questions 6 

- Compare gender proportions and age across conditions

```{r}
```

## Real-world Implication: Targeted Mobilization

- Persuasion is hard
- Much more cost-effective to turn non-voters into voters
- Popularized targeted mobilization tactic among political organizations
  + Use voter files, etc. to figure out which side you'd vote for if you were to vote
  + Female, early 20s, living in Philly: good chance that you'd vote for the Dems
  + Shame you into voting if it seems like you're on their side
- You may get one of these mails

# Gay Marriage Experiment

## Background
- Question: can we effectively persuade people to change their minds about fundamental issues?
- Contact hypothesis: out-group hostility diminishes when people from different groups interact with one another.
- Two randomized control trials in Los Angeles (2013)
- Timed around the Supreme Court decision to legalize gay marriage in CA
- LaCour & Green (2015). â€œWhen contact changes minds: An experiment of transmission of support for gay equality.â€ Science.

## Study Design

- Randomized treatment: gay (ð‘› = 22) vs. straight (ð‘› = 19) canvassers with similar characteristics
- same-sex marriage vs. recycling scripts (20 min conversation) 
- a total of 4 treatments: 2 Ã— 2 factorial design
  + Same-Sex Marriage Script by Gay Canvasser
  + Same-Sex Marriage Script by Straight Canvasser
  + Recycling Script by Gay Canvasser
  + Recycling Script by Straight Canvasser
- control group: no canvassing.

- same-sex marriage scripts:
  + gay canvassers: they would like to get married but the law prohibits it. 
  + straight canvassers: their gay child, friend, or relative would like to get married but the law prohibits it.

- Outcome measured via unrelated panel survey: self-reported support for same-sex marriage.

---

### Data
- study: Source of the data (1 = Study1, 2 = Study2) 
- treatment: Five possible treatment assignment options
- wave: wave (a total of 7 waves)
  + Wave 1: before treatment
  + Wave 2: 3 days after treatment
  + Wave 5: after legalization of same sex marriage by the Supreme Court
  + Wave 7: 9 months after treatment
- ssm: Survey  5 point scale on same-sex marriage, higher scores indicate support.

### Exercise (Focus on  Study 1)
1. Compare people's support for same-sex marriage before treatment
2. Calculate treatment effects at Wave 2 
3. Examine script effects (ssm vs recycling) separately for gay canvassers and straight canvassers at Wave 2
4. Calculate treatment effects at Waves 5 

## Questions 1 $ 2
Wave 1
```{r}
```
Wave 2
```{r}
```

## Question 3
```{r}
```

## Question 5

```{r}
```


## Aftermath

- Looked like a ground-breaking study but...

![](green.png){width=700px}


--- 
# Class 6


## Today's Agenda
- Social Pressure Experiment 
- Same Sex Marriage Experiment
- Measurement
  - Descriptive Statistics
  - Download and save the following files in your working directory

## Datasets
```{r}
social<-read.csv("social.csv")
gay<-read.csv("gay.csv")
gallup<-read.csv("gallup.csv") # new
comm290<-read.csv("comm290 full.csv") # new
```

## Housekeeping

- Assignment 1
  + Due tonight by midnight
  + Submit your Rmd and output files 
  + Do not include the `View()` function in your Rmd code (it won't knit)
  + Your Rmd file should be in your working directory

- Collaboration
  + You're encouraged to work with other students on the materials or future homework
  + If you do, write the name of the students you worked with 
  + Important: you should write your own code for homework assignments
  + Send me an email if you're interested in forming a study group (I will couple you with other students)

# Social Pressure and Voting

## Background

- Question: why do people vote at all, when there's no chance that your vote will change the election outcome? Perhaps, people are pressured/shamed into voting
- The effect of social pressure on voting
- A large scale randomized experiment in Michigan (2006)
- Gerber et al. (2008). "Social Pressure and Voter Turnout: Evidence from a Large-Scale Field Experiment." American Political Science Review.

## Design

- Randomly assigned Michigan households to receive nothing (control) or one of "get-out-the-vote" messages (treatment)
- Sent out mails days before the 2006 primary election
- Treatments
  - Civic Duty: DO YOUR CIVIC DUTY â€“ VOTE!
  - Hawthorne: Civic duty + YOU ARE BEING STUDIED!
    + Hawthorne effect: people act differently when they know they're being studied
  - Neighbors: Civic duty + Threaten to publicize whether you voted to your neighbors
- Compared turnout rates across conditions using public voting records

## "Civic Duty" Treatment

![](civic.png){width=500px}


## "Hawthorne" Traetment

![](hawthorne.png){width=500px}

## "Neighbor" Treatment

![](neighbor.png){width=500px}



## Variables
```{r}
names(social)
```
  - primary2004: voted in 2004 primary (1) or didn't vote (0)
  - primary2006: voted in 2006 primary (1) or didn't vote (0)
  - messages: experimental conditions
  - hhsize: household size
  - sex
  - yearofbirth: year of birth

## In-class Exercise

1. Inspect the data and the variables that it contains. Provide the number of observations per conditions. 
2. Change the messages variable to a factor variable and re-order it so that the control group comes first, and the neighbors condition comes last.
3. Compare the 2006 turnout rates across conditions. Calculate the effects of each treatment message. 
4. Compare the 2004 turnout rates across conditions. Think about what this finding means.
5. Compare gender proportions and age across conditions


## Question 1

- Inspect the data and the variables that it contains. Provide the number of observations per conditions.




## Question 2

- Change the messages variable to a factor variable and re-order it so that the control group comes first, and the neighbors condition comes last.



## Questions 3 

- Compare the 2006 turnout rates across conditions. 
- Calculate the effects of each treatment message.




## Questions 6 

- Compare gender proportions and age across conditions



## Real-world Implication: Targeted Mobilization

- Persuasion is hard
- Much more cost-effective to turn non-voters into voters
- Popularized targeted mobilization tactic among political organizations
  + Use voter files, etc. to figure out which side you'd vote for if you were to vote
  + Female, early 20s, living in Philly: good chance that you'd vote for the Dems
  + Shame you into voting if it seems like you're on their side
- You may get one of these mails



# Gay Marriage Experiment

## Background
- Question: can we effectively persuade people to change their minds about fundemental issues?
- Contact hypothesis: out-group hostility diminishes when people from different groups interact with one another.
- Two randomized control trials in Los Angeles (2013)
- Timed around the Supreme Court decision to legalize gay marriage
- LaCour & Green (2015). â€œWhen contact changes minds: An experiment of transmission of support for gay equality.â€ Science.

## Study Design

- Randomized treatment: gay (ð‘› = 22) vs. straight (ð‘› = 19) canvassers with similar characteristics
- same-sex marriage vs. recycling scripts (20 min conversation) 
- a total of 4 treatments: 2 Ã— 2 factorial design
  + Same-Sex Marriage Script by Gay Canvasser
  + Same-Sex Marriage Script by Straight Canvasser
  + Recycling Script by Gay Canvasser
  + Recycling Script by Straight Canvasser
- control group: no canvassing.

- same-sex marriage scripts:
  + gay canvassers: they would like to get married but the law prohibits it. 
  + straight canvassers: their gay child, friend, or relative would like to get married but the law prohibits it.

- Outcome measured via unrelated panel survey: self-reported support for same-sex marriage.



## Data
- study: Source of the data (1 = Study1, 2 = Study2) 
- treatment: Five possible treatment assignment options
- wave: wave (a total of 7 waves surveys)
  + Wave 1: before treatment
  + Wave 2: 3 days after treatment (randomly assigned conversation with a canvasser)
  + Wave 5: after legalization of same sex marrige by the Supreme Court
  + Wave 7: 9 months after treatment
- ssm: Survey  5 point scale on same-sex marriage, higher scores indicate support.

## Exercise (Focus on  Study 1)
```{r}
gay.s1<-subset(gay,study==1)
```
1. Compare people's support for same-sex marriage before treatment
2. Calculate treatment effects at Wave 2 
3. Examine script effects (difference between ssm vs recycling) separately for gay canvassers and straight canvassers at Wave 2
4. Calculate treatment effects at Waves 5 
   
   
- Hint
  - Use the `subset()` function to divide the data into different waves
  - We will go over each of the questions in next class.


# Measurement

## Concepts & Measurement

- Social science is about developing and testing causal theories:
  + Does outgroup contact inï¬‚uence opinions on same-sex marriage?
  + Does vaccine promotion increase parents' intention to vaccinate?

- Theories are made up of concepts:
  + outgroup contact, opinions on same-sex marriage, intention to vaccinate, etc
  + We took these for granted when talking about causality.

- Important to consider how we measure these concepts.
  + Measurement: process of turning a concept into a number for each observation
  + Some more straightforward: what is your age?
  + Others more complicated: what does it mean to be "conservative" vs "liberal"?
  + Have to create an operational deï¬nition of a concept to make it into a variable in our dataset.

## Example 

- Concept: presidential approval

- Conceptual definition
  + Extent to which US adults support the actions and policies of the current US president

- Operational definition: 
  + "On a scale from 1 to 5, where 1 is least supportive and 5 is more supportive, how much would you say you support the job that Donald Trump is doing as president?"

## Measurement error

- Random errors
  + Stem from miscommunication, confusion, data-entry error, etc.
    + "Wait.. Was 1 least supportive, or most supportive?"
  + "Do you consider yourself a liberal or conservative?"
  + Random errors tend to cancel out when we take averages
- Bias
  + Systematic errors, often in the same direction
  + Example: social desirability bias
    + "Did you vote in 2018?" "YES!"

## Sometimes measurement is intentionally biased
![](poll.png){width=300px}


## Descriptive statistics

- A variable is a series of measurements about some concept.

- Descriptive statistics are numerical summaries of those measurements.

- Two salient features of a variable that we want to know:
  + Central tendency: where is the middle/typical/average value.
  + Spread around the center: are all the data close to the center or spread out?

## Center of the data

- Typical/average value
- Mean: sum of the values divided by the number of observations
- Median: middle value if `N` is odd or average of two middle values if `N` is even
  + `N`: Number of observations
```{r}
median(c(1,3,4))
median(c(1,3,4,10))
```

## Center of the data

- Median is more robust to outliers
```{r}
median(c(0,1,2,3,4,5))
mean(c(0,1,2,3,4,5))
median(c(0,1,2,3,4,100))
mean(c(0,1,2,3,4,100))
```
## Spread of the data

- Are the data close to the center?
- Range: min to max
- Quantiles: divide observations into groups based on values of variables
  + median: divide into two (upper-half vs lower-half)
  + quartiles: divide into four
    + first quartile (25%)
    + second quartile (50%)
    + third quartile (75%)
    + fourth quartile (100% `=` maximum)


```{r}
gallup<-read.csv("gallup.csv")
summary(gallup)
```

## Spread of the data

- Interquartile range (IQR)
    - How spread out is the middle half of the data?
```{r}
IQR(gallup$weight,na.rm=T) # na.rm=T: removing missing values 
```

- **Missing values in R**
  + Very common: people skip parts of surveys, etc.
  + Almost all data have missing values 
  + We have been using "cleaned" datasets up until this point
  + Coded as `NA` ("Not Available") in R
  + When there are missing values, you should tell R what to do with them
  + `na.rm=T`: "remove NAs from this cacluation = TRUE"
    + If you set this to FLASE, you'll get an error message or simply "NA"
    + Some functions handle NAs automatically (e.g., Summary), but often you have to specify



## Spread of the data
  
- quntiles (5 groups), deciles (10 groups), percentiles (100 groups), etc.
- use quantile() function to return these values

```{r}
quantile(gallup$weight, probs = seq(from = 0, to = 1 , by = 0.1), na.rm=T) # creating deciles
```

## Standard deviation 
- On average, how far away are data points from the mean?
- Steps 
  + step 1: Subtract each data point by the mean
  + step 2: Square each resulting difference
  + step 3: Take the sum of these values
  + step 4: Divide by n or n-1 (R's default): Variance
  + step 5: Take the squareroot

```{r}
diff=comm290$terms-mean(comm290$terms) # Step 1 
diff.sq=diff^2  # step 2
sum.diff.sq=sum(diff.sq) # step 3
terms.var=sum.diff.sq/14  # step 4
sqrt(terms.var) # step 5
```


## Standard deviation 
- use the `var()` and `sd()` function to calculate the variance and SD

```{r}
var(comm290$terms)
sd(comm290$terms)
```

## Gallup Survey Data

```{r}
names(gallup)
```

  - wellbeing: Please imagine a ladder with steps numbered from zero at the bottom to ten at the top. The top of the ladder represents the best possible life for you and the bottom of the ladder represents the worst possible life for you. On which step of the ladder would you say you personally feel you stand at this time?
    + 10: best possible, ... 0: worst possible
  - goodlooking: You always feel good about your physical appearance.
    + 5: strongly agree, ... 0: strongly disagree
  - exercise: number of days you exercised in last week 

## In-class Exercise

  1. Calculate the deciles of height
  2. Calculate interqurtile range of weight
  3. Calculate the mean, median, standard deviation of height for people in their 10s, 20s, ... 80s
  4. Calculate the proportions of American males taller than 6 feet.
  5. Is there a relationship between exercise and well-being? Calculate the mean of wellbeing across exercise.
  6. Follow the five steps to calculate the standard deviation of height, and then use the sd() function to see if you got the right number
  

---

# Class 7

## Today's Agenda

- Same Sex Marriage Experiment
- Measurement
- Missing Values in R
- Descriptive Statistics


## Hosekeeping

- Assignment 1
  + Graded assignments were returned
- Assignment 2
  + Will be up later today
  + Due on 10/6
  + *Harder* than Assignment 1
    + Start early!
- Study group

## Homework Assignment 1 Review

- Great job!   
- One common mistake: `%` vs. percentage points
  - percent $\rightsquigarrow$ ratio between two numbers
  - percentage points $\rightsquigarrow$  difference between two percentages
- Example: The effect of the neighbors message in the social pressure experiment
  - Turnout rate increased from 0.30 (control) to 0.38 (neighbors)
  - The turnout rate among the neighbor group is .38/.30 = 128% of the turnout rate among the control group.
- Statements summarizing the result
  - The neighbor treatment increased turnout by 28% (O)
  - The neighbor treatment increased turnout by 8 percentage points (O)
  - The neighbor treatment increased turnout by 8% (X)

## Datasets

```{r}
gay<-read.csv("gay.csv")
gallup<-read.csv("gallup.csv")
comm290<-read.csv("comm290 full.csv")
```


# Same Sex Marriage Experiment

## Study Design

- Randomized treatment: gay (ð‘› = 22) vs. straight (ð‘› = 19) canvassers with similar characteristics
- same-sex marriage vs. recycling scripts (20 min conversation) 
- a total of 4 treatments: 2 Ã— 2 factorial design
  + Same-Sex Marriage Script by Gay Canvasser
  + Same-Sex Marriage Script by Straight Canvasser
  + Recycling Script by Gay Canvasser
  + Recycling Script by Straight Canvasser
- control group: no canvassing.

- same-sex marriage scripts:
  + gay canvassers: they would like to get married but the law prohibits it. 
  + straight canvassers: their gay child, friend, or relative would like to get married but the law prohibits it.

- Outcome measured via unrelated panel survey: self-reported support for same-sex marriage.


## Exercise (Focus on  Study 1)
1. Compare people's support for same-sex marriage before treatment
2. Calculate treatment effects at Wave 2 
3. Examine script effects (difference between ssm vs recycling) separately for gay canvassers and straight canvassers at Wave 2
4. Calculate treatment effects at Waves 5 

## Questions 1 
Compare people's support for same-sex marrage before treatment
```{r}
gay<-read.csv("gay.csv")
gay.s1<-subset(gay,study==1)
s1.w1<-subset(gay.s1, wave==1)
tapply(s1.w1$ssm,s1.w1$treatment,mean)
```

## Questions 2
Calculate treatment effects at Wave 2 
```{r}
s1.w2<-subset(gay.s1, wave==2)
means.w2<-tapply(s1.w2$ssm,s1.w2$treatment,mean)
```

## Questions 2
Calculate treatment effects at Wave 2 
```{r}
effects.w2<-means.w2-means.w2[1]
effects.w2
```

## Questions 2
Calculate treatment effects at Wave 2 
```{r}
round(effects.w2*100)
```
- The treatment effects are 7, -4, 10 and 12 percentage points.


## Question 3
Examine script effects (ssm vs recycling) separately for gay canvassers and straight canvassers at Wave 2
```{r results='hold'}
scr.effect.gay<-means.w2[4]-means.w2[2]
scr.effect.str<-means.w2[5]-means.w2[3]
scr.effect.gay
scr.effect.str
```

```{r results='hold'}
names(scr.effect.gay)<-"Script effect for gay canvassers" # using the names() function to assign a name to the object
names(scr.effect.str)<-"Script effect for straight canvassers"
scr.effect.gay 
scr.effect.str
```
## Question 4
Calculate treatment effects at Waves 5 
```{r}
s1.w5<-subset(gay.s1, wave==5)
tapply(s1.w5$ssm,s1.w5$treatment,mean)-tapply(s1.w5$ssm,s1.w5$treatment,mean)[1]
```


## Aftermath

- Looked like a ground-breaking study but...

![](green.png){width=700px}

# Measurement

## Concepts & Measurement

- Social science is (largely) about developing and testing causal theories:
  + Does outgroup contact influence opinions on same-sex marriage?
  + Does vaccine promotion increase parents' intention to vaccinate?

- Theories are made up of concepts:
  + outgroup contact, opinions on same-sex marriage, intention to vaccinate, etc
  + We took these for granted when talking about causality.

- Important to consider how we measure these concepts.
  + Measurement: process of turning a concept into a number for each observation
  + Some more straightforward: what is your age?
  + Others more complicated: what does it mean to be "conservative" vs "liberal"?
  + Have to create an operational definition of a concept to make it into a variable in our data.

## Example 

- Concept: presidential approval

- Conceptual definition
  + Extent to which US adults support the actions and policies of the current US president

- Operational definition: 
  + "On a scale from 1 to 5, where 1 is least supportive and 5 is more supportive, how much would you say you support the job that Donald Trump is doing as president?"

## Measurement error

- Random errors
  + Stem from miscommunication, confusion, data-entry error, etc.
    + "Wait.. Was 1 least supportive, or most supportive?"
  + "Do you consider yourself a liberal or conservative?"
  + Random errors tend to cancel out when we take averages
- Bias
  + Systematic errors, often in the same direction
  + Example: social desirability bias
    + "Did you vote in 2018?" "YES!"

## Sometimes measurement is intentionally biased
![](poll.png){width=300px}


## Descriptive statistics

- A variable is a series of measurements about some concept.

- Descriptive statistics are numerical summaries of those measurements.

- Two salient features of a variable that we want to know:
  + Central tendency: where is the middle/typical/average value.
  + Spread around the center: are all the data close to the center or spread out?


## Gallup Survey Data 

```{r}
names(gallup)
```

  - wellbeing: Please imagine a ladder with steps numbered from zero at the bottom to ten at the top. The top of the ladder represents the best possible life for you and the bottom of the ladder represents the worst possible life for you. On which step of the ladder would you say you personally feel you stand at this time?
    + 10: best possible, ... 0: worst possible
  - goodlooking: You always feel good about your physical appearance.
    + 5: strongly agree, ... 0: strongly disagree
  - exercise: number of days you exercised in last week 

## Gallup Survey Data 
  
```{r}
summary(gallup)
```

## Missing values in R
- People skip parts of surveys, etc.
- Almost all data have missing values
- Something you have to deal with very often
- We have been using "cleaned" datasets up until this point
- Coded as `NA` ("Not Available") in R
- When there are missing values, sometimes, you should tell R what to do with them
- `na.rm=T`: "remove `NA`s from this cacluation = `TRUE`"
    + Some functions handle NAs automatically (e.g., `summary()`), but often you have to specify to remove `NA`s


## Missing values in R

```{r}
summary(gallup$weight)
mean(gallup$weight)
mean(gallup$weight,na.rm=T)
```
- `is.na()` function can tell you whether a value is missing 
```{r}
x<-c(1,NA,3,4,5)
is.na(x)
```

## Missing values in R
- Check the number of missing values in a variable
```{r}
table(is.na(gallup$weight))
sum(is.na(gallup$weight))
```

## Missing values in R

- `na.omit()` function: Drop observations with at least one missing value
```{r}
gallup.nom<-na.omit(gallup)
nrow(gallup)
nrow(gallup.nom)
```


  
## Center of the data

- Typical/average value
- Mean: sum of the values divided by the number of observations
- Median: middle value if `N` is odd or average of two middle values if `N` is even
  + `N`: Number of observations
```{r}
median(c(1,3,4))
median(c(1,3,4,10))
```

## Center of the data

- Median is more robust to outliers
```{r}
median(c(0,1,2,3,4,5))
mean(c(0,1,2,3,4,5))
median(c(0,1,2,3,4,100))
mean(c(0,1,2,3,4,100))
```


## Spread of the data

- Are the data close to the center?
- Range: min to max
- Quantiles
  + Cutoffs that divide observations into groups based on values of variables
  + median: divide into two (upper-half & lower-half)
  + quartiles: divide into four
    + first quartile (25%)
    + second quartile (50%)
    + third quartile (75%)


```{r}
summary(gallup$weight)
```

## Spread of the data

- Interquartile range (IQR)
    - How spread out is the middle half of the data?
```{r}
IQR(gallup$weight,na.rm=T) # na.rm=T: removing missing values 
IQR(na.omit(gallup$weight))
```

- quntiles (5 groups), deciles (10 groups), percentiles (100 groups), etc.
- use the `quantile()` function to return these values

```{r}
quantile(gallup$weight, probs = seq(from = 0, to = 1 , by = 0.1), na.rm=T) # creating deciles
```



## Standard deviation 
- On average, how far away are data points from the mean?
- Steps 
  + step 1: Subtract each data point by the mean
  + step 2: Square each resulting difference
  + step 3: Take the sum of these values
  + step 4: Divide by n or n-1 (R's default): Variance
  + step 5: Take the squareroot

```{r}
diff=comm290$terms-mean(comm290$terms) # Step 1 
diff.sq=diff^2  # step 2
sum.diff.sq=sum(diff.sq) # step 3
terms.var=sum.diff.sq/14  # step 4
sqrt(terms.var) # step 5
```


## Standard deviation 
- use the `var()` and `sd()` function to calculate the variance and SD

```{r}
var(comm290$terms)
sd(comm290$terms)
```



## In-class Exercise

  1. Calculate the mean, median, standard deviation of height for people in their 10s, 20s, ... 80s
  2. Calculate the proportions of American males taller than 6 feet.
  3. Is there a relationship between exercise and well-being? Calculate the mean of wellbeing across exercise.

## Question 1

- Calculate the mean, median, standard deviation of satisfaction with one's appearance for people in their 10s, 20s, ... 80s or later
  + Hint: create a new variable (`age8`) that divides people into 8 age groups using square brackets.
  
## Question 1

- Calculate the mean, median, standard deviation of satisfaction with one's appearance for people in their 10s, 20s, ... 80s or later

```{r}
gallup$age8[gallup$age<20]<-"10s"
gallup$age8[gallup$age>=20 & gallup$age<30]<-"20s"
gallup$age8[gallup$age>=30 & gallup$age<40]<-"30s"
gallup$age8[gallup$age>=40 & gallup$age<50]<-"40s"
gallup$age8[gallup$age>=50 & gallup$age<60]<-"50s"
gallup$age8[gallup$age>=60 & gallup$age<70]<-"60s"
gallup$age8[gallup$age>=70 & gallup$age<80]<-"70s"
gallup$age8[gallup$age>=80]<-"80s or older"
tapply(gallup$goodlooking,gallup$age8,mean,na.rm=T)
tapply(gallup$goodlooking,gallup$age8,median,na.rm=T)
```

## Question 2

Calculate the mean, median, standard deviation of exercise frequency for people in their 10s, 20s, ... 80s or later

## Question 2

Calculate the mean, median, standard deviation of exercise frequency for people in their 10s, 20s, ... 80s or later

```{r}
tapply(gallup$goodlooking,gallup$age8,sd,na.rm=T)
```

## Question 3

Estimate the percentage of American males who are 6 feet tall or taller.

## Question 3

Estimate the percentage of American males who are 6 feet tall or taller.

```{r}
gallup.m<-subset(gallup,female==0)
gallup.m$six[gallup.m$height>=72]<-1
gallup.m$six[gallup.m$height<72]<-0
tall.pct<-mean(gallup.m$six,na.rm=T)*100
round(tall.pct, digits=0)
```

## Question 3

Estimate the percentage of American males who are 6 feet tall or taller. 

- We can use the `ifelse()` function instead of `[]`
  + `ifelse(X, Y, Z)`
  + `X`: some condition
  + `Y` is returned if `X` is satisfied (x is true)
  + `Z` is returned if `X` is NOT satisfied (x is false)
```{r}
gallup.m$six<-ifelse(gallup.m$height>=72,1,0)
mean(gallup.m$six,na.rm=T)*100
```



---

# Class 8 

## Today's Agenda

- Homework 2 data
- Descriptive stats of the Gallup data
- Visualizing a single variable

## Data
```{r}
comm290<-read.csv("comm290 full.csv")
gallup<-read.csv("gallup.csv")
climate<-read.csv("climate.csv")
```

# Homework 2 data

## Cliamte change experiment
```{r results='hold'}
summary(climate)
table(climate$cond)
```
- Survey experiment conducted with previous comm290 studdents
- Sample of Republican voters
- Baseline Qs $\rightsquigarrow$ One of three messages $\rightsquigarrow$ Follow-up Qs
  - Treatment effect: compare people in treatment groups against those in the control group
- https://upenn.co1.qualtrics.com/jfe/form/SV_1SQtKdXWc5m64T3 

# Descriptive Statistics (Continued)

## Standard deviation 
- On average, how far away are data points from the mean?
- Steps 
  + step 1: Subtract each data point by the mean
  + step 2: Square each resulting difference
  + step 3: Take the sum of these values (using the `sum()` function)
  + step 4: Divide by n or n-1 (R's default): Variance
  + step 5: Take the squareroot (using the `sqrt()` function)

## Standard deviation 
- Example: SD of familiar stat terms you marked in the comm 290 survey
```{r}
mean(comm290$terms)
diff=comm290$terms-mean(comm290$terms) # Step 1 Subtract each data point by the mean
diff.sq=diff^2  # step 2 Square each resulting difference
sum.diff.sq=sum(diff.sq) # step 3 Take the sum of these values
terms.var=sum.diff.sq/(length(diff)-1)  # step 4 Divide by n or n-1
sqrt(terms.var) # step 5 Take the squareroot
```
- Central Tendency: what is the representative value?
  - On average, comm 290 students knew 10 stat terms out of 14
- Spread: how much do the data vary from student to student?
  - On average, each student's familiar terms *deviated* from the mean by 3 terms

## Standard deviation 
- use the `var()` and `sd()` function to calculate the variance and SD
```{r}
var(comm290$terms)
sd(comm290$terms)
```



## In-class Exercise

- In-class Exercise
  1. Calculate the percentiles of height
  2. Calculate interquartile range of exercise
  3. Calculate the mean, median, standard deviation of wellbeing for people in their 10s, 20s, ... 80s or older
  4. Calculate the difference between the mean and median of wellbeing for each age group
  5. Estimate the percentage of American males who are 6 feet tall or taller.


## Question 1

Calculate the percentiles of height
  - Use the `quantile()` function

## Question 1

Calculate the percentiles of height

```{r}
gallup<-read.csv("gallup.csv")
quantile(gallup$height, probs = seq(0,1,by=0.01),na.rm=T)
```

## Question 2

Calculate interquartile range of height
  - use the `IQR()` function

## Question 2

Calculate interquartile range of height


```{r}
IQR(na.omit(gallup$height))
```


## Question 3

- Calculate the mean, median, standard deviation of well-being for people in their 10s, 20s, ... 80s or older
  + Hint: create a new variable (`age8`) that divides people into 8 age groups using square brackets.
  
## Question 3

- Calculate the mean, median, standard deviation of well-being for people in their 10s, 20s, ... 80s or older

```{r}
gallup$age8[gallup$age<20]<-"10s"
gallup$age8[gallup$age>=20 & gallup$age<30]<-"20s"
gallup$age8[gallup$age>=30 & gallup$age<40]<-"30s"
gallup$age8[gallup$age>=40 & gallup$age<50]<-"40s"
gallup$age8[gallup$age>=50 & gallup$age<60]<-"50s"
gallup$age8[gallup$age>=60 & gallup$age<70]<-"60s"
gallup$age8[gallup$age>=70 & gallup$age<80]<-"70s"
gallup$age8[gallup$age>=80]<-"80s or older"
```

## Question 3

- Calculate the mean, median, standard deviation of well-being for people in their 10s, 20s, ... 80s or later

```{r}
tapply(gallup$wellbeing,gallup$age8,mean,na.rm=T)
tapply(gallup$wellbeing,gallup$age8,median,na.rm=T)
tapply(gallup$wellbeing,gallup$age8,sd,na.rm=T)
```

## Question 4

Calculate the difference between the mean and median of wellbeing for each age group

## Question 4

Calculate the difference between the mean and median of wellbeing for each age group

```{r}
tapply(gallup$wellbeing,gallup$age8,mean,na.rm=T)-tapply(gallup$wellbeing,gallup$age8,median,na.rm=T)

```

## Question 5

Estimate the percentage of American males who are 6 feet tall or taller.

## Question 5

Estimate the percentage of American males who are 6 feet tall or taller.

```{r}
gallup.m<-subset(gallup,female==0)
gallup.m$six[gallup.m$height>=72]<-1
gallup.m$six[gallup.m$height<72]<-0
tall.pct<-mean(gallup.m$six,na.rm=T)*100
round(tall.pct, digits=0)
```

## Question 5

Estimate the percentage of American males who are 6 feet tall or taller. 

- We can use the `ifelse()` function instead of `[]`
  + `ifelse(X, Y, Z)`
  + `X`: some condition
  + `Y` is returned if `X` is satisfied (x is true)
  + `Z` is returned if `X` is NOT satisfied (x is false)
```{r}
gallup.m$six<-ifelse(gallup.m$height>=72,1,0) # NA's don't get any value
mean(gallup.m$six,na.rm=T)*100
```

## Question 6

- Use the the `ifelse()` function to estimate the percentage of American who exercise everyday


## Question 6

- Use the the `ifelse()` function to estimate the percentage of American who exercise everyday
```{r}
gallup$exercise7<-ifelse(gallup$exercise==7,1,0)
mean(gallup$exercise7,na.rm=T)*100
```



# Visualizing a Single Variable

## Barplot

- The `barplot()` function visualizes the distribution of a categorical variable
  + Categorical variable: Values have no numeric information
    + race, gender, zipcodes, etc.
    + 19104 is **not** greater than 19103
- Create a frequency table and apply the `barplot()` function on the table

## Barplot of race

```{r, eval = FALSE}
race.table<-table(gallup$race)
barplot(race.table)
```

```{r, echo = FALSE}
race.table<-table(gallup$race)
barplot(race.table)
```


## Barplot of race

- Various options: `barplot (vector, names = , xlab = , ylab = , main = )`
  + names: labels for each category (if you want to change them)
  + xlab: x-axis label
  + ylab: y-axis label
  + main: title of the figure

```{r, eval = FALSE}
barplot(race.table,
        names = c("Asian","Black","Hispanic","Other Race","White"),
        xlab = "Race",
        ylab = "Number of Respondents",
        main = "Race Distribution in Gallup")
```

## Barplot of race

```{r, echo = FALSE}
barplot(race.table,
        names = c("Asian","Black","Hispanic","Other Race","White"),
        xlab = "Race",
        ylab = "Number of Respondents",
        main = "Race Distribution in Gallup")
```


## Barplot of race
- Let's resort the table (tallest bar first) using the `sort()` function
```{r, eval = FALSE}
race.table<-table(gallup$race)
barplot(sort(race.table, decreasing = TRUE),
        xlab = "Race",
        ylab = "Number of Respondents",
        main = "Race Distribution in Gallup")
```

## Barplot of race
- Let's resort the table (tallest bar first) using the `sort()` function
```{r, echo = FALSE}
race.table<-table(gallup$race)
barplot(sort(race.table, decreasing = TRUE),
        xlab = "Race",
        ylab = "Number of Respondents",
        main = "Race Distribution in Gallup")
```

## Barplot of race
- Let's display proportions instead of cases
```{r, eval = FALSE}
race.prop.table<-prop.table(table(gallup$race))
barplot(race.prop.table,
        xlab = "Race",
        ylab = "Proportion of Respondents",
        main = "Race Distribution in Gallup")
```

## Barplot of race
- Let's display proportions instead of cases
```{r, echo = FALSE}
race.prop.table<-prop.table(table(gallup$race))
barplot(race.prop.table,
        xlab = "Race",
        ylab = "Proportion of Respondents",
        main = "Race Distribution in Gallup")
```

## Using barplot to compare means

```{r, eval=FALSE}
social<-read.csv("social.csv")
turnout<-tapply(social$primary2006,social$messages,mean)
barplot(turnout, main = "Turnout Rates by Conditions", 
        xlab = "Experimental Conditions", 
        ylab = "Turnout Rate")
```
## Using barplot to display group means

```{r, echo=FALSE}
social<-read.csv("social.csv")
turnout<-tapply(social$primary2006,social$messages,mean)
barplot(turnout, main = "Turnout Rates by Conditions", 
        xlab = "Experimental Conditions", 
        ylab = "Turnout Rate")
```


## Using bar plots to display treatment effects

```{r, eval=FALSE}
te<-turnout-turnout[2]
barplot(te, main = "Treatment Effects by Conditions", 
        xlab = "Experimental Conditions", ylab = "Treatment Effects")
```
## Using bar plots to display treatment effects

```{r, echo=FALSE}
te<-turnout-turnout[2]
barplot(te, main = "Treatment Effects by Conditions", 
        xlab = "Experimental Conditions", ylab = "Treatment Effects")
```


## Using bar plots to display treatment effects
  - let's remove control

```{r, eval=FALSE}
te<-turnout-turnout[2]
barplot(te[-2], main = "Treatment Effects by Conditions", 
        xlab = "Experimental Conditions", ylab = "Treatment Effects")
```

## Using bar plots to display treatment effects
  - let's remove control

```{r, echo=FALSE}
te<-turnout-turnout[2]
barplot(te[-2], main = "Treatment Effects by Conditions", 
        xlab = "Experimental Conditions", ylab = "Treatment Effects")
```


## Histogram

- The `hist()` function creates a histogram that visualizes distribution of continuous variables
  + Continuous variable: values have numeric information
    + height, age, weight
  + Histogram
    + Classify people into bins
    + Count the number of observations or proportions in each bin
    + Display frequency (raw number) or density (proportions/bin width)
  
- `hist (variable, freq = , xlab = , main = )`
  + variable: the variable you want to display
  + `freq = TRUE`: display frequency; `freq = FALSE`: display density

## Histogram of age

```{r}
hist(gallup$age,freq=FALSE, xlab="Respondent's Age",
     main="Distribution of Gallup Respondent's Age")
```

## Density in a histogram 


  - The area of each block = proportions of observations in each block
  - The areas of the blocks sum to 1 (100%)


## Age density with 5 bins
- You can modify bin locations/numbers specifying `nclass` (number of bins) or `breaks` (location of the bin breaks) 
```{r}
hist(gallup$age,freq=FALSE, xlab="Respondent's Age",
     main="Distribution of Gallup Respondent's Age",
     nclass = 5)
```

## Age density with custom breaks
```{r}
hist(gallup$age,freq=FALSE, xlab="Respondent's Age",
     main="Distribution of Gallup Respondent's Age",
     breaks = c(18,30,45,60,75,100))
```


## Adjust the y-axis
  - `ylim = c(min,max)` 
```{r, eval=FALSE}
hist(gallup$age,freq=FALSE, xlab="Respondent's Age",
     main="Distribution of Gallup Respondent's Age",
     breaks = c(18,30,45,60,75,100), ylim = c(0,0.03)
     )
```

  
## Adjust the y-axis
- `ylim = c(min,max)` 

```{r, echo=FALSE}
hist(gallup$age,freq=FALSE, xlab="Respondent's Age",
     main="Distribution of Gallup Respondent's Age",
     breaks = c(18,30,45,60,75,100), ylim = c(0,0.03)
     )
```
  

## Next class
  - A brief lecture on machine learning
  - Will use dataset created with machine learning to visualize descriptive stats 
  - Data manipulation


# CLASS 9 

## Today's Agenda


- Visualizing a single variable
- Machine learning
- Facebook data


## Homework 2

- Due in one week
- Inefficient coding: you may get partial credits
- Question 1
  - Hint: The Gallup survey is NOT an experimental study; exercise frequency wasn't randomized.
- Question 4
  - You should get the same number as the `sd()` function
- There are two Question 4s. Questions 4-6 in Part 2 should be Questions 5-7. 
- If you haven't, start now!


## Data
```{r}
gallup<-read.csv("gallup.csv")
fb<-read.csv("fb.csv")
```



# Visualizing a Single Variable

## Barplot

- The `barplot()` function visualizes the distribution of a categorical variable
  + Categorical variable: Values have no numeric information
    + race, gender, zipcodes, etc.
    + 19104 is **not** greater than 19103
- Create a frequency table and apply the `barplot()` function on the table

## Barplot of race

```{r, eval = FALSE}
race.table<-table(gallup$race)
barplot(race.table)
```

```{r, echo = FALSE}
race.table<-table(gallup$race)
barplot(race.table)
```


## Barplot of race

- Various options: `barplot (vector, names = , xlab = , ylab = , main = )`
  + names: labels for each category (if you want to change them)
  + xlab: x-axis label
  + ylab: y-axis label
  + main: title of the figure

```{r, eval = FALSE}
barplot(race.table,
        names = c("Asian","Black","Hispanic","Other Race","White"),
        xlab = "Race",
        ylab = "Number of Respondents",
        main = "Race Distribution in Gallup")
```

## Barplot of race

```{r, echo = FALSE}
barplot(race.table,
        names = c("Asian","Black","Hispanic","Other Race","White"),
        xlab = "Race",
        ylab = "Number of Respondents",
        main = "Race Distribution in Gallup")
```


## Barplot of race
- Let's resort the table (tallest bar first) using the `sort()` function
```{r, eval = FALSE}
race.table<-table(gallup$race)
barplot(sort(race.table, decreasing = TRUE),
        xlab = "Race",
        ylab = "Number of Respondents",
        main = "Race Distribution in Gallup")
```

## Barplot of race
- Let's resort the table (tallest bar first) using the `sort()` function
```{r, echo = FALSE}
race.table<-table(gallup$race)
barplot(sort(race.table, decreasing = TRUE),
        xlab = "Race",
        ylab = "Number of Respondents",
        main = "Race Distribution in Gallup")
```

## Barplot of race
- Let's display proportions instead of cases
```{r, eval = FALSE}
race.prop.table<-prop.table(table(gallup$race))
barplot(race.prop.table,
        xlab = "Race",
        ylab = "Proportion of Respondents",
        main = "Race Distribution in Gallup")
```

## Barplot of race
- Let's display proportions instead of cases
```{r, echo = FALSE}
race.prop.table<-prop.table(table(gallup$race))
barplot(race.prop.table,
        xlab = "Race",
        ylab = "Proportion of Respondents",
        main = "Race Distribution in Gallup")
```

## Using barplot to compare means

```{r, eval=FALSE}
social<-read.csv("social.csv")
turnout<-tapply(social$primary2006,social$messages,mean)
barplot(turnout, main = "Turnout Rates by Conditions", 
        xlab = "Experimental Conditions", 
        ylab = "Turnout Rate")
```
## Using barplot to display group means

```{r, echo=FALSE}
social<-read.csv("social.csv")
turnout<-tapply(social$primary2006,social$messages,mean)
barplot(turnout, main = "Turnout Rates by Conditions", 
        xlab = "Experimental Conditions", 
        ylab = "Turnout Rate")
```


## Using bar plots to display treatment effects

```{r, eval=FALSE}
te<-turnout-turnout[2]
barplot(te, main = "Treatment Effects by Conditions", 
        xlab = "Experimental Conditions", ylab = "Treatment Effects")
```
## Using bar plots to display treatment effects

```{r, echo=FALSE}
te<-turnout-turnout[2]
barplot(te, main = "Treatment Effects by Conditions", 
        xlab = "Experimental Conditions", ylab = "Treatment Effects")
```


## Using bar plots to display treatment effects
  - let's remove control

```{r, eval=FALSE}
te<-turnout-turnout[2]
barplot(te[-2], main = "Treatment Effects by Conditions", 
        xlab = "Experimental Conditions", ylab = "Treatment Effects")
```

## Using bar plots to display treatment effects
  - let's remove control

```{r, echo=FALSE}
te<-turnout-turnout[2]
barplot(te[-2], main = "Treatment Effects by Conditions", 
        xlab = "Experimental Conditions", ylab = "Treatment Effects")
```


## Histogram

- The `hist()` function creates a histogram that visualizes distribution of continuous variables
  + Continuous variable: values have numeric information
    + height, age, weight
  + Histogram
    + Classify people into bins
    + Count the number of observations or proportions in each bin
    + Display frequency (raw number) or density (proportions/bin width)
  
- `hist (variable, freq = , xlab = , main = )`
  + variable: the variable you want to display
  + `freq = TRUE`: display frequency; `freq = FALSE`: display density

## Histogram of age

```{r}
hist(gallup$age,freq=FALSE, xlab="Respondent's Age",
     main="Distribution of Gallup Respondent's Age")
```

## Density in a histogram 


  - The area of each block = proportions of observations in each block
  - The areas of the blocks sum to 1 (100%)


## Age density with 5 bins
- You can modify bin locations/numbers specifying `nclass` (number of bins) or `breaks` (location of the bin breaks) 
```{r}
hist(gallup$age,freq=FALSE, xlab="Respondent's Age",
     main="Distribution of Gallup Respondent's Age",
     nclass = 5)
```

## Age density with custom breaks
```{r}
hist(gallup$age,freq=FALSE, xlab="Respondent's Age",
     main="Distribution of Gallup Respondent's Age",
     breaks = c(18,30,45,60,75,100))
```


## Adjust the y-axis
  - `ylim = c(min,max)` 
```{r, eval=FALSE}
hist(gallup$age,freq=FALSE, xlab="Respondent's Age",
     main="Distribution of Gallup Respondent's Age",
     breaks = c(18,30,45,60,75,100), ylim = c(0,0.03)
     )
```

  
## Adjust the y-axis
- `ylim = c(min,max)` 

```{r, echo=FALSE}
hist(gallup$age,freq=FALSE, xlab="Respondent's Age",
     main="Distribution of Gallup Respondent's Age",
     breaks = c(18,30,45,60,75,100), ylim = c(0,0.03)
     )
```
  

# Textual Data and Machine Learning

## Textual Data
- Our datasets were mostly from human responses
  + Observation: individual
  + Variables: demographics, opinions, etc.
  + Surveys, survey experiments, voter files, etc.

- Textual data
  + Observation: tweets, facebook posts, news articles, congressional speeches, etc.
  + Variables: tone, ideology, civility, etc.
    + You can probably handle thousands of documents
    + But when you have millions of documents $\rightsquigarrow$ scraping and machine learning


## Perspective API

- https://www.perspectiveapi.com
- Supervised machine learning model that measures the toxicity of a document
- Trained on NYT and Wikipedia comments

## Machine Learning 

- Vast literature
- Supervised machine learning
  + Goal: Classify a large set of documents into some categories of interest (civil vs toxic)
  + How to do it
    - Randomly sample a **training set** from the entire set of documents
    - Human-label the training set
        + "Is this comment civil or uncivil?"
        + You're often hired to do it
    - Use the training set to **train a model**
        + Learn the relationship between unique words each doc contains and its classification
        + Model: Coverts frequencies of particular words into probability of a document belonging to a category (toxic)
    - Use the model to classify the rest of documents
    - Validate the model using another sample



## Facebook comments data

- Scraped 6M comments posted on all news articles from 34 news outlets' Facebook page (October 6-16, 2018)
- Coded each comment's toxicity using Perspective API
- Our data: randomly sampled 2000 comments from the entire data
- Variables
  + comment_text: FB comment
  + post_text: Title of news article
  + outlet: news outlet that published the article
  + outlet bias: average ideology of each outlet's audience (-10: very liberal, 0: neutral, 10: very conservative)
  + toxic: toxicity score from Perspective API

## Exercise
1. Compute the mean, median, standard deviation of toxicity
2. Create a histogram of toxicity
3. Create a bar plot for news outlets
4. Create a bar plot for news outlets, with top 10 outlets
5. Categorize comments into three groups (conservative outlets, neutral outlets, liberal outlets)
   + Conservative: 2 to 10
   + Neutral: -1.999 to 1.999
   + Liberal: -10 to -2
6. Compare average toxicity levels by the political leanings of outlets using the three categories
7. Create a boxplot to compare the means


## Question 1

Compute the mean, median, standard deviation of toxicity


## Question 1

Compute the mean, median, standard deviation of toxicity

```{r}
fb<-read.csv("fb.csv")
summary(fb$toxic)
sd(fb$toxic)
```



## Question 2
Create a histogram of toxicity



## Question 2
Create a histogram of toxicity
```{r}
hist(fb$toxic, freq = FALSE, xlab = "Toxicity of Comments", main = "Distribution of Toxicity of Facebook Comments")
```


## Question 3

Create a bar plot for news outlets

## Question 3

Create a bar plot for news outlets

```{r, fig.width=9}
outlet.tab<-table(fb$outlet)
barplot(outlet.tab, xlab = "News Outlet", main = "Distribution of News Outlets", ylim = c(0, 500))
```


## Question 4

Create a bar plot for news outlets, with top 10 outlets

## Question 4

Create a bar plot for news outlets, with top 10 outlets

```{r, fig.width=9}
top.outlet.tab<-sort(outlet.tab, decreasing = TRUE)[1:10]
barplot(top.outlet.tab, xlab = "News Outlet", main = "Distribution of News Outlets", ylim = c(0, 500))
```

## Question 5 

- Categorize comments into three groups (conservative outlets, neutral outlets, liberal outlets)
   + Conservative: 2 to 10
   + Neutral: -1.999 to 1.999
   + Liberal: -10 to -2

## Question 5 

- Categorize comments into three groups (conservative outlets, neutral outlets, liberal outlets)
   + Conservative: 2 to 10
   + Neutral: -1.999 to 1.999
   + Liberal: -10 to -2
   
```{r}
fb$outlet3[fb$outlet_bias>=2]<-"conservative"
fb$outlet3[fb$outlet_bias<2 & fb$outlet_bias>-2]<-"neutral"
fb$outlet3[fb$outlet_bias<=-2]<-"liberal"
```

## Q6

Compare average toxicity levels by the political leanings of outlets using the three categories

## Q6

Compare average toxicity levels by the political leanings of outlets using the three categories

```{r}
toxic.means<-tapply(fb$toxic,fb$outlet3,mean)
toxic.means
```

## Question 7

Create a boxplot to compare the means

## Question 7

Create a boxplot to compare the means

```{r, fig.height=4}
barplot(toxic.means,xlab="Political Leaning of News Outlet", 
        ylab = "Average Toxicity of Comments", 
        main = "Facebook comment toxicity by outlet ideology")
```


# CLASS 10


## Today's Agenda
- Finish up Facebook exercise
- Intro to `library`
- Recoding variables

## Housekeeping
```{r}
version
```

# Facebook exercise

## Question 5 

- Categorize comments into three groups (conservative outlets, neutral outlets, liberal outlets)
   + Conservative: 2 to 10
   + Neutral: -1.999 to 1.999
   + Liberal: -10 to -2
   
```{r}
fb<-read.csv("fb.csv")
fb$outlet3[fb$outlet_bias>=2]<-"conservative"
fb$outlet3[fb$outlet_bias<2 & fb$outlet_bias>-2]<-"neutral"
fb$outlet3[fb$outlet_bias<=-2]<-"liberal"
```

## Q6

Compare average toxicity levels by the political leanings of outlets using the three categories

## Q6

Compare average toxicity levels by the political leanings of outlets using the three categories

```{r}
toxic.means<-tapply(fb$toxic,fb$outlet3,mean)
toxic.means
```

## Question 7

Create a boxplot to compare the means

## Question 7

Create a boxplot to compare the means

```{r, fig.height=4}
barplot(toxic.means,xlab="Political Leaning of News Outlet", 
        ylab = "Average Toxicity of Comments", 
        main = "Facebook comment toxicity by outlet ideology")
```


# Data Manipulation 1: Recoding

## Data cleaning
- Raw data are almost never ready for statistical analyses
  - Variables coded in "wrong" ways $\checkmark$
  - Multiple partial datasets that should be merged into one data frame, etc.
- Typically, data cleaning will take up the lion's share of your time
- Essential step between collecting data and doing actual analyses

### Recoding
- Often you need to transform a variable to have different values
- Most typical situations: you want to...
  + Change scale of numeric variables
  + Convert numeric variables to factor variables, and vice versa
  + Combine responses to sequence of questions into a single variable

## Packages
- One of the strengths of R is the availability of numerous useful packages
- Packages are collection of functions that help you code, make graphs, etc.
- Use the `install.packages()` function to install, and the `library()` function to load
  + You don't need to install a package every time, but you need to load packages you're going to use after restarting R
  + Don't put the `install.packages()` function in your rmd file
- Install `tidyverse`, `haven`, and `labelled` which we'll use throughout this semester
```{r message=FALSE}
library(tidyverse) # extremely popular data manipulation/visualization tools
library(haven) # let you use Stata (.dta), SPSS (.sav) files 
library(labelled) # let you access data labels easily
```

## ANES
- American National Election Studies
- Nationally representative surveys conducted since 1948
- Publicly available at https://electionstudies.org/
- We will use its 2016 data ("anes.dta")
```{r}
anes<-read_dta("anes.dta")
dim(anes) # 1842 variables!
```

## Labelled data
- Most survey data contain "labels" (brief info about variables and their values)
- Use the `lookfor` (from the `labelled` package) function to search for keywords on labels
```{r}
look_for(anes,"obama",details = FALSE) # variables in anes including "obama" in their labels
```
- You can create a separate dataframe with a list of variables and labels by not specifying any keyword.
```{r}
anes.vars<-look_for(anes,"",details = FALSE)
```
- Labels give you stripped down info about variables.
- For more detailed information: see the "anes_timeseries_2016_userguidecodebook.pdf" file
  - Most publicly available survey data come with a "codebook"
  - Typically provides original questionnaires, coding rules, descriptive stats, etc.


## Labelled data
- Let's focus on `V161237` (affect for Obama: proud)
- use `head` function to access variable/value labels
  
```{r}
head(anes$V161237)
```



## Case 1: Changing scale 
- Responses on a 5-point scale $\rightsquigarrow$ 0-1 scale
- Approach 1: use `[]`
  - Perhaps the most intuitive way (not recommended but you can take this approach for HW)
```{r}
anes$obama.proud[anes$V161237==1]<-0
anes$obama.proud[anes$V161237==2]<-.25
anes$obama.proud[anes$V161237==3]<-.5
anes$obama.proud[anes$V161237==4]<-.75
anes$obama.proud[anes$V161237==5]<-1
```

## Case 1: Changing scale 
- Responses on a 5-point scale $\rightsquigarrow$ 0-1 scale
- Approach 2: use a formula after removing `NA`'s
  - First remove `NA`'s
```{r}
anes$obama.proud<-anes$V161237
anes$obama.proud[anes$obama.proud==-8|anes$obama.proud==-9]<-NA
table(anes$obama.proud)
```

## Case 1: Changing scale 
- Approach 2: use a formula after removing `NA`'s
  - Then re-scale using a formula
```{r}
anes$obama.proud<- (anes$obama.proud-1)/4 
# subtracting by 1 to make it range btw 0-4
# then dividing by 4 to scale it to 0-1
table(anes$obama.proud)
# 1 = always proud of obama; 0 = never proud of obama
```


## Case 1: Changing scale 
- Approach 3: use a formula with the `case_when()`function
  - `case_when(condition ~ new_value)`
```{r}
anes$obama.proud<- case_when(anes$V161237>=0 ~ (anes$V161237-1)/4 )
# observations with anes$V161237<0 will be NA's
```
- Or you can specify multiple conditions and corresponding values in `case_when`
  - `case_when(condition_1 ~ new_value_1, condition_2 ~ new_value_2, ...)`
```{r}
anes$obama.proud<- case_when(anes$V161237==5 ~ 1, # condition 1
                             anes$V161237==4 ~ .75, # condition 2
                             anes$V161237==3 ~ .5,
                             anes$V161237==2 ~ .25,
                             anes$V161237==1 ~ 0) 
```

## Exercise
- Inspect the `V161236` variable
- Re-scale the variable so that it ranges between 0 and 1

## Exercise
- Inspect the `V161236` variable
- Re-scale the variable so that it ranges between 0 and 1
```{r}

```

## Case 2: Changing scale & reverse coding
- Sometimes, you may want to flip the values of a variable
- You want to use `V161236` as an indicator of positive emotions for Obama 
- Let's reverse-code this variable such that...
  + 5 (angry always) $\rightsquigarrow$ 0, 
  + 4 (most of the times) $\rightsquigarrow$ .25, 
  ... 
  + 1 (never) $\rightsquigarrow$ 1
```{r}
anes$obama.angry.r[anes$V161236==5]<-0
anes$obama.angry.r[anes$V161236==4]<-0.25
anes$obama.angry.r[anes$V161236==3]<-0.5
anes$obama.angry.r[anes$V161236==2]<-0.75
anes$obama.angry.r[anes$V161236==1]<-1
```

## Case 2: Changing scale & reverse coding
- Alternative approach: use a formula `(5-V161236)/4`
```{r}
anes$obama.angry.r<-case_when(anes$V161236>0 ~ (5-anes$V161236)/4)
# observations satisfying anes$V161236>0 will get (5-V161236)/4
# subtracting 5 by obama.angry make it range btw 0-4 where 0 is "angry always" and 4 is "never"
# then divide by 4 to scale it to 0-1
table(anes$obama.angry.r)
tapply(anes$obama.proud, anes$obama.angry.r, mean, na.rm=T)
```



## Exercise

- `V162171`: 7pt scale liberal-conservative: self placement
  -*"Where would you place yourself on this scale, or havenâ€™t you thought much about this?"*

- Question 1: Create a new variable for liberal political ideology, where 1 indicates "extremely liberal" and 0 indicates "extremely conservative"
  - Treat `-9, -7, -6, 99` as missing


## Exercise

- `V162171`: 7pt scale liberal-conservative: self placement
  -*"Where would you place yourself on this scale, or havenâ€™t you thought much about this?"*

- Question 1: Create a new variable for liberal political ideology, where 1 indicates "extremely liberal" and 0 indicates "extremely conservative"
  - Treat `-9, -7, -6, 99` as missing
  
```{r}
anes$liberal<-case_when(anes$V162171>0 & anes$V162171<8~ (7-anes$V162171)/6)
table(anes$liberal)
```

## Exercise

- Question 2: Create a new variable for conservative political ideology, where 1 indicates "extremely conservative" and 0 indicates "extremely conservative"

## Exercise

- Question 2: Create a new variable for conservative political ideology, where 1 indicates "extremely conservative" and 0 indicates "extremely conservative"
```{r}
anes$conservative<-1-anes$liberal
```

## Exercise

- Question 3: Create a new variable for liberal political ideology, this time including the "Havenâ€™t thought much about this" response at the midpoint

## Exercise

- Question 3: Create a new variable for liberal political ideology, this time including the "Havenâ€™t thought much about this" response at the midpoint (0.5)
```{r}
anes$liberal<-case_when(anes$V162171>0 & anes$V162171<8~ (7-anes$V162171)/6,
                        anes$V162171==99~.5) # second condition to recode 99 into midpoint (0.5)
table(anes$liberal)
```

## Exercise
- Political efficacy: Citizens' belief that they can understand and affect public affairs.
- Measured with four items: `V162215, V162216, V162217, V162218`
- Use these variables to create four indicators of political efficacy, all of which vary between 0 and 1, such that 1 is the most efficacious belief

## Case 3: Numeric into Factor
- Surveys often code categorical variables in numbers
- You may want to covert them into factor variables with appropriate value labels
- Example: `V161270` (Level of Education)
```{r}
head(anes$V161270)
```

## Case 3: Numeric into Factor
- I want to convert this variable to a factor variable called "educ" with the following categories (in that order)
  + No High School Diploma
  + High School
  + Some College/ Associate Degree
  + BA
  + MA
  + Professional/PhD
  
## Case 3: Numeric into Factor
- You can use the indexing approach
```{r}
# maybe indexing is easier in this case?
anes$educ[anes$V161270>=1 & anes$V161270<=9]<-"no high"
anes$educ[anes$V161270==10 | anes$V161270==90]<-"high school"
anes$educ[anes$V161270>=11 & anes$V161270<=12]<-"some college"
anes$educ[anes$V161270==13]<-"ba"
anes$educ[anes$V161270==14]<-"ma"
anes$educ[anes$V161270==15|anes$V161270==16]<-"phd"
anes$educ<-factor(anes$educ, levels = c("no high","high school","some college","ba","ma","phd"))
```

## Case 3: Numeric into Factor
- using `case_when`
```{r}
anes$educ<-NA
anes$educ<-case_when(anes$V161270>=1 & anes$V161270<=9~"no high",
                     anes$V161270==10 | anes$V161270==90~"high school",
                     anes$V161270>=11 & anes$V161270<=12~"some college",
                     anes$V161270==13~"ba",
                     anes$V161270==14~"ma",
                     anes$V161270==15|anes$V161270==16~"phd")
anes$educ<-factor(anes$educ, levels = c("no high","high school","some college","ba","ma","phd"))
```

## Exercise

- `V161310x` (Respondent race)
- Turn this variable into a factor variable with race names

## Exercise

- V161310x (Respondent race)
- Turn this variable into a factor variable with race names
```{r}
# when your original variable is numeric, you can skip specifying original values
# Replacement is done based on its position, such that 1 = White, 2 = Black, ..
anes$race<-case_when(anes$V161310x==1~"White",
                     anes$V161310x==1~"Black",
                     anes$V161310x==2~"Asian",
                     anes$V161310x==3~"Native",
                     anes$V161310x==4~"Hispanic",
                     anes$V161310x==5~"Other")

table(anes$race) 
```

## Case 4: Factor into Numeric
- You may want to covert a factor/character variable to a numeric one
- Example: I want to compute approximate years of education from the educ variable
```{r}
anes$yoe<-case_when(anes$educ=="no high"~9,
                    anes$educ=="high school"~12,
                    anes$educ=="some college"~14, 
                    anes$educ=="ba"~16, 
                    anes$educ=="ma"~18,
                    anes$educ=="phd"~21)
table(anes$yoe)
```


## Case 5: Sequence of questions into a single variable
- Researchers often ask multiple questions to measure one variable
- Example: 7-point party identification scale 
  + `V161155` "Generally speaking, do you usually think of yourself as [a Democrat, a Republican / a Republican, a Democrat], an independent, or what?"
  + `V161156` IF R CONSIDERS SELF A DEMOCRAT: / IF R CONSIDERS SELF A REPUBLICAN : "Would you call yourself a strong [Democrat / Republican] or a not very strong [Democrat / Republican]?"
  + `V161157` IF Râ€™S PARTY IDENTIFICATION IS INDEPENDENT, NO PREFERENCE, OTHER, DK: "Do you think of yourself as closer to the Republican Party or to the Democratic Party?"
- I want to use these three variables to create a 7-point scale, where 
  + 1 = strong Rep, 2 = moderate Rep, 3 = "leaning" Rep, 4 = pure independent, 5 = "leaning" Dem=, 6 = moderate Dem, 7 = strong Dem.
  
## Case 5: Sequence of questions into a single variable
```{r}
anes$pid[anes$V161155==2&anes$V161156==1]<-1 # identify as Rep & strong
anes$pid[anes$V161155==2&anes$V161156==2]<-2 # identify as Rep & not very strong
anes$pid[anes$V161157==1]<-3 # independent & closer to the Rep party
anes$pid[anes$V161157==2]<-4 # independent & closer to neither
anes$pid[anes$V161157==3]<-5
anes$pid[anes$V161155==1&anes$V161156==2]<-6
anes$pid[anes$V161155==1&anes$V161156==1]<-7
```


## PID (Party Identification)
- In public opinion/political comm literature, researchers often compare Dems and Reps
- You may want to create a binary PID variable that omits pure independents
```{r}
anes$dem<-case_when(anes$pid>=5~0,
                    anes$pid<=3~1)
# Or..
anes$dem<-NA
anes$dem[anes$pid>=5]<-0
anes$dem[anes$pid<=3]<-1
table(anes$dem)
```

## Exercise

- Support for Obamacare (the ACA) was measured by three questions: `V161113`, `V161114a`, `V161114b`
- Create a 7-point scale for support for Obamacare
- Rescale it to vary between 0-1


# CLASS 11


## Today's Agenda
- Final project
- Recoding

## Housekeeping
- HW2 due tonight

# Final Project

## Survey exeriment
- We will design & run a new experiment
- Similar to the climate change experiment
  - The persuasive effects of some informational treatments on attitudes or intentions
  - Two or more conditions, including a control group
  - Outcome variables (e.g., opinions) measured before and after the treatment
  - Sample (n = 750) from Amazon's Mechanical Turk (online labor market)
- Potential topics
  - Climate change (how to convince people that CC is human-caused) $\checkmark$
  - COVID19 (is there some piece of info that could make people wear masks)
  - Get-out-the-vote (how to convince people to vote)
  - YOUR IDEAS?

## Send your ideas (not mandatory)
- Post on Canvas...
  - Topic and research question (How does X affect Y?)
  - Rough idea about X (messages that you will randomly provide to participants)
  - Outcome variables you have in mind
  - Comment on other students' proposals
- Rough ideas are okay!
  - Extra-credits on your participation score if you submit 
  - More credits if we choose your proposal $\checkmark$
  - Due by 10/11 (midnight)
- Once we settle on a topic and RQ, we will design the details of the study

## Data & Libraries
```{r}
library(tidyverse)
library(labelled)
library(haven)
anes<-read_dta("anes.dta")
```

## Case 1: Changing scale
- Inspect the `V161236` variable
- Re-scale the variable so that it ranges between 0 and 1
```{r}
anes$obama.angry<-case_when(anes$V161236>=0 ~ (anes$V161236-1)/4)
```

## Case 2: Changing scale & reverse coding
- Sometimes, you may want to flip the values of a variable
- You want to use `V161236` as an indicator of positive emotions for Obama 
- Let's reverse-code this variable such that...
  + 5 (angry always) $\rightsquigarrow$ 0, 
  + 4 (most of the times) $\rightsquigarrow$ .25, 
  + 3 (about half the time) $\rightsquigarrow$ .5,
  + 2 (some of the time) $\rightsquigarrow$ .75, 
  + 1 (never) $\rightsquigarrow$ 1
```{r warning=FALSE}
anes$obama.angry.r[anes$V161236==5]<-0
anes$obama.angry.r[anes$V161236==4]<-0.25
anes$obama.angry.r[anes$V161236==3]<-0.5
anes$obama.angry.r[anes$V161236==2]<-0.75
anes$obama.angry.r[anes$V161236==1]<-1
```

## Case 2: Changing scale & reverse coding
- Alternative approach: use a formula `(5-V161236)/4`
```{r}
anes$obama.angry.r<-case_when(anes$V161236>0 
    # observations satisfying this will get the following for obama.angry.r
                              ~ (5-anes$V161236)/4)
    # subtracting 5 by obama.angry make it range btw 0-4 where 0 is "angry always" and 4 is "never"
    # then divide by 4 to scale it to 0-1
table(anes$obama.angry.r)
```



## Exercise

- `V162171`: 7pt scale liberal-conservative: self placement
  -*"Where would you place yourself on this scale, or havenâ€™t you thought much about this?"*

- Question 1: Create a new variable for liberal political ideology, where 1 indicates "extremely liberal" and 0 indicates "extremely conservative"
  - Treat `-9, -7, -6, 99` as missing


## Exercise

- `V162171`: 7pt scale liberal-conservative: self placement
  -*"Where would you place yourself on this scale, or havenâ€™t you thought much about this?"*

- Question 1: Create a new variable for liberal political ideology, where 1 indicates "extremely liberal" and 0 indicates "extremely conservative"
  - Treat `-9, -7, -6, 99` as missing
  
```{r}
anes$liberal<-case_when(anes$V162171>0 & anes$V162171<8~ (7-anes$V162171)/6)
table(anes$liberal)
```

## Exercise

- Question 2: Create a new variable for conservative political ideology, where 1 indicates "extremely conservative" and 0 indicates "extremely conservative"

## Exercise

- Question 2: Create a new variable for conservative political ideology, where 1 indicates "extremely conservative" and 0 indicates "extremely conservative"
```{r}
anes$conservative<-1-anes$liberal
```

## Exercise

- Question 3: Create a new variable for liberal political ideology, this time including the "Havenâ€™t thought much about this" response at the midpoint

## Exercise

- Question 3: Create a new variable for liberal political ideology, this time including the "Havenâ€™t thought much about this" response at the midpoint (0.5)
```{r}
anes$liberal<-case_when(anes$V162171>0 & anes$V162171<8~ (7-anes$V162171)/6,
                        anes$V162171==99~.5) # second condition to recode 99 into midpoint (0.5)
table(anes$liberal)
```

## Exercise
- Political efficacy: Citizens' belief that they can understand and affect public affairs.
- Measured with four items: `V162215, V162216, V162217, V162218`
- Use these variables to create four indicators of political efficacy, all of which vary between 0 and 1, such that 1 is the most efficacious belief

## Case 3: Numeric into Factor
- Surveys often code categorical variables in numbers
- You may want to covert them into factor variables with appropriate value labels
- Example: `V161270` (Level of Education)
```{r}
head(anes$V161270)
```

## Case 3: Numeric into Factor
- I want to convert this variable to a factor variable called "educ" with the following categories (in that order)
  + No High School Diploma
  + High School
  + Some College/ Associate Degree
  + BA
  + MA
  + Professional/PhD
  
## Case 3: Numeric into Factor
- You can use the indexing approach
```{r}
# maybe indexing is easier in this case?
anes$educ[anes$V161270>=1 & anes$V161270<=9]<-"no high"
anes$educ[anes$V161270==10 | anes$V161270==90]<-"high school"
anes$educ[anes$V161270>=11 & anes$V161270<=12]<-"some college"
anes$educ[anes$V161270==13]<-"ba"
anes$educ[anes$V161270==14]<-"ma"
anes$educ[anes$V161270==15|anes$V161270==16]<-"phd"
anes$educ<-factor(anes$educ, levels = c("no high","high school","some college","ba","ma","phd"))
```

## Case 3: Numeric into Factor
- using `case_when`
```{r}
anes$educ<-NA
anes$educ<-case_when(anes$V161270>=1 & anes$V161270<=9~"no high",
                     anes$V161270==10 | anes$V161270==90~"high school",
                     anes$V161270>=11 & anes$V161270<=12~"some college",
                     anes$V161270==13~"ba",
                     anes$V161270==14~"ma",
                     anes$V161270==15|anes$V161270==16~"phd")
anes$educ<-factor(anes$educ, levels = c("no high","high school","some college","ba","ma","phd"))
```

## Exercise

- `V161310x` (Respondent race)
- Turn this variable into a factor variable with race names

## Exercise

- V161310x (Respondent race)
- Turn this variable into a factor variable with race names
```{r}
# when your original variable is numeric, you can skip specifying original values
# Replacement is done based on its position, such that 1 = White, 2 = Black, ..
anes$race<-case_when(anes$V161310x==1~"White",
                     anes$V161310x==1~"Black",
                     anes$V161310x==2~"Asian",
                     anes$V161310x==3~"Native",
                     anes$V161310x==4~"Hispanic",
                     anes$V161310x==5~"Other")

table(anes$race) 
```

## Case 4: Factor into Numeric
- You may want to covert a factor/character variable to a numeric one
- Example: I want to compute approximate years of education from the educ variable
```{r}
anes$yoe<-case_when(anes$educ=="no high"~9,
                    anes$educ=="high school"~12,
                    anes$educ=="some college"~14, 
                    anes$educ=="ba"~16, 
                    anes$educ=="ma"~18,
                    anes$educ=="phd"~21)
table(anes$yoe)
```


## Case 5: Sequence of questions into a single variable
- Researchers often ask multiple questions to measure one variable
- Example: 7-point party identification scale 
  + `V161155` "Generally speaking, do you usually think of yourself as [a Democrat, a Republican / a Republican, a Democrat], an independent, or what?"
  + `V161156` IF R CONSIDERS SELF A DEMOCRAT: / IF R CONSIDERS SELF A REPUBLICAN : "Would you call yourself a strong [Democrat / Republican] or a not very strong [Democrat / Republican]?"
  + `V161157` IF Râ€™S PARTY IDENTIFICATION IS INDEPENDENT, NO PREFERENCE, OTHER, DK: "Do you think of yourself as closer to the Republican Party or to the Democratic Party?"
- I want to use these three variables to create a 7-point scale, where 
  + 1 = strong Rep, 2 = moderate Rep, 3 = "leaning" Rep, 4 = pure independent, 5 = "leaning" Dem=, 6 = moderate Dem, 7 = strong Dem.
  
## Case 5: Sequence of questions into a single variable
```{r}
anes$pid[anes$V161155==2&anes$V161156==1]<-1 # identify as Rep & strong
anes$pid[anes$V161155==2&anes$V161156==2]<-2 # identify as Rep & not very strong
anes$pid[anes$V161157==1]<-3 # independent & closer to the Rep party
anes$pid[anes$V161157==2]<-4 # independent & closer to neither
anes$pid[anes$V161157==3]<-5
anes$pid[anes$V161155==1&anes$V161156==2]<-6
anes$pid[anes$V161155==1&anes$V161156==1]<-7
```


## PID (Party Identification)
- In public opinion/political comm literature, researchers often compare Dems and Reps
- You may want to create a binary PID variable that omits pure independents
```{r}
anes$dem<-case_when(anes$pid>=5~0,
                    anes$pid<=3~1)
# Or..
anes$dem<-NA
anes$dem[anes$pid>=5]<-0
anes$dem[anes$pid<=3]<-1
table(anes$dem)
```

## Exercise

- Support for Obamacare (the ACA) was measured by three questions: `V161113`, `V161114a`, `V161114b`
- Create a 7-point scale for support for Obamacare
- Rescale it to vary between 0-1



# CLASS 12


## Today's Agenda
- Recoding


## Housekeeping
- HW3 out tonight
- Due in 2 weeks
- Send your ideas for the final project



## Data & Libraries
```{r}
library(tidyverse)
library(labelled)
library(haven)
library(psych) # new
anes<-read_dta("anes.dta")
social<-read_csv("social.csv")
```

## Recoding
- Case 1: Re-scaling
- Case 2: Reverse coding

## Case 3: Numeric into Factor
- Surveys often code categorical variables in numbers
- You may want to covert them into factor variables with appropriate value labels
- Example: `V161270` (Level of Education)
```{r}
head(anes$V161270)
```

## Case 3: Numeric into Factor
- I want to convert this variable to a factor variable called "educ" with the following categories (in that order)
  + No High School Diploma
  + High School
  + Some College/ Associate Degree
  + BA
  + MA
  + Professional/PhD
  
## Case 3: Numeric into Factor
- You can use the indexing approach
```{r}
# maybe indexing is easier in this case?
anes$educ[anes$V161270>=1 & anes$V161270<=9]<-"no high"
anes$educ[anes$V161270==10 | anes$V161270==90]<-"high school"
anes$educ[anes$V161270>=11 & anes$V161270<=12]<-"some college"
anes$educ[anes$V161270==13]<-"ba"
anes$educ[anes$V161270==14]<-"ma"
anes$educ[anes$V161270==15|anes$V161270==16]<-"phd"
anes$educ<-factor(anes$educ, levels = c("no high","high school","some college","ba","ma","phd"))
```

## Case 3: Numeric into Factor
- using `case_when`
```{r}
anes$educ<-NA
anes$educ<-case_when(anes$V161270>=1 & anes$V161270<=9~"no high",
                     anes$V161270==10 | anes$V161270==90~"high school",
                     anes$V161270>=11 & anes$V161270<=12~"some college",
                     anes$V161270==13~"ba",
                     anes$V161270==14~"ma",
                     anes$V161270==15|anes$V161270==16~"phd")
anes$educ<-factor(anes$educ, levels = c("no high","high school","some college","ba","ma","phd"))
```

## Exercise
- Create a binary variable where 0 indicates non-college graduates and 1 indicates college graduates (those with a BA or higher degree)


## Exercise

- `V161310x` (Respondent race)
- Turn this variable into a factor variable with race names

## Exercise

- V161310x (Respondent race)
- Turn this variable into a factor variable with race names
```{r}
# when your original variable is numeric, you can skip specifying original values
# Replacement is done based on its position, such that 1 = White, 2 = Black, ..
anes$race<-case_when(anes$V161310x==1~"White",
                     anes$V161310x==2~"Black",
                     anes$V161310x==2~"Asian",
                     anes$V161310x==3~"Native",
                     anes$V161310x==4~"Hispanic",
                     anes$V161310x==5~"Other")

table(anes$race) 
```

## Case 4: Factor into Numeric
- You may want to covert a factor/character variable to a numeric one
- Example: I want to compute approximate years of education from the educ variable
```{r}
anes$yoe<-case_when(anes$educ=="no high"~9,
                    anes$educ=="high school"~12,
                    anes$educ=="some college"~14, 
                    anes$educ=="ba"~16, 
                    anes$educ=="ma"~18,
                    anes$educ=="phd"~21)
table(anes$yoe)
```


## Case 5: Sequence of questions into a single variable
- Researchers often ask multiple questions to measure one variable
- Example: 7-point party identification scale 
  + `V161155` "Generally speaking, do you usually think of yourself as [a Democrat, a Republican / a Republican, a Democrat], an independent, or what?"
  + `V161156` IF R CONSIDERS SELF A DEMOCRAT: / IF R CONSIDERS SELF A REPUBLICAN : "Would you call yourself a strong [Democrat / Republican] or a not very strong [Democrat / Republican]?"
  + `V161157` IF Râ€™S PARTY IDENTIFICATION IS INDEPENDENT, NO PREFERENCE, OTHER, DK: "Do you think of yourself as closer to the Republican Party or to the Democratic Party?"
- I want to use these three variables to create a 7-point scale, where 
  + 1 = strong Rep, 2 = moderate Rep, 3 = "leaning" Rep, 4 = pure independent, 5 = "leaning" Dem=, 6 = moderate Dem, 7 = strong Dem.
  
## Case 5: Sequence of questions into a single variable
```{r}
anes$pid[anes$V161155==2&anes$V161156==1]<-1 # identify as Rep & strong
anes$pid[anes$V161155==2&anes$V161156==2]<-2 # identify as Rep & not very strong
anes$pid[anes$V161157==1]<-3 # independent & closer to the Rep party
anes$pid[anes$V161157==2]<-4 # independent & closer to neither
anes$pid[anes$V161157==3]<-5
anes$pid[anes$V161155==1&anes$V161156==2]<-6
anes$pid[anes$V161155==1&anes$V161156==1]<-7
```


## PID (Party Identification)
- In public opinion/political comm literature, researchers often compare Dems and Reps
- You may want to create a binary PID variable that omits pure independents
```{r}
anes$dem<-case_when(anes$pid>=5~0,
                    anes$pid<=3~1)
# Or..
anes$dem<-NA
anes$dem[anes$pid>=5]<-0
anes$dem[anes$pid<=3]<-1
table(anes$dem)
```

## Exercise

- Support for Obamacare (the ACA) was measured by three questions: `V161113`, `V161114a`, `V161114b`
- Create a 7-point scale for support for Obamacare
- Rescale it to vary between 0-1




## Difference of two variables as a measure of a concept

- Sometimes, you may want to operationalize your variable as the difference between two variables
- Example: Racism against African Americans
  + Notoriously difficult concept to measure
  + One attempt: difference between racial stereotypes
    + Larger white vs. black gaps in your assessments &rarr; the more racially prejudiced you are
- `V162345`: Where would you rate Whites in general on this scale? (Hard-working to Lazy)
- `V162346`: Where would you rate Blacks in general on this scale? (Hard-working to Lazy)

## Difference of two variables as a measure of a concept

```{r}
anes$V162345[anes$V162345<0]<-NA # getting rid of NA's
anes$V162346[anes$V162346<0]<-NA # getting rid of NA's
summary(anes$V162345) # 1 = Hard working (Whites)
anes$racism1<-anes$V162346-anes$V162345 # Positive Difference: Belief that Whites are more hard working
```



## Exercise

- Create another indicator of racism using `V162349` and `V162350` (Racial stereotypes on violence)
- Calculate the mean of this variable
- Create a histogram of this variable
- Create a separate histogram of this variable for White respondents and Black respondents

## Exercise
- Using the `social` data, create a new variable measuring **change** in turnout between the 2004 and 2006 primaries



# CLASS 13


## Today's Agenda
- Final Project Update
- Scaling
- Inter-item reliability

## Housekeeping
- HW2 grading still in progress
- BlueJeans problem?


## Final Project
- Thank you for sharing your ideas!
- Privacy paradox: people say they're concerned about privacy, but act as if they're not
- How much do people actually value their online privacy?
  - Conventional way: asking a bunch of survey questions about privacy concerns
  - Our way: randomize (a) the sensitivity of survey questions and (b) monetary incentives
    - Factor 1: with or without sensitive private questions (health, sexual life, etc)
    - Factor 2: amount of compensation for sharing data (e.g., 0, 0.05, 0.5, 1)
  - Outcome: consent/object to share their answers with third party sources
- Why this topic?
  - 3 students suggested it
  - Feasible topic for an online experiment
  - Could be a novel contribution to the literature (i.e., publication)


## Next step
- Find relevant studies not just in comm but also in psych, econ, etc.
- Design the details of the experiment
- Need your help on these (participation score)


## Data & Libraries
```{r}
library(tidyverse)
library(labelled)
library(haven)
library(psych) # new
anes<-read_dta("anes.dta")
social<-read_csv("social.csv")
```


## Exercise

- Create another indicator of racism using `V162349` and `V162350` (Racial stereotypes on violence)
- Calculate the mean of this variable
- Create a histogram of this variable
- Create a separate histogram of this variable for White respondents and Black respondents

## Exercise
- Using the `social` data, create a new variable measuring **change** in turnout between the 2004 and 2006 primaries


# Scaling

## Multi-Item Scales
- Researchers often use multiple items to capture a single variable
  + Example: `racism1` (lazy-hardworking) & `racism2` (violent-peaceful)
- Each item is susceptible to measurement errors
  + Alternative: use many items to capture just one concept  
  + In theory, these errors should cancel each other out when averaged
  + Reduce the amount of chance errors, and maximize the amount of the true variation
  + Advisable to use multiple items when measuring your key outcome variables


## Cronbach's Alpha 
- Reliability of a multi-item scale
  + Items should be consistent among one another, inasmuch as they get at the same concept
- Cronbach's Alpha
  + Most widely used test of inter-item reliability 
  + We will not go over the formula
    + Stronger correlations among items &rarr; higher Cronbach's Alpha
    + More items &rarr; higher Cronbach's Alpha
    + .8 $\leq\alpha$: Good
    + .6 $\leq \alpha<$.8: okay
    +  $\alpha<$ .6: too low (conventional cutoff)

## Cronbach's Alpha example: perceived discrimination against minorities in the US
- Use the `alpha()` function from the `psych` package
- `alpha(data-subset)`
  - we should create a subset of our data with just the variables that we want for our reliability test
  - we can use the `select()` function to do that
    + `select(data, list of variables)`
```{r}
anes$disc1<-case_when(anes$V162357>0~(5-anes$V162357)/4)
anes$disc2<-case_when(anes$V162358>0~(5-anes$V162358)/4)
anes$disc3<-case_when(anes$V162359>0~(5-anes$V162359)/4)
anes$disc4<-case_when(anes$V162361>0~(5-anes$V162361)/4)
anes$disc5<-case_when(anes$V162362>0~(5-anes$V162362)/4)
anes$disc6<-case_when(anes$V162364>0~(5-anes$V162364)/4)
anes$disc7<-case_when(anes$V162366>0~(5-anes$V162366)/4)
```

## Cronbach's Alpha example: perceived discrimination against minorities in the US
- Use the `alpha()` function from the psych package
- `alpha(data-subset)`
  - we should create a subset of our data with just the variables that we want for our reliability test
  - we can use the `select()` function to do that
    + `select(data, list of variables)`
```{r}
alpha(select(anes,disc1,disc2,disc3,disc4,disc5,disc6,disc7)) #alpha = .89
alpha(select(anes,disc1:disc7)) #alpha = .89
```

## Scaling
- Measure a concept with multiple items
- Check the reliability of the scale: if Cronbach's alpha >.6
- Take the average of the items using the `rowMeans()` function
```{r fig.height = 3}
anes$disc<-rowMeans((select(anes,disc1:disc7)))
```



## Exercise 
- Check the reliability of the four-item scale of sexism
  + `V161507, V161508, V161509, V161510`
- Create a new variable called "sexism" in the `anes` data by computing the average among the four items
- Find the respondent gender variable using the `lookfor` function, and create separate histograms of sexism among male and female respondents

## Exercise 
- Check the reliability of the four-item scale of sexism
  + `V161507, V161508, V161509, V161510`
- Create a new variable called "sexism" in the `anes` data by computing the average among the four items
- Find the respondent gender variable using the `lookfor` function, and create separate histograms of sexism among male and female respondents
```{r fig.height=3}
anes$sexism1<-ifelse(anes$V161507>0, (5-anes$V161507)/4,NA)
anes$sexism2<-ifelse(anes$V161508>0, (5-anes$V161508)/4,NA)
anes$sexism3<-ifelse(anes$V161509>0, (5-anes$V161509)/4,NA)
anes$sexism4<-ifelse(anes$V161510>0, (5-anes$V161510)/4,NA)
alpha(select(anes,sexism1:sexism4))
```

## Exercise 

- Check the reliability of the four-item scale of sexism
  + `V161507, V161508, V161509, V161510`
- Create a new variable called "sexism" in the `anes` data by computing the average among the four items
- Find the respondent gender variable using the `lookfor` function, and create separate histograms of sexism among male and female respondents
```{r fig.height=3}
anes$sexism<-rowMeans(select(anes,sexism1:sexism4))
hist(anes$sexism[anes$V161342==1],
     main = "Distribution of sexism among men", xlab="")
```

## Exercise 

- Check the reliability of the four-item scale of sexism
  + `V161507, V161508, V161509, V161510`
- Create a new variable called "sexism" in the anes data where higher number means more sexist views
- Find the respondent gender variable using the lookfor function, and create separate histograms of sexism among male and female respondents
```{r fig.height=3}
hist(anes$sexism[anes$V161342==2],
     main = "Distribution of sexism among women",xlab="")
```


## Exercise 2
- There are many multi-item scales in the ANES data (e.g., sexism scale)
- Identify one from the ANES codebook (pdf file on Canvas) 
- Examine its reliability using the alpha function
- Create a new variable

# CLASS 14

## Today's Agenda
- Intro to Qualtrics
- Creating surveys on Qualtrics (group activity)
- Finalize on data manipulation

## Housekeeping
- HW2 was returned
- HW3 deadline extended to 10/18
- HW3 Correction: two variable names in Question 3 were wrong: should be `popen` and `preturn`, not `open` and `return`
- Syllabus updated



## Data & Libraries
```{r message=FALSE}
library(tidyverse)
library(labelled)
library(haven)
library(psych) 
anes<-read_dta("anes.dta")
atp1<-read_sav("ATP W1.SAV") # new
atp5<-read_sav("ATP W5.SAV") # new
statevotes<-read_csv("statevotes.csv")  # new
```



# Data collection on Qualtrics

## Qualtrics

- Qualtrics is an online service for creating and administering surveys
- Our initial survey was conducted on Qualtrics
- Penn students are licensed to use it
- https://upenn.qualtrics.com




## Group activity 
- Choose a topic for a survey to be conducted on Penn students. You may consider the following...
  + COVID-19
  + Future career
  + Campus culture
  + Political engagement/opinions
  + 2020 Elections
  + Social media, etc.
- Come up with a simple research question that can answered with 5-10 questions. For example...
  + Are Penn students living in "echo chambers"?
  + Is there a gender gap in expected earning among Penn students? etc.
- Write your topic and research question in the following google doc: https://docs.google.com/document/d/1aNo-4MqLTYpGJwccdSf2OKzL2DukmOyeqxLnwFCcJQ0/edit?usp=sharing

## Group activity 
- Write survey questions in the google doc
- Create a survey on Qualtrics and send me the link (one link per group)
- Also submit your Qualtrics export (qsf) file


## Group activity: next step
- I will combine the items into one survey
- YOU should complete the survey (send me your completion code)
- Then ask 4 friends (Penn students) to take the survey 
- I will ask other professors to distribute the survey
- We will analyze the data to answer your research questions 


# Data Manipulation 


## Exercise 
- Check the reliability of the four-item scale of sexism
  + `V161507, V161508, V161509, V161510`
- Create a new variable called "sexism" in the `anes` data by computing the average among the four items
- Find the respondent gender variable using the `lookfor` function, and create separate histograms of sexism among male and female respondents
  - FYI: When you use `:` in `select` R Recognizes which variable are between sexism1 and sexism4 by the order of the variables in the data (not alphabetical order)
```{r fig.height=3}
anes$sexism1<-ifelse(anes$V161507>0, (5-anes$V161507)/4,NA)
anes$sexism2<-ifelse(anes$V161508>0, (5-anes$V161508)/4,NA)
anes$sexism3<-ifelse(anes$V161509>0, (5-anes$V161509)/4,NA)
anes$sexism4<-ifelse(anes$V161510>0, (5-anes$V161510)/4,NA)
alpha(select(anes,sexism1:sexism4)) 
```

## Exercise 

- Check the reliability of the four-item scale of sexism
  + `V161507, V161508, V161509, V161510`
- Create a new variable called "sexism" in the `anes` data by computing the average among the four items
- Find the respondent gender variable using the `lookfor` function, and create separate histograms of sexism among male and female respondents
```{r fig.height=3}
anes$sexism<-rowMeans(select(anes,sexism1:sexism4))
hist(anes$sexism[anes$V161342==1],
     main = "Distribution of sexism among men", xlab="")
```

## Exercise 

- Check the reliability of the four-item scale of sexism
  + `V161507, V161508, V161509, V161510`
- Create a new variable called "sexism" in the anes data where higher number means more sexist views
- Find the respondent gender variable using the lookfor function, and create separate histograms of sexism among male and female respondents
```{r fig.height=3}
hist(anes$sexism[anes$V161342==2],
     main = "Distribution of sexism among women",xlab="")
```


## Exercise 2
- There are many multi-item scales in the ANES data (e.g., sexism scale)
- Identify one from the ANES codebook (pp.9-33) 
- Examine its reliability using the alpha function
- Create an index

## Merging

- Sometimes, you may have parts of data that you need to merge into one data
- Panel data: repeated surveys on the same individuals overtime
- Pewâ€™s American Trends Panel Datasets (on-going since 2014)
  + https://www.pewresearch.org/american-trends-panel-datasets/
- Interested in the relationship between two variables measured at different waves? You should merge them.
- Example: Are those concerned about digital privacy less likely to use social media?
  - Social media use: measured at Wave 1
  - Privacy concern: measured at Wave 5


## Merging
- Use the `full_join` function: `full_join(data1, data2, by = ID`)`
  - `ID` $\rightsquigarrow$ some unique number of string assigned to each individual
```{r, warning=FALSE}
atp1<-read_sav("ATP W1.SAV")
atp5<-read_sav("ATP W5.SAV")
pew<-full_join(atp1, atp5, by = "QKEY") #full_join() will merge all respondents into the new data
pew2 <-left_join(atp1, atp5, by = "QKEY") # left_join() will exclude those who were not in atp1
pew3 <-inner_join(atp1, atp5, by = "QKEY") # inner_join() will exclude those who were not in both waves
```

## Exercise
- Examine the relationship between privacy concern (`Q6_W5`) and Facebook use (`Q16_1_W1`)

## Exercise

- You can attach aggregate level data (e.g., state level information) to individual level data
- Merge the `statevotes` data (Trump vs Clinton vote shares in 2016 by state) to the ANES data
  + Use FIPS code to match the datasets
  + In ANES, FIPS is recorded in V163001a 
```{r, message=FALSE}
head(statevotes)
```

## Exercise
- You can attach aggregate level data (e.g., state level information) to individual level data
- Merge the "statevotes" data (Trump vs Clinton vote shares in 2016 by state) to the ANES data
  + Use FIPS code to match the datasets
  + In ANES, FIPS is recorded in V163001a 
```{r, warning=FALSE}
anes$fips<-anes$V163001a
anes<-full_join(anes,statevotes,"fips")
```


# CLASS 15


## Today's Agenda
- Finalize on data manipulation
- Election forecasting and voter turnout

## Homework 3
- Deadline extended to 10/25 (midnight)

## Final Project
- Contribute to literature search and experiment design
- Details on Canvas
- Credits for participation score


## Data & Libraries
```{r message=FALSE}
library(tidyverse)
library(labelled)
library(haven)
library(psych) 
anes<-read_dta("anes.dta")
atp1<-read_sav("ATP W1.SAV") 
atp5<-read_sav("ATP W5.SAV") 
statevotes<-read_csv("statevotes.csv") 
vote<-read_csv("vote.csv")
```


## Exercise 
- Check the reliability of the four-item scale of sexism
  + `V161507, V161508, V161509, V161510`
- Create a new variable called "sexism" in the `anes` data by computing the average among the four items
- Find the respondent gender variable using the `lookfor` function, and create separate histograms of sexism among male and female respondents
  - FYI: When you use `:` in `select` R Recognizes which variable are between sexism1 and sexism4 by the order of the variables in the data (not alphabetical order)
```{r fig.height=3}
anes$sexism1<-ifelse(anes$V161507>0, (5-anes$V161507)/4,NA)
anes$sexism2<-ifelse(anes$V161508>0, (5-anes$V161508)/4,NA)
anes$sexism3<-ifelse(anes$V161509>0, (5-anes$V161509)/4,NA)
anes$sexism4<-ifelse(anes$V161510>0, (5-anes$V161510)/4,NA)
alpha(select(anes,sexism1:sexism4)) 
```

## Exercise 

- Check the reliability of the four-item scale of sexism
  + `V161507, V161508, V161509, V161510`
- Create a new variable called "sexism" in the `anes` data by computing the average among the four items
- Find the respondent gender variable using the `lookfor` function, and create separate histograms of sexism among male and female respondents
```{r fig.height=3}
anes$sexism<-rowMeans(select(anes,sexism1:sexism4))
hist(anes$sexism[anes$V161342==1],
     main = "Distribution of sexism among men", xlab="")
```

## Exercise 

- Check the reliability of the four-item scale of sexism
  + `V161507, V161508, V161509, V161510`
- Create a new variable called "sexism" in the anes data where higher number means more sexist views
- Find the respondent gender variable using the lookfor function, and create separate histograms of sexism among male and female respondents
```{r fig.height=3}
hist(anes$sexism[anes$V161342==2],
     main = "Distribution of sexism among women",xlab="")
```


## Exercise 2
- There are many multi-item scales in the ANES data (e.g., sexism scale)
- Identify one from the ANES codebook (pp.9-33) 
- Examine its reliability using the alpha function
- Create an index

## Merging

- Sometimes, you may have parts of data that you need to merge into one data
- Panel data: repeated surveys on the same individuals overtime
- Pewâ€™s American Trends Panel Datasets (on-going since 2014)
  + https://www.pewresearch.org/american-trends-panel-datasets/
- Interested in the relationship between two variables measured at different waves? You should merge them.
- Example: Are those concerned about digital privacy less likely to use social media?
  - Social media use: measured at Wave 1
  - Privacy concern: measured at Wave 5


## Merging
- Use the `full_join` function: `full_join(data1, data2, by = ID`)`
  - `ID` $\rightsquigarrow$ some unique number of string assigned to each individual
```{r, warning=FALSE}
atp1<-read_sav("ATP W1.SAV")
atp5<-read_sav("ATP W5.SAV")
pew<-full_join(atp1, atp5, by = "QKEY") #full_join() will merge all respondents into the new data
pew2 <-left_join(atp1, atp5, by = "QKEY") # left_join() will exclude those who were not in atp1
pew3 <-inner_join(atp1, atp5, by = "QKEY") # inner_join() will exclude those who were not in both waves
```

## Exercise
- Examine the relationship between privacy concern (`Q6_W5`) and Facebook use (`Q16_1_W1`)

## Exercise

- You can attach aggregate level data (e.g., state level information) to individual level data
- Merge the `statevotes` data (Trump vs Clinton vote shares in 2016 by state) to the ANES data
  + Use FIPS code to match the datasets
  + In ANES, FIPS is recorded in V163001a 
```{r, message=FALSE}
head(statevotes)
```

## Exercise
- You can attach aggregate level data (e.g., state level information) to individual level data
- Merge the "statevotes" data (Trump vs Clinton vote shares in 2016 by state) to the ANES data
  + Use FIPS code to match the datasets
  + In ANES, FIPS is recorded in V163001a 
```{r, warning=FALSE}
anes$fips<-anes$V163001a
anes<-full_join(anes,statevotes,"fips")
```



# Election forecasting and turnout

## Background
- Election forecasts were covered A LOT in 2016
- Many people checked out sites like 538 constantly; they probably still do 
- Does election forecasting demobilize voters?
  - You may go vote if you thought it's close
  - You may be less willing if you thought the outcome was all but certain
  - In 2016, models gave Clinton a 70% to 99% change
- Westwood, Messing and Lelkes (2020), "Projecting Confidence: How the Probabilistic Horse Race Confuses and Demobilizes the Public," *Journal of Politics*

## Experimental Design
- People participated in a voting game
  - If your team wins you win $2
  - If your team loses you lose $2
  - You can pay $1 to cast a vote and increase the change of winning 
    - Even if you don't vote you win/lose $2 depending on whether your team wins
- Treatment: Randomized the chance that your team wins (0.01% to 90%)
- Outcome: voted or not
- **Hypothesis**: people will be less likely to vote if the change of winning is distant from 50/50 (in either direction)

## Exercise
- Recode the `probability` variable into a new variable measuring the distance from even odds (.5). For example...
  - .5 $\rightsquigarrow$ 0
  - .75 $\rightsquigarrow$ .25
  - .25 $\rightsquigarrow$ .25
  - .9 $\rightsquigarrow$ .4
  - .1 $\rightsquigarrow$ .4 , and so on...
- Using the `abs()` function could be useful

## Exercise
- Recode the distance variable to classify people into the following 6 categories
  - distance = 0 (assigned with 50/50)
  - 0 < distance $\le$ .1  
  - 1 < distance $\le$ .2  
  - 2 < distance $\le$ .3  
  - 3 < distance $\le$ .4  
  - 4 < distance $\le$ .5  

## Exercise
- Compare voting (`voted`) by the 6 categories


## Implication
- The perception that the election outcome is all but certain can substantially demoralize voters
- Democrats are more likely to follow these sites + liberal outlets cover them more frequently
- Would Trump have been elected if there wore no probabilistic forecasting?

# CLASS 16


## Today's Agenda

- Predicting presidential elections
- Loops
- Evaluating predictions

## Logistics
- Questions about HW3?
  - Pay attention to non-responses (e.g., -9,-8, 999, etc)
  - When turning those values into `NA` don't use quotation marks
- Thank you for contributing to the final project!
  - I will start programming the experiment
  - Once we settle on the design, we will submit the plan to the IRB (fingers crossed)
  - The experiment will be conducted in November, funded by the Annenberg school ($1,000)

## Data
```{r, message=FALSE}
# Libraries
library(tidyverse)
library(haven)
# Datasets
polls<-read_csv("polls.csv")
pres<-read_csv("pres.csv")
```



# Prediction

## Where are headed?

- Up to now: two uses for statistics in research
  + Causality: how one thing affects another 
  + Measurement: amorphous concept $\rightsquigarrow$ data
- Now: third use of statistics 
- **Prediction**: making a best guess about unknown quantity using data. 


## Predicting presidential elections
- Each poll has a lot of chance errors (even if n >1,000)
- When you take the average of multiple polls, chance errors cancel each other out  $\rightsquigarrow$ more precise prediction
- Another complication: Electoral college system
  + Must win an absolute majority of 538 electoral votes 
  + Must win at least 270 votes 
- Must predict winner of each state, and then aggregate EC votes

## Prediction Strategy
- Predict state-level support for each candidate using polls 
- Allocate electoral college votes of that state to its predicted winner 
- Aggregate EC votes across states to determine the predicted winner 
- Coding strategy:

  1. Subset polls by state.
  2. Further subset the latest polls
  3. Average the latest polls to estimate support for each candidate
  4. Allocate the electoral votes to the candidate who has greatest support
  5. Repeat this for all states and aggregate the electoral votes

- Sounds like a lot of subsets, ughâ€¦

## Loops

```{r}
values <- c(2, 4, 6)
```
- Letâ€™s say you want to create a new variable that multiplies each value in a vector by 2.
- Easy in R: `values * 2` 
- Pretend you didnâ€™t know this 

## Manually changing values
 
```{r}
values <- c(2, 4, 6)
results<- rep(NA,3)
## number of values n <- length(values)

## create container to hold results results <- rep(NA, times = n)

## multiply each value by 2
results[1] <- values[1] * 2
results[2] <- values[2] * 2
results[3] <- values[3] * 2

## print results 
results
```


## Loops in R

- Basic structure
```{r}
# for (i in X) {
#   expression1
#   expression2
#   expression3
#   ...
# }

```

- Elements of a loop:
  1. `i`: counter that controls the iterations in the loop.
  2. `X`: vector containing a set of ordered values the counter takes (e.g., 1 through 10).
  3. `expression`: a set of expressions that will be repeatedly evaluated.
  4. `{ }`: curly braces to define beginning and end of the loop.
  
## Loops in R (example)

```{r}
values <- c(2, 4, 6)
## create container to hold results 
results <- rep(NA, 3)
## begin loop
for (i in 1:3) {
  results[i] <- values[i] * 2
}
results
```

# 2012 polling prediction

## Prepwork
```{r}
head(polls) # state polls conducted in each state
head(pres) # election outcomes in each state
class(polls$middate)
polls$daysleft<-as.Date("2008-11-04")-polls$middate 
# as.Date function converts a character string in "year-month-day" format into a Date object
# you can compute difference between two date objects
polls$margin<-polls$Obama-polls$McCain
pres$margin<-pres$Obama-pres$McCain
```


## Prepwork
```{r}
class(polls$middate)
polls$daysleft<-as.Date("2008-11-04")-polls$middate 
# as.Date function converts a character string in "year-month-day" format into a Date object
# you can compute difference between two date objects

polls$margin<-polls$Obama-polls$McCain
pres$margin<-pres$Obama-pres$McCain
```
## Poll prediction using all polls
- We will calculated the predicted vote margins in each state based on the polls 
- And save that as a new variable in `pres`
```{r}
pres$predict<-NA # new "container" variable

for (i in 1:51) {
  
  # subsetting polls into each state
  state.data <- subset(polls, state == pres$state[i]) 
  
  # mean of margin for each state for latest polls
  pres$predict[i] <- mean(state.data$margin)
}
head(pres)
```

## Long way to do this would be...
```{r}
pres$prd<-NA
AL<-subset(polls,state=="AL")
pres$prd[1]<-mean(AL$margin)

AK<-subset(polls,state=="AK")
pres$prd[2]<-mean(AK$margin)

AR<-subset(polls,state=="AR")
pres$prd[3]<-mean(AR$margin)

# Repeat this 48 times more!
```

## Poll prediction using latest polls
```{r}
pres$predict2<-NA
for (i in 1:51) {
  # subsetting polls into each setate
  state.data <- subset(polls, state == pres$state[i]) 
  # mean of margin for each state for latest polls
  pres$predict2[i] <- mean(state.data$margin[state.data$daysleft == min(state.data$daysleft)])
}
```


## Evaluating the predictions

- prediction error = actual outcome - predicted outcome

```{r}
errors<-pres$margin - pres$predict2
names(errors)<-pres$state
mean(errors) # bias: average prediction error
sqrt(mean(errors^2)) # root mean-square error: average magnitude of error
```

## Histogram of errors
```{r}
hist(errors, main = "2012 Election Poll Prediction Error",
     xlab = "Error in Obama's margin of victory in each state",
     freq = F)
```

## Histogram of errors
```{r}
hist(errors, main = "2012 Election Poll Prediction Error",
     xlab = "Error in Obama's margin of victory in each state",
     freq = F)
abline(v=0, col="red") 
# you can add a straight line to a plot using the abline() function
```

## Electoral College

- Election prediction: need to sum up electoral votes won by Obama
```{r}
sum(pres$EV[pres$margin>0]) # actual electoral votes
sum(pres$EV[pres$predict>0]) # predicted electoral votes (all polls)
sum(pres$EV[pres$predict2>0]) # predicted electoral votes (latest polls)
```
- Polls indicated that Obama would won
- Using latest polls, we were able to get a closer number

## Accuracy rates

```{r}
mean(sign(pres$margin)==sign(pres$predict))
mean(sign(pres$margin)==sign(pres$predict2))# both had 94% accuracy rate
```

- Which state did polls call wrong?

```{r}
pres$state[sign(pres$margin)!=sign(pres$predict)]
pres$state[sign(pres$margin)!=sign(pres$predict2)] # first approach made a large error, because it called FL wrong
```


## How does 538 differ?


- What we did is the core idea behind election forecasters like 538.
- Of course, Nate Silver's models are much more complicated.
- What do they do differently?
  + Use a longer history of polls but down-weight older polls. 
  + Up-weight/down-weight polls from polling ï¬rms with low/high past prediction error.
  + Up-weight polls with better methodologies.
  + Combine poll-based predictions with predictions based on â€œfundamentalsâ€ like economic performance, popularity of the incumbent president.


# CLASS 17


## Quick announcement
- No class on 11/3 (election day)

## Today's Agenda

- Intro to Linear Regression 
  - Scatter plots
  - Correlation and R-squared
- Intro to the ggplot2 package
  + part of the tidyverse package


## Data and Libraries

```{r message=FALSE}
library(tidyverse)
library(ggrepel) # new
face<- read_csv("face.csv")
mask<- read_csv("mask.csv")
symptom<-read_csv("symptom.csv")
pop<-read_csv("pop.csv")
```

# Intro to Regression

## Where are we? Where are we going?

- So far: manipulating data to create/reshape variables
  + Essentially, prep for regression
- Now: how can we use one variable to predict or explain another?
- The single most essential tool in data analysis: linear regression

# Prediction using a second variable

## Example: Facial appearance and election outcome
- Background: The importance of appearance in elections
- Students saw head shots of two candidates from a US congressional election (less than 1 sec)
- Evaluated perceived competence
- Question: Do these snap judgements predict election outcomes?
  + Facial competence: independent variable (AKA explanatory variable, predictor)
  + Election outcome: dependent variable (AKA outcome variable)
-
![](face.png){width=50%}

## Data

- face.csv
```{r}
head(face)
```
- Key variables
  + d.comp/r.comp: competence measure for the Dem/Rep candidate (student rating)
  + d.votes/r.votes: number of votes for the Dem/Rep candidate (election outcome)

## Prep: creating the outcome variable
```{r}
face$d.share<-face$d.votes/(face$d.votes+face$r.votes) # share votes for Dem candidates
face$r.share<-face$r.votes/(face$d.votes+face$r.votes) 
face$diff.share<-face$d.share-face$r.share # vote share margin
```


## Plotting the data using ggplot
- ggplot2: extremely popular data visualization package
- Three key components
  + Data: the raw material you want to display
  + Aesthetics: mapping of your data
  + Layers: graphics to be displayed (scatter plot, line, barplot, etc)
```{r fig.height=3, fig.width=3}
ggplot(face,aes(x=d.comp,y=diff.share))+geom_point()
```

## Tweaking the scatter plot
```{r fig.height=3.5, fig.width=3.5}
ggplot(face,aes(x=d.comp,y=diff.share))+
  geom_point(size=2,alpha=0.5)+ # modifying size of dots (size) and color transparency (alpha)
  xlim(0,1)+ylim(-1,1)+ # defining the limits of x- and y-axes
  labs(x="Competence scores for Democrats",y="Democratic margin in vote share", 
       title = "Facial competence and vote share")+
  theme_bw() # you can customize plots using theme() function;
```

# Exercise: Mask wearing and COVID19

## "A powerful argument for wearing a mask, in visual form" (Washington Post)

- Article: https://wapo.st/3kAN6qc
  - What does the chart tell you?
  - What does it NOT tell you?
- Data: https://bit.ly/3os7vjI
  - I downloaded the latest data, and manipulated them a bit to skip some tedious steps
  - (You should be able to do it yourself)


## In-class exercise: updating the WP chart 
```{r}
head(mask)
head(symptom)
```

## In-class exercise: updating the WP chart 

- Step 1: merge the two data into one
  - HINT: use the `full_join (data1, data2, by = "ID")` function
```{r}
```

## In-class exercise: updating the WP chart 

- Step 2: create a basic scatter plot using the merged data (w/o any options specified)
```{r}
```

## In-class exercise: updating the WP chart 

- Step 3: change the labels, and make other stylistic changes if you want

## In-class exercise: updating the WP chart 

- Step 3: change the labels, and make other stylistic changes if you want
```{r fig.show='hide'}
covid<-full_join(mask,symptom,by="geo_value")
ggplot(covid,aes(x=mask.wearing,y=covid.symptom))+
  geom_point(size=4,alpha=0.5)+ 
  labs(x="% of people wearing masks in public",y="% of people know someone with covid19 symptoms", 
       title = "Fewer covid-19 symptoms in states with higher rates of mask use")+
  theme_bw() 
```


## In-class exercise: updating the WP chart 

- Step 4: Let's display state names instead of dots
  - specify the variable the labels in the `aes(x =, y =, label=)` argument
  - use the `geom_text` function instead of `geom_point`
```{r fig.show='hide'}
covid<-full_join(mask,symptom,by="geo_value")
ggplot(covid,aes(x=mask.wearing,y=covid.symptom))+
  geom_text(aes(label = geo_value))+ 
  labs(x="% of people wearing masks in public",y="% of people know someone with covid19 symptoms", 
       title = "Fewer covid-19 symptoms in states with higher rates of mask use")+
  theme_bw() 
```

## In-class exercise: updating the WP chart

- Step 4: Let's display state names instead of dots
  - specify the variable the labels in the `aes(x =, y =, label=)` argument
  - use the `geom_text` function instead of `geom_point`
    - drop overlapping labels by setting `check_overlap = T` in `geom_text`
```{r fig.show='hide'}
covid<-full_join(mask,symptom,by="geo_value")
ggplot(covid,aes(x=mask.wearing,y=covid.symptom))+
  geom_text(aes(label = geo_value), check_overlap= T)+ 
  labs(x="% of people wearing masks in public",y="% of people know someone with covid19 symptoms", 
       title = "Fewer covid-19 symptoms in states with higher rates of mask use")+
  theme_bw() 
```
## In-class exercise: updating the WP chart 

- Step 4: Let's display state names instead of dots
  - specify the variable the labels in the `aes(x =, y =, label=)` argument
  - use the `geom_text` function instead of `geom_point`
    - drop overlapping labels by setting `check_overlap = T` in `geom_text`
    - you may use the `ggrepel` package to deal with the overlapping values, forcing them to repel away from one another
```{r fig.show='hide'}
covid<-full_join(mask,symptom,by="geo_value")
ggplot(covid,aes(x=mask.wearing,y=covid.symptom))+
  geom_text_repel(aes(label = geo_value))+ 
  labs(x="% of people wearing masks in public",y="% of people know someone with covid19 symptoms", 
       title = "Fewer covid-19 symptoms in states with higher rates of mask use")+
  theme_bw() 
```
## In-class exercise: updating the WP chart 

- Step 4: Let's display state names instead of dots
  - specify the variable the labels in the `aes(x =, y =, label=)` argument
  - use the `geom_text` function instead of `geom_point`
    - drop overlapping labels by setting `check_overlap = T` in `geom_text`
    - you may use the `ggrepel` package to deal with the overlapping values, forcing them to repel away from one another
    - perhaps capitalize the state abbreviations using the `toupper` function
```{r fig.show='hide'}
covid<-full_join(mask,symptom,by="geo_value")
ggplot(covid,aes(x=mask.wearing,y=covid.symptom))+
  geom_text_repel(aes(label = toupper(geo_value)))+ 
  labs(x="% of people wearing masks in public",y="% of people know someone with covid19 symptoms", 
       title = "Fewer covid-19 symptoms in states with higher rates of mask use")+
  theme_bw() 
```

## In-class exercise: updating the WP chart 

- More Options: Vary the size of the points by state population

  - First, merge the `pop` data to the `covid` data
```{r fig.show='hide'}
covid<-full_join(covid,pop,by="geo_value")
```

## In-class exercise: updating the WP chart 
- More Options: Vary the size of the points by state population
  - First, merge the `pop` data to the `covid` data
  - define the `size` of points as `population` in `aes()`
    - don't specify `size` in `geom_point()`, which will override the aes argument
```{r fig.show='hide'}
ggplot(covid,aes(x=mask.wearing,y=covid.symptom, size = population))+
  geom_point()+ 
  labs(x="% of people wearing masks in public",y="% of people know someone with covid19 symptoms", 
       title = "Fewer covid-19 symptoms in states with higher rates of mask use")+
  theme_bw() 
```


## In-class exercise: updating the WP chart 
- More Options: Vary the size of the points by state population
  - First, merge the `pop` data to the `covid` data
  - define the `size` of points as `population` in `aes()`
    - don't specify `size` in `geom_point()`, which will override
    - you can turn off scientific notation in `R` by `options(scipen=999)`
```{r fig.show='hide'}
options(scipen=999)
ggplot(covid,aes(x=mask.wearing,y=covid.symptom, size = population))+
  geom_point()+ 
  labs(x="% of people wearing masks in public",y="% of people know someone with covid19 symptoms", 
       title = "Fewer covid-19 symptoms in states with higher rates of mask use")+
  theme_bw() 
```

## In-class exercise: updating the WP chart 
- More Options: Vary the size of the points by state population
  - First, merge the `pop` data to the `covid` data
  - define the `size` of points as `population` in `aes()`
    - don't specify `size` in `geom_point()`, which will override
    - you can turn off scientific notation in R by `options(scipen=999)`
```{r fig.show='hide'}
ggplot(covid,aes(x=mask.wearing,y=covid.symptom, size = population))+
  geom_point()+ 
  labs(x="% of people wearing masks in public",y="% of people know someone with covid19 symptoms", 
       title = "Fewer covid-19 symptoms in states with higher rates of mask use")+
  theme_bw() 
```

## In-class exercise: updating the WP chart 
- More Options: Vary the color of the points by state population
  - define the `color` of points as `population` in `aes()`
  - use the `scale_colour_continous()` option to map colors for low/high values
```{r fig.show='hide'}
ggplot(covid,aes(x=mask.wearing,y=covid.symptom, color = population))+
  geom_point()+ 
  labs(x="% of people wearing masks in public",y="% of people know someone with covid19 symptoms", 
       title = "Fewer covid-19 symptoms in states with higher rates of mask use")+
  scale_colour_continuous(low = "grey", high = "black")+ 
  theme_bw() 
```


## In-class exercise: updating the WP chart 
- More Options: Vary both the color and size of the points by state population
  - define both `size` and `color` in `aes()`
```{r fig.show='hide'}
ggplot(covid,aes(x=mask.wearing,y=covid.symptom, size = population, color = population))+
  geom_point()+ 
  labs(x="% of people wearing masks in public",y="% of people know someone with covid19 symptoms", 
       title = "Fewer covid-19 symptoms in states with higher rates of mask use")+
  scale_colour_continuous(low="grey",high="black")+ 
  theme_bw() 
```



## In-class exercise: updating the WP chart 
- More Options: Vary both the color and size of the points by state population
  - define both `size` and `color` in `aes()`
    - integrate the two guides into one using the `guides` option 
```{r fig.show='hide'}
ggplot(covid,aes(x=mask.wearing,y=covid.symptom, size = population, color = population))+
  geom_point()+ 
  labs(x="% of people wearing masks in public",y="% of people know someone with covid19 symptoms", 
       title = "Fewer covid-19 symptoms in states with higher rates of mask use")+
  scale_colour_continuous(low="grey",high="black")+ 
  guides(color=guide_legend(), size = guide_legend())+ # integrating guides
  theme_bw()
```



## Correlations
- Correlation: measure of the strength of the relationship between two variables
  - Pearson's *r*:  how many SDs of your dependent variable changes when your IV changes by one SD
    - $r = 1$: perfect positive correlation
    - $r = -1$: perfect negative correlation
    - $r = 0$ : no correlation
  - R-squred: Pearson's *r* squared $\rightsquigarrow$ % of the variabce of your DV (i.e., outcome) that you can predict with your IV (i.e., predictor)
```{r}
r.face<-cor(face$d.comp, face$diff.share) # Pearson's r
r.face
r2.face<-r.face^2
r2.face
```

## In-class exercise
  - Caclulate the correlation between mask-wearing and covid symptoms
  - Calculate the R-squred between mask and covid
```{r}
```



## Correlation and data clouds (more examples)
```{r echo=F, fig.width=5}
par(mfrow = c(2, 2), cex = 0.6)
N <- 100
x1 <- rnorm(N)
y1 <- x1/15 + rnorm(N)
y2 <- x1 + rnorm(N) / 2
y3 <- -x1 + rnorm(N)
y4 <- -2.5 + x1^2 + rnorm(N) / 10
plot(x1, y1, xlim = c(-3, 3), ylim = c(-3, 3), xlab = "", ylab = "",
     main = paste("(a) correlation =", round(cor(x1, y1), 2)))
plot(x1, y2, xlim = c(-3, 3), ylim = c(-3, 3), xlab = "", ylab = "",
     main = paste("(b) correlation =", round(cor(x1, y2), 2)))
plot(x1, y3, xlim = c(-3, 3), ylim = c(-3, 3), xlab = "", ylab = "",
     main = paste("(c) correlation =", round(cor(x1, y3), 2)))
plot(x1, y4, xlim = c(-3, 3), ylim = c(-3, 3), xlab = "", ylab = "",
     main = paste("(d) correlation =", round(cor(x1, y4), 2)))
```
- positive corr $\rightsquigarrow$ upward slope  
- negative corr $\rightsquigarrow$ downward slope  
- strong corr $\rightsquigarrow$ tighter, closer to a line  
- correlation can't capture nonlinear relationship   
  
# Linear Regression

## Regression: Finding the line that fits best
```{r, echo=F, message=F, fig.width=6, fig.height=5, fig.align='center'}
library(patchwork)
m1<-lm(diff.share~d.comp,face)
g1<-ggplot(face,aes(x=d.comp,y=diff.share))+
  geom_point(size=2,alpha=0.5)+ # modifying size of dots (size) and color transparency (alpha)
  xlim(0,1)+ylim(-1,1)+ # defining the limits of x- and y-axes
  labs(x="Competence scores for Democrats",y="Democratic margin", title = "(A)")+
  geom_abline(intercept=coef(m1)[1],slope=coef(m1)[2],color="red4",size=1.5,alpha=0.8)+
  theme_bw()+
  theme(panel.grid.major=element_blank(),
        panel.grid.minor=element_blank(),
        panel.border=element_rect(colour="black"))
g2<-ggplot(face,aes(x=d.comp,y=diff.share))+
  geom_point(size=2,alpha=0.5)+ # modifying size of dots (size) and color transparency (alpha)
  xlim(0,1)+ylim(-1,1)+ # defining the limits of x- and y-axes
  labs(x="Competence scores for Democrats",y="Democratic margin", title = "(B)")+
  geom_abline(intercept=-coef(m1)[1],slope=-coef(m1)[2],color="red4",size=1.5,alpha=0.8)+
  theme_bw()+
  theme(panel.grid.major=element_blank(),
        panel.grid.minor=element_blank(),
        panel.border=element_rect(colour="black"))
g3<-ggplot(face,aes(x=d.comp,y=diff.share))+
  geom_point(size=2,alpha=0.5)+ # modifying size of dots (size) and color transparency (alpha)
  xlim(0,1)+ylim(-1,1)+ # defining the limits of x- and y-axes
  labs(x="Competence scores for Democrats",y="Democratic margin", title = "(C)")+
  geom_abline(intercept=0,slope=0,color="red4",size=1.5,alpha=0.8)+
  theme_bw()+
  theme(panel.grid.major=element_blank(),
        panel.grid.minor=element_blank(),
        panel.border=element_rect(colour="black"))
g4<-ggplot(face,aes(x=d.comp,y=diff.share))+
  geom_point(size=2,alpha=0.5)+ # modifying size of dots (size) and color transparency (alpha)
  xlim(0,1)+ylim(-1,1)+ # defining the limits of x- and y-axes
  labs(x="Competence scores for Democrats",y="Democratic margin", title = "(D)")+
  geom_abline(intercept=-1,slope=2,color="red4",size=1.5,alpha=0.8)+
  theme_bw()+
  theme(panel.grid.major=element_blank(),
        panel.grid.minor=element_blank(),
        panel.border=element_rect(colour="black"))
(g1+g2)/(g3+g4)
```
- Which one is the best, and why?


# CLASS 18

## Housekeeping
- No class on next Tuesday
- Go vote!

## Data
```{r message=F}
library(tidyverse)
library(haven)
library(labelled)
anes<-read_dta("anes.dta")
face<-read_csv("face.csv")
covid<-read_csv("covid.csv")
stat<-read_csv("stat.csv")
```

## Go Vote!
```{r message=F}
anes$turnout<-case_when(anes$V162031x>=0~anes$V162031x)
anes$age<-case_when(anes$V161267>=0~anes$V161267)
tapply(anes$turnout, anes$age, mean, na.rm=T)
```

## Go Vote!
```{r}
mean(anes$turnout,na.rm=T) # 30 ppts higher than the actual turnout rate
tapply(anes$turnout, anes$age, mean, na.rm=T)-0.3
```

## Go Vote!
- Let's visualize the age gap in turnout using `ggplot`
- Prep: saving our finding in a separate data frame to be included in `ggplot`
```{r}
result <-tibble(.rows=73) # tibble function creates a data frame (with 73 rows)
result$turnout <- tapply(anes$turnout, anes$age, mean, na.rm=T)-0.3 
# storing the 73 means (Y-axis)
result$age<-sort(unique(anes$age))
# unique function extracts unique values in age
# storing the 73 unique ages (X-axis)
head(result)
```

## In-class exercise: Go Vote!
- Use the `result` data to visualize the relationship between age and turnout using a scatter plot

## In-class exercise: Go Vote!
- Use the `result` data to visualize the relationship between age and turnout using a scatter plot
```{r message=F, fig.height=4, fig.width=5}
ggplot(result,aes(y = turnout, x = age))+
  geom_point()
```


## In-class exercise: Go Vote!
- visualize the relationship between age and turnout using a scatter plot
  - tweaking the plot
```{r fig.show='hide'}
ggplot(result,aes(y = turnout, x = age))+
  geom_point()+
  labs (x = "age", y = "turnout rate")+
  scale_x_continuous (breaks = seq (from = 20, to = 90, by = 10))+
  # customizing the breaks on the x-axis
  geom_hline(yintercept = 0.56, color = "red")+
  # adding a horizontal line at y = average turnout rate; 
  geom_smooth()+
  # adding a smoothed line--we will talk more about this soon
  theme_bw()+
  theme(panel.grid.major=element_blank(),
        panel.grid.minor=element_blank())
```

## In-class exercise: Go Vote!

```{r echo=FALSE, message= F, fig.height=4, fig.width=5}
ggplot(result,aes(y = turnout, x = age))+
  geom_point()+
  labs (x = "age", y = "turnout rate")+
  scale_x_continuous (breaks = seq (from = 20, to = 90, by = 10))+
  # customizing the breaks on the x-axis
  geom_hline(yintercept = 0.56, color = "red")+
  # adding a horizontal line at y = average turnout rate; 
  geom_smooth()+
  # adding a smoothed line--we will talk more about this soon
  theme_bw()+
  theme(panel.grid.major=element_blank(),
        panel.grid.minor=element_blank())
```


## Go Vote!
```{r}
mean(anes$turnout[anes$age<25],na.rm=T)-mean(anes$turnout[anes$age>=70],na.rm=T)
mean(anes$turnout[anes$age<25],na.rm=T)/mean(anes$turnout[anes$age>=70],na.rm=T)
```
- Young people are 23 percentage points less likely to vote than old people
- In democratic representation, what young people want count only 75% of what old people want
- Maybe there's little incentive for politicians to listen to young people as carefully as they listen to old people


## Today's Agenda

- Understand linear regression
- Run regressions in R

   
  
# Linear Regression

## Example: Facial appearance and election outcome
- code we wrote last time 
```{r fig.show='hide'}
face$d.share<-face$d.votes/(face$d.votes+face$r.votes) # share votes for Dem candidates
face$r.share<-face$r.votes/(face$d.votes+face$r.votes) 
face$diff.share<-face$d.share-face$r.share # vote share margin
figure<-ggplot(face,aes(x=d.comp,y=diff.share))+
  geom_point(size=2,alpha=0.3)+ 
  xlim(0,1)+ylim(-1,1)+ 
  labs(x="Competence scores for Democrats",y="Democratic margin in vote share", 
       title = "Facial competence and vote share")+
  theme_bw()
figure
```

## Example: Facial appearance and election outcome
```{r echo=F, fig.height=4, fig.width=4}
figure<-ggplot(face,aes(x=d.comp,y=diff.share))+
  geom_point(size=2,alpha=0.3)+ 
  xlim(0,1)+ylim(-1,1)+ 
  labs(x="Competence scores for Democrats",y="Democratic margin in vote share", 
       title = "Facial competence and vote share")+
  theme_bw()
figure
```


## Regression: Finding the line that fits best
```{r, echo=F, message=F, fig.width=6, fig.height=5, fig.align='center'}
library(patchwork)
m1<-lm(diff.share~d.comp,face)
g1<-ggplot(face,aes(x=d.comp,y=diff.share))+
  geom_point(size=2,alpha=0.5)+ # modifying size of dots (size) and color transparency (alpha)
  xlim(0,1)+ylim(-1,1)+ # defining the limits of x- and y-axes
  labs(x="Competence scores for Democrats",y="Democratic margin", title = "(A)")+
  geom_abline(intercept=coef(m1)[1],slope=coef(m1)[2],color="red4",size=1.5,alpha=0.8)+
  theme_bw()+
  theme(panel.grid.major=element_blank(),
        panel.grid.minor=element_blank(),
        panel.border=element_rect(colour="black"))
g2<-ggplot(face,aes(x=d.comp,y=diff.share))+
  geom_point(size=2,alpha=0.5)+ # modifying size of dots (size) and color transparency (alpha)
  xlim(0,1)+ylim(-1,1)+ # defining the limits of x- and y-axes
  labs(x="Competence scores for Democrats",y="Democratic margin", title = "(B)")+
  geom_abline(intercept=-coef(m1)[1],slope=-coef(m1)[2],color="red4",size=1.5,alpha=0.8)+
  theme_bw()+
  theme(panel.grid.major=element_blank(),
        panel.grid.minor=element_blank(),
        panel.border=element_rect(colour="black"))
g3<-ggplot(face,aes(x=d.comp,y=diff.share))+
  geom_point(size=2,alpha=0.5)+ # modifying size of dots (size) and color transparency (alpha)
  xlim(0,1)+ylim(-1,1)+ # defining the limits of x- and y-axes
  labs(x="Competence scores for Democrats",y="Democratic margin", title = "(C)")+
  geom_abline(intercept=0,slope=0,color="red4",size=1.5,alpha=0.8)+
  theme_bw()+
  theme(panel.grid.major=element_blank(),
        panel.grid.minor=element_blank(),
        panel.border=element_rect(colour="black"))
g4<-ggplot(face,aes(x=d.comp,y=diff.share))+
  geom_point(size=2,alpha=0.5)+ # modifying size of dots (size) and color transparency (alpha)
  xlim(0,1)+ylim(-1,1)+ # defining the limits of x- and y-axes
  labs(x="Competence scores for Democrats",y="Democratic margin", title = "(D)")+
  geom_abline(intercept=-1,slope=2,color="red4",size=1.5,alpha=0.8)+
  theme_bw()+
  theme(panel.grid.major=element_blank(),
        panel.grid.minor=element_blank(),
        panel.border=element_rect(colour="black"))
(g1+g2)/(g3+g4)
```
- Which one is the best, and why?






## How to add a regression line
- `geom_smooth()` function adds a smoothed line to display the salient pattern in the data
- One of the smoothing methods is linear regression (previously we added a non-linear line)
  + specify `method = "lm` within the `geom_smooth()` function
```{r fig.show='hide'}
figure+geom_smooth(method = "lm") # add a smoothing line whose method is linear regression
```

## Let's add a regression line
```{r echo=F, fig.height=4, fig.width=4, message = F}
figure+geom_smooth(method = "lm") # add a smoothing line whose method is linear regression
```
   
- Regression Line: our **best guess** about the trend (Only a guess, not the truth!)
- Grey area around the regression line $\rightsquigarrow$ Confidence Interval
- Confidence Interval: A range of **plausible guesses** given the margin of error
  - We will learn more about CIs and MOEs next time!

## Tweaking the line
- I want to change the color of the line and remove the confidence interval 
- use the `colors()` function to see the complete list of colors
```{r fig.height=4, fig.width=4, message=F}
figure+geom_smooth(method = "lm", color="red4", se=FALSE) 
```


## Exercise (covid.csv)
- Display the relationship between `mask.wearing` and `covid.sympton` in a scatter plot
- Add a regression line

## Exercise (covid.csv)
- Last time we suspected that the Washington Post story cherry-picked an outcome variable (percent of people who know somebody with COVID symptoms) to present the cleanest pattern
- Let's see if how robust the finding is by looking at other outcome variables.
- Create four more scatter plots with regression lines
  - Between `mask.wearing` and `covid.sympton2` (percent of people with symptoms)
  - Between `mask.wearing` and `hospital`
  - Between `mask.wearing` and `death`
  - Between `mask.wearing` and `case`

# Regression Coefficients

## Regression Coefficients (1): Intercept
- Intercept $\alpha$: value of Y when X is 0 in the regression line 
  
```{r echo=FALSE, fig.width=6}
m1<-lm(diff.share~d.comp,face)
figure+
    theme(panel.grid.major=element_blank(),
        panel.grid.minor=element_blank(),
        panel.border=element_rect(colour="black"))+
  geom_abline(intercept=coef(m1)[1],slope=coef(m1)[2],color="red4",size=1.5,alpha=0.8)+
  geom_vline(xintercept=0,linetype="dashed",alpha=0.3)+
  annotate("curve", x =0.1, y = -0.75, xend = 0, yend = coef(m1)[1],
           arrow = arrow(angle = 30, length = unit(4, "mm")),size=0.5, curvature = 0)+
  annotate(geom="text",x =0.1, y = -0.75,label=c("intercept\n-0.31"), size=4,vjust=1)
```


## Regression Coefficients (2): Slope
- Slope ($\beta$): change in Y as X increases from 0 to 1   
  
```{r echo=FALSE, fig.width=6}
m1<-lm(diff.share~d.comp,face)
figure+
    theme(panel.grid.major=element_blank(),
        panel.grid.minor=element_blank(),
        panel.border=element_rect(colour="black"))+
  geom_abline(intercept=coef(m1)[1],slope=coef(m1)[2],color="red4",size=1.5,alpha=0.8)+
  geom_vline(xintercept=0,linetype="dashed",alpha=0.3)+geom_vline(xintercept=1,linetype="dashed",alpha=0.3)+
  annotate("curve", x =0.1, y = -0.75, xend = 0, yend = coef(m1)[1],
           arrow = arrow(angle = 30, length = unit(4, "mm")),size=0.5, curvature = 0)+
  annotate(geom="text",x =0.1, y = -0.75,label=c("intercept\n-0.31"), size=4,vjust=1)
```

## Regression Coefficients (2): Slope
- Slope ($\beta$): change in Y as X increases from 0 to 1   
  
    
```{r echo=FALSE, fig.width=6}
m1<-lm(diff.share~d.comp,face)
figure+
    theme(panel.grid.major=element_blank(),
        panel.grid.minor=element_blank(),
        panel.border=element_rect(colour="black"))+
  geom_abline(intercept=coef(m1)[1],slope=coef(m1)[2],color="red4",size=1.5,alpha=0.8)+
  geom_vline(xintercept=0,linetype="dashed",alpha=0.3)+geom_vline(xintercept=1,linetype="dashed",alpha=0.3)+
  annotate("curve", x =0.1, y = -0.75, xend = 0, yend = coef(m1)[1],
           arrow = arrow(angle = 30, length = unit(4, "mm")),size=0.5, curvature = 0)+
  annotate(geom="text",x =0.1, y = -0.75,label=c("intercept\n-0.31"), size=4,vjust=1)+
  annotate("curve", x =0.9, y = 0.6, xend = 1, yend = coef(m1)[1]+coef(m1)[2],
           arrow = arrow(angle = 30, length = unit(4, "mm")),size=0.5, curvature = 0)+
  annotate(geom="text",x =0.9, y =  0.6,label=c("0.35"), size=4,vjust=0)
```



## Regression Coefficients (2): Slope
- Slope ($\beta$): change in Y as X increases from 0 to 1   
  
    
```{r echo=FALSE, fig.width=6}
m1<-lm(diff.share~d.comp,face)
figure+
    theme(panel.grid.major=element_blank(),
        panel.grid.minor=element_blank(),
        panel.border=element_rect(colour="black"))+
  geom_abline(intercept=coef(m1)[1],slope=coef(m1)[2],color="red4",size=1.5,alpha=0.8)+
  geom_vline(xintercept=0,linetype="dashed",alpha=0.3)+geom_vline(xintercept=1,linetype="dashed",alpha=0.3)+
  geom_hline(yintercept =coef(m1)[1],linetype="dashed",alpha=0.3)+
  annotate("curve", x =0.1, y = -0.75, xend = 0, yend = coef(m1)[1],
           arrow = arrow(angle = 30, length = unit(4, "mm")),size=0.5, curvature = 0)+
  annotate(geom="text",x =0.1, y = -0.75,label=c("intercept\n-0.31"), size=4,vjust=1)+
  annotate("curve", x =0.9, y = 0.6, xend = 1, yend = coef(m1)[1]+coef(m1)[2],
           arrow = arrow(angle = 30, length = unit(4, "mm")),size=0.5, curvature = 0)+
  annotate("curve", x =0, y = coef(m1)[1], xend = 1, yend = coef(m1)[1],
           arrow = arrow(angle = 30, length = unit(2, "mm")),size=1, color="navy",curvature = 0,alpha=0.4)+
  annotate("curve", x =1, y = coef(m1)[1], xend = 1, yend = coef(m1)[1]+coef(m1)[2],
           arrow = arrow(angle = 30, length = unit(2, "mm")),size=1, color="navy",curvature = 0,alpha=0.4)+
  annotate(geom="text",x =0.9, y =  0.6,label=c("0.35"), size=4,vjust=0)+
  annotate(geom="text",x =0.75, y =  -0.7,label=c("Y goes from -0.31 to 0.35\nas X goes from 0 to 1:\nSlope=0.35-(-0.31)=0.66"), size=4,vjust=0)

```



## Estimates of a regression model
- **Parameters ($\alpha$, $\beta$)**: true values the coefficients that are not directly observable because of chance errors (requiring us to see the entire universe of politicians running in every single race)
  - What about the regression line that we just saw?
  - The line was drawn from the **estimates** of the coefficients
- An estimate is our best guess about the true parameter given the data we observed
- Facial competence example: We estimated the intercept ($\alpha$) to be -.31 and the slope ($\beta$) to be .66
  - Our estimated model: $Vote Margin = -0.31+0.66 \times Appearance$
  - This model allow us to predict the election outcome based on the independent variable (`d.comp`)
    - When `d.comp=0`, the predicted outcome is $-0.31+0.66 \times 0 = -0.31$
    - When `d.comp=0.5` (mean), the predicted outcome is $-0.31+0.66 \times 0.5 \approx 0.02$
    - When `d.comp=0.9` (maximum), the predicted outcome is $-0.31+0.9 \times 0.48 \approx 0.28$

# Running a regression model

## Ordinary Least Squares (OLS)
- How do draw the line that fits the data best?
  - Minimize prediction error 
    + actual outcome: value of Y represented in each dot 
    + predicted outcome: value of Y represented in the regression line
    + prediction error (residual): the gap between the line and each dot 

## Ordinary Least Squares (OLS)
```{r echo=FALSE, fig.width=6}
m1<-lm(diff.share~d.comp,face)
figure+
    theme(panel.grid.major=element_blank(),
        panel.grid.minor=element_blank(),
        panel.border=element_rect(colour="black"))+
  geom_abline(intercept=coef(m1)[1],slope=coef(m1)[2],color="red4",size=1.5,alpha=0.8)+
  annotate("curve", x =face$d.comp, y = face$diff.share, xend = face$d.comp, yend = coef(m1)[1]+coef(m1)[2]*face$d.comp,
           arrow = arrow(angle = 30, length = unit(0.5, "mm")),size=0.3, color="navy",curvature = 0,alpha=0.7)
```

## Ordinary Least Squares (OLS)
- How do we figure out the best line to draw?
  - Minimize prediction error 
    + actual outcome: value of Y represented in each dot 
    + predicted outcome: value of Y represented in the regression line
    + prediction error (residual): the gap between the line and each dot 
- Least squares method: 
  - square these prediction errors and sum them up
  - find the line that minimizes the sum of squared errors
  - you can hand calculate the coefficients if there are just a few obs (though even that can be tedious)
  - R does it for us!

## Linear regression in R
- R will calculate least squares line for a data set using the `lm()` function
  - Syntax: `lm(y ~ x, data)`
  - y is the dependent variable
  - x is the independent variable
  - Jargon: "regress y on x"
    - *"Table 1 reports the results of a regression model where y is regressed on x..."*
```{r}
m1<-lm(diff.share~d.comp,face)
m1
```
---
```{r echo=FALSE, fig.width=8, fig.height=6, fig.align='center'}
m1<-lm(diff.share~d.comp,face)
figure+
    theme(panel.grid.major=element_blank(),
        panel.grid.minor=element_blank(),
        panel.border=element_rect(colour="black"))+
  geom_abline(intercept=coef(m1)[1],slope=coef(m1)[2],color="red4",size=1.5,alpha=0.8)+
  geom_vline(xintercept=0,linetype="dashed",alpha=0.3)+geom_vline(xintercept=1,linetype="dashed",alpha=0.3)+
  geom_hline(yintercept =coef(m1)[1],linetype="dashed",alpha=0.3)+
  annotate("curve", x =0.1, y = -0.75, xend = 0, yend = coef(m1)[1],
           arrow = arrow(angle = 30, length = unit(4, "mm")),size=0.5, curvature = 0)+
  annotate(geom="text",x =0.1, y = -0.75,label=c("intercept\n-0.31"), size=4,vjust=1)+
  annotate("curve", x =0.9, y = 0.6, xend = 1, yend = coef(m1)[1]+coef(m1)[2],
           arrow = arrow(angle = 30, length = unit(4, "mm")),size=0.5, curvature = 0)+
  annotate("curve", x =0, y = coef(m1)[1], xend = 1, yend = coef(m1)[1],
           arrow = arrow(angle = 30, length = unit(2, "mm")),size=1, color="navy",curvature = 0,alpha=0.4)+
  annotate("curve", x =1, y = coef(m1)[1], xend = 1, yend = coef(m1)[1]+coef(m1)[2],
           arrow = arrow(angle = 30, length = unit(2, "mm")),size=1, color="navy",curvature = 0,alpha=0.4)+
  annotate(geom="text",x =0.9, y =  0.6,label=c("0.35"), size=4,vjust=0)+
  annotate(geom="text",x =0.75, y =  -0.7,label=c("Y goes from -0.31 to 0.35\nas X goes from 0 to 1:\nSlope=0.35-(-0.31)=0.66"), size=4,vjust=0)

```

## Fitted values
- R can show you each of the fitted values
```{r}
head(face$diff.share) # actual outcome
head(fitted(m1)) # predicted outcome
head(face$diff.share-fitted(m1)) # residual/prediction error
```


## Regression exercise 1 (covid)
- Run a linear regression model to examine the relationship between mask wearing (X) and covid-related deaths (Y)

## Regression exercise 2 (ANES 2016)
- Run a linear regression model to examine the relationship between age (X) and turnout (Y)

## Regression exercise 3(stat)
- Examine the relationship between the number of stat classes taken (X) and number of familiar statistical terms (Y)


# CLASS 19

## Housekeeping
- Syllabus updated
- HW3 grading in progress
- HW4 deadline extended to 11/15

## Today's Agenda

- Last time
  - Understanding regression as a concept
  - Running regression models in R
- Today
  - Regression with binary variables


## Libraries and Datasets

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(haven)
library(labelled)
library(emmeans) # new
face<- read_csv("face.csv")
gallup<-read_csv("gallup.csv")
anes<-read_dta("anes.dta")
yg<-read_sav("yg.sav") # YouGov survey data on social media and privacy concern 
```


## In-class exercise (ANES 2016)
- Run a linear regression model to examine the relationship between feeling for Clinton (V161086) and feeling for Trump (V161087)
  + Regress Trump on Clinton
  + Create a graph with a scatter plot and a regression line


## In-class exercise (ANES 2016)
- Run a linear regression model to examine the relationship between feeling for Clinton (V161086) and feeling for Trump (V161087)
  + Regress Trump on Clinton
  + Create a graph with a scatter plot and a regression line
```{r}
anes$clinton<-case_when(anes$V161086>=0 & anes$V161086<=100~ anes$V161086)
anes$trump<-case_when(anes$V161087>=0 & anes$V161087<=100~ anes$V161087)
m.anes1<-lm(trump~clinton,anes)
m.anes1
```
## In-class exercise (ANES 2016)
```{r warning=FALSE, message = FALSE}
ggplot(anes,aes(x = clinton, y = trump))+
  geom_point()+
  geom_smooth(method="lm", color="red4", fill = "red3")+
  labs(x="Feeling for Clinton", y = "Feeling for Trump")+theme_bw()
```

## In-class exercise (ANES 2016)
```{r warning=FALSE, message = FALSE}
ggplot(anes,aes(x = clinton, y = trump))+
  geom_point(alpha=0.1, size = 2, position = "jitter")+
  geom_smooth(method="lm", color="red4", fill = "red3")+
  labs(x="Feeling for Clinton", y = "Feeling for Trump")+theme_bw()
```


## In-class exercise (ANES 2016)
- Does nationalism predict people's attitude toward Trump?
- Run a linear regression model to examine the relationship between nationalism (V162271:V162274) and feeling for Trump (V161087)
  + Use the four items of nationalism (V162271:V162274) create one index ranging 0-1 where 1 indicates the most nationalistic view
  + Regress feeling for Trump on nationalism

## In-class exercise (ANES 2016)
- Does nationalism predict people's attitude toward Trump?
- Run a linear regression model to examine the relationship between nationalism (V162271:V162274) and feeling for Trump (V161087)
  + Use the four items of nationalism (V162271:V162274) create one index ranging 0-1 where 1 indicates the most nationalistic view
  + Regress feeling for Trump on nationalism


# Regression with Binary variables

## When your independent variable is binary
- Regress your outcome variable (Y) on an independent variable (X) that is either 0 or 1
- Examples
  + comparing women ($X=1$) vs men ($X=0$)
  + comparing Democrats ($X=1$) vs Republicans ($X=0$)
  + comparing treatment group ($X=1$) vs control group ($X=0$)
    + tip: In many cases, it doesn't really matter which group is coded '1' and which group is coded '0.' But for experiments, control group should be '0' 
- Intercept ($\alpha$): average value of $Y$ when $X$ is 0 
  + $\rightsquigarrow$ e.g., mean among men
- Slope ($\beta$): change in mean of $Y$ when $X$ becomes 1 
  + $\rightsquigarrow$ e.g., mean difference women and men
- Intercept + Slope ($\alpha+\beta$): average value of $Y$ when $X$ is 1 
  + $\rightsquigarrow$ e.g., mean among women

## Binary Predictor Example: Feeling for Trump by Binary PID
- Create a binary PID variable where 1 includes Democrats/ Leaning Democrats and 0 includes Republicans and Leaning Republicans
  + Use V161155 & V161157

## Binary Predictor Example: Feeling for Trump by Binary PID
- Create a binary PID variable where 1 includes Democrats/ Leaning Democrats and 0 includes Republicans and Leaning Republicans
  + Use V161155 & V161157
```{r}
anes$dem[anes$V161155==1|anes$V161157==3]<-1 
anes$dem[anes$V161155==2|anes$V161157==1]<-0
```

## Binary Predictor Example: Feeling for Trump by Binary PID
- Create a binary PID variable where 1 includes Democrats/ Leaning Democrats and 0 includes Republicans and Leaning Republicans
  + Use V161155 & V161157
- Compare the mean of `trump` by `dem` using tapply()

## Binary Predictor Example: Feeling for Trump by Binary PID
- Create a binary PID variable where 1 includes Democrats/ Leaning Democrats and 0 includes Republicans and Leaning Republicans
  + Use V161155 & V161157
- Compare the mean of `trump` by `dem` using tapply()
```{r}
means<-tapply(anes$trump,anes$dem,mean,na.rm=T)
means
means[2]-means[1]
```

## Binary Predictor Example: Feeling for Trump by Binary PID
- Create a binary PID variable where 1 includes Democrats/ Leaning Democrats and 0 includes Republicans and Leaning Republicans
  + Use V161155 & V161157
- Compare the mean of `trump` by `dem` using tapply()
- Regress `trump` on `dem` 

## Binary Predictor Example: Feeling for Trump by Binary PID
- Create a binary PID variable where 1 includes Democrats/ Leaning Democrats and 0 includes Republicans and Leaning Republicans
  + Use V161155 & V161157
- Compare the mean of `trump` by `dem` using tapply()
- Regress `trump` on `dem` 
```{r}
m.anes3<-lm(trump~dem,anes)
m.anes3
```


## Binary Predictor Example: Feeling for Trump by Binary PID
```{r echo=F, warning=F, message = F, fig.height = 4}
ggplot(anes,aes(x=dem,y=trump))+
  geom_point(size=2,alpha=0.05)+
  geom_smooth(color="black",method="lm",alpha=0.5)+
  labs(x="Partisanship",y = "Feeling for Trump")+
  scale_x_continuous(breaks = c(0,1),
                     labels = c("Republicans","Democrats"))+
  geom_hline(yintercept =means[1],linetype="dashed",alpha=0.8,color="red4")+
  geom_hline(yintercept =means[2],linetype="dashed",alpha=0.8,color="red4")+
  annotate("curve", x =0.05, y = 75, xend = 0, yend = means[1],
           arrow = arrow(angle = 15, length = unit(3, "mm")),size=0.5, curvature = 0, alpha=0.5)+
  annotate("curve", x =0.95, y = 30, xend = 1, yend = means[2],
           arrow = arrow(angle = 15, length = unit(3, "mm")),size=0.5, curvature = 0, alpha=0.5)+
annotate("curve", x =0.5, y = 30, xend = 0.5, yend = mean(means),
           arrow = arrow(angle = 15, length = unit(3, "mm")),size=0.5, curvature = 0, alpha=0.5)+
  annotate(geom="text",x =0.13, y = 77,label=expression(hat(alpha) == 62.88~(Rep~Mean)), size=4,vjust=0, parse=TRUE)+
  annotate(geom="text",x =0.85, y = 32,label=expression(hat(alpha)+hat(beta) == 14.09~(Dem~Mean)), size=4,vjust=0, parse=TRUE)+
  annotate(geom="text",x =0.5, y = 30,label=expression(hat(beta) == -48.79~(Mean~Difference)), size=4,vjust=1, parse=TRUE)+

  theme_bw()+
      theme(panel.grid.major=element_blank(),
        panel.grid.minor=element_blank(),
        panel.border=element_rect(colour="black"))
```
   
- Recall: a regression model finds a line that minimizes prediction errors
- Prediction errors are minimized at the means ($\hat\alpha$ and $\hat\alpha+\hat\beta$)

## Marginal Means (AKA Predicted Means)
- "Marginal Means" $\rightsquigarrow$ Means of Y at particular values of X
- `lm()` function doesn't directly give you marginal means at non-zero X values (e.g., Democratic mean)
- use `coef()` to return coefficients, and compute marginal means
```{r}
coef(m.anes3)
coef(m.anes3)[1]+coef(m.anes3)[2]
```
## Marginal Means
- Using `emmeans()` function can be convenient
  + syntax: emmeans(model,formula)
```{r}
emmeans(m.anes3,~dem)
```

## In-class exercise (Gallup)
- Compare the mean of `goodlooking` between women and men
  + use `tapply()`
  + use `lm()`
  + compute marginal means

## In-class exercise (YouGov survey)
- Is there a gender difference in privacy concern?
  - Create a binary gender variable using `gender_o`
  - Rescale `pii_concern_1` to 0--1 where 1 indicates being most concerned
  - `pii_concern_1`: "I am concerned about how much data there is about me on the internet."
  - Regress privacy concern on gender


## In-class exercise (YouGov survey)
- Is there a educational gap in privacy concern?
  - Create a binary education variable using `educ`, where 0 means without a 4-year college degree, and 1 means 4-year college or higher
  - Regress privacy concern on college
  
## In-class exercise (YouGov survey)
- Is there a relationship between social media use and privacy concern?
  - Step 1: Inspect two variables measuring social media use frequency (`fb_freq`, `twitter_freq`)
  - Step 2: Recode the social media use variable (`fb_freq`, `twitter_freq`) so that higher numbers indicate more frequent use
  - Step 3: Regress Facebook use on privacy concern
  - Step 4: Regress Twitter use on privacy concern
- Do your findings confirm the privacy paradox hypothesis?


# CLASS 20

## Today's Agenda

- Last time
  - Regression with binary predictor
- Today
  - Regression with binary outcome
  - Uncertainty
  
## Housekeeping

- Survey will be out this week
- We will run the experiment in 2 weeks


## Libraries and Datasets

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(haven)
library(labelled)
library(emmeans) 
gallup<-read_csv("gallup.csv")
anes<-read_dta("anes.dta")
yg<-read_sav("yg.sav")
```

## Recap: When your independent variable is binary
- Special case of linear regression where the value of IV is either 0 or 1
- Examples
  - You want to compare men and women on an outcome of interest (DV) 
  - Your IV is called 'female' where men are coded 0, and women are coded 1
  - Syntax: `lm(DV~female,data)`
- How to interpret the findings?
  - Intercept ($\alpha$ coefficient): average among men
  - Slope ($\beta$ coefficient): difference in means between men and women
  - Intercept + Slope: average among women
  - Marginal means: the averages of `Y` at specific values of `X` (`X = 0` or `X = 1`)
    - Use `coef()` or `emmeans()`


## In-class exercise (YouGov survey)
- Is there a educational gap in privacy concern?
  - Create a binary education variable using `educ`, where 0 means without a 4-year college degree, and 1 means 4-year college or higher
  - Regress privacy concern (`pii_concern_1`) on college and calculate the marginal means
  
## In-class exercise (YouGov survey)
- Is there a relationship between social media use and privacy concern?
  - Step 1: Inspect two variables measuring social media use frequency (`fb_freq`, `twitter_freq`)
  - Step 2: Recode the social media use variable (`fb_freq`, `twitter_freq`) so that higher numbers indicate more frequent use
  - Step 3: Regress Facebook use on privacy concern and calculate the marginal means
  - Step 4: Regress Twitter use on privacy concern and calculate the marginal means
- Do your findings confirm the privacy paradox hypothesis?



## When your dependent variable is binary
- Special case of linear regression where the value of DV is either 0 or 1
  - Your outcome: Whether one believes in climate change (1) or not (0)
  - Average of a binary variable $\rightsquigarrow$ probability/proportion
  - Average of this outcome
    - probability that an individual believes in climate change 
    - proportion of people believing in climate change 
- $\alpha$: average value of $Y$ when $X$ is 0 
  + $\rightsquigarrow$ probability that $Y_i=1$ when $X_i=0$
- $\beta$:change in $Y$ when $X$ becomes 1 
  + $\rightsquigarrow$ change in probability that $Y_i=1$ when $X$ increases by one unit

## Binary DV example: climate change belief
- Using the ANES 2016 data, create a binary variable where 0 means the belief that global warming hasn't been happening, and 1 means the belief that it has been happening
  + V161221
- Regress climate denial on watching the Sean Hannity show
  + V161370
  
## Binary DV example: climate change belief
```{r}
head(anes$V161221)
head(anes$V161370)
```


## Binary DV example: climate change belief
```{r warning=F}
# using indexing to recode (fine)
anes$climate[anes$V161221==1]<-1
anes$climate[anes$V161221==2]<-0
anes$hannity[anes$V161370==0]<-0
anes$hannity[anes$V161370==1]<-1

# using ifelse to recode (my preference)
anes$climate<-ifelse(anes$V161221>0, 2-anes$V161221,NA)
anes$hannity<-ifelse(anes$V161370>=0,anes$V161370,NA)

# run regression
m.anes4<-lm(climate~hannity,anes)
```

## Binary DV example: climate change belief
```{r}
m.anes4
```

- $\hat\alpha=0.85$: among those who don't watch Hannity, 85% said global warming has been happening
- $\hat\beta=-0.28$: Hannity viewers were less likely to say global warming has been happening by 28 percentaige points.

## Binary DV example: climate change belief
- A <span style="color:red">Fox News</span> Effect?
- Let's regress climate denial on watching the Sean Hannity show among Republicans

## Binary DV example: climate change belief
- A <span style="color:red">Fox News</span> Effect?
- Let's regress climate denial on watching the Sean Hannity show among Republicans

```{r warning=F}
anes$dem[anes$V161155==1|anes$V161157==3]<-1 
anes$dem[anes$V161155==2|anes$V161157==1]<-0
m.anes5<-lm(climate~hannity,subset(anes,dem==0)) # using subset function to run this model among Reps
m.anes5
```
- $\hat\alpha=0.75$: among Republicans who don't watch Hannity, 75% said global warming has been happening
- $\hat\beta=-0.21$: Hannity viewers were less likely to say global warming has been happening by 21 percentaige points.
- We "controlled for" partisanship (we ruled out partisanship as an alternative explanation)


## Binary DV example 2: climate change and feeling for scientists
- Regress **climate** on feeling for scientists (V162112)

## Binary DV example 2: climate change and feeling for scientists
- Regress **climate** on feeling for scientists
```{r}
anes$scientist<-ifelse(anes$V162112>=0&anes$V162112<=100,anes$V162112,NA)
m.anes6<-lm(climate~scientist,anes)
m.anes6
```
- $\hat\alpha=0.54$ $\rightsquigarrow$ among those who dislike scientists the most, the probability of beliving in global warming is 54%

- $\hat\beta=0.0037$ $\rightsquigarrow$ one unit increase in the feeling scale is associated with 0.37 percentage point increase in belief in global warming 

# Uncertainty

## Percentage of Americans believing in climate change
```{r}
prop.table(table(anes$climate))
mean(anes$climate,na.rm=T)
lm(climate~1,anes) # lm(var~1,data) will give you the mean of var in data
```

## Uncertainty
- The data suggest 82% of Americans think global warming is happening
  - It should be clear that **polling results** $\neq$ **truth**
- What can we learn about belief in climate change in **population** from this one sample?
- How sure can we be about this number?
  - If our sample was 300, would you be more/less sure? How much?
  - What if sample was 30?!

## Statistical inference always involves uncertainty
- Statistical inference is using data to guess something about the population 
- Parameter ($\theta$): unknown quantity of interest
  + population mean (e.g., % of all voters who will vote for Trump)
  + population average treatment effect (e.g., you ran an experiment on entire US population!)
- Point estimate ($\hat\theta$): a single â€œbest guessâ€ as to the value of $\theta$
  + sample mean (e.g., % of respondents who will vote for Trump)
  + sample average treatment effect
- Gap between $\theta$ and point estimate ($\hat\theta$) $\rightsquigarrow$ change error

## Standard error 
- Imagine you ran the same study over and over again with a new sample from the population
- Each study will give you its best guess (point estimate)
- The point estimate will vary study to study because of chance error (i.e., sampling variability) even if your study uses exactly the same methods
- Standard error: the **standard deviation** of the point estimates $\rightsquigarrow$ how large is the chance error on average
  + But we rarely get to run the same study over and over again
  + Turns out we can estimate it
  + For more on how to calculate it, and math behind it, see Imai (Chapter 7)
  + In practice, R calculates it for us!
- Important: standard error becomes smaller when...
  + $\rightsquigarrow$ larger sample
  + $\rightsquigarrow$ standard deviation of DV is smaller (in a regression)
  + $\rightsquigarrow$ standard deviation of IV is larger (in a regression)

## Simulation example
- Imagine we know a true parameter: 80% of Americans believe in climate change
- Imagine we ran surveys with different sample sizes (30, 300, 3000 and 30000)
- Imagine we ran 5000 studies with each sample size option
  - How widely would the numbers vary when you have 30 obs vs 30,000 obs?
```{r}
means30<-rep(NA,5000) # empty container variable
means300<-rep(NA,5000) # empty container variable
means3000<-rep(NA,5000) # empty container variable
means30000<-rep(NA,5000) # empty container variable
for (i in 1:5000) {
  means30[i]<-mean(rbinom(30,1,.8)) # sampling 30 people X repeat 5000 times
  means300[i]<-mean(rbinom(300,1,.8)) # sampling 300 people X repeat 5000 times
  means3000[i]<-mean(rbinom(3000,1,.8)) # sampling 3000 people X repeat 5000 times
  means30000[i]<-mean(rbinom(30000,1,.8)) # sampling 30000 people X repeat 5000 times
}
st.erros<-c(sd(means30),sd(means300),sd(means3000),sd(means30000))
st.erros
```

## Simulation: Distribution of the "best guess" that each study will give you
```{r echo=F, warning=F, message=F}
library(patchwork)
g1<-ggplot(as.data.frame(means30),aes(x=means30))+geom_histogram(bins=100)+xlim(0.3,1.01)+
  theme_bw()+labs(title="sample n = 30", x = "sample seans")+geom_vline(xintercept=0.8,linetype="dashed",color="red4")
g2<-ggplot(as.data.frame(means300),aes(x=means300))+geom_histogram(bins=100)+xlim(0.3,1.01)+
  theme_bw()+labs(title="sample n = 300", x = "sample seans")+geom_vline(xintercept=0.8,linetype="dashed",color="red4")
g3<-ggplot(as.data.frame(means3000),aes(x=means3000))+geom_histogram(bins=100)+xlim(0.3,1.01)+
  theme_bw()+labs(title="sample n = 3000", x = "sample seans")+geom_vline(xintercept=0.8,linetype="dashed",color="red4")
g4<-ggplot(as.data.frame(means30000),aes(x=means30000))+geom_histogram(bins=100)+xlim(0.3,1.01)+
  theme_bw()+labs(title="sample n = 30000", x = "sample seans")+geom_vline(xintercept=0.8,linetype="dashed",color="red4")
(g1+g2)/(g3+g4)
```

- Sample size, sample size, sample size!


# CLASS 21


## Today's Agenda


- Today
  - Understanding and estimating standard error
  - Related measures of uncertainty: confidence interval, t-value, p-value
  - Hypothesis testing

## Housekeeping

- HW4 due 11/15   

## Libraries and Datasets

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(haven)
library(labelled)
face<- read_csv("face.csv")
stat<-read_csv("stat.csv")
gallup<-read_csv("gallup.csv")
anes<-read_dta("anes.dta")
```


# Uncertainty

## Percentage of Americans believing in climate change
```{r warning=FALSE}
anes$climate[anes$V161221==1]<-1 # recreating the variable from last class
anes$climate[anes$V161221==2]<-0
prop.table(table(anes$climate))
mean(anes$climate,na.rm=T)
lm(climate~1,anes) # lm(var~1,data) will give you the mean of var in data
```

## Uncertainty
- The data suggest 82% of Americans think global warming is happening
  - It should be clear that **polling results** $\neq$ **truth**
- What can we learn about belief in climate change in **population** from this one sample?
- How sure can we be about this number?
  - If our sample was 300, would you be more/less sure? How much?
  - What if sample was 30?!

## Statistical inference always involves uncertainty
- Statistical inference is using data to guess something about the population 
- Parameter ($\theta$): unknown quantity of interest
  + population mean (e.g., % of all voters who will vote for Trump)
  + population average treatment effect (e.g., you ran an experiment on entire US population!)
- Point estimate ($\hat\theta$): a single â€œbest guessâ€ as to the value of $\theta$
  + sample mean (e.g., % of respondents who will vote for Trump)
  + sample average treatment effect
- Gap between $\theta$ and point estimate ($\hat\theta$) $\rightsquigarrow$ change error

## Standard error 
- Imagine you ran the same study over and over again with a new sample from the population
- Each study will give you its best guess (point estimate)
- The point estimate will vary study to study because of chance error (i.e., sampling variability) even if your study uses exactly the same methods
- Standard error: the **standard deviation** of the point estimates $\rightsquigarrow$ how large is the chance error on average
  + But we rarely get to run the same study over and over again
  + Turns out we can estimate it
  + For more on how to calculate it, and math behind it, see Imai (Chapter 7)
  + In practice, R calculates it for us!
- Important: standard error becomes smaller when...
  + $\rightsquigarrow$ larger sample
  + $\rightsquigarrow$ standard deviation of DV is smaller (in a regression)
  + $\rightsquigarrow$ standard deviation of IV is larger (in a regression)

## Simulation example
- Imagine we know a true parameter: 80% of Americans believe in climate change
- Imagine we ran surveys with different sample sizes (30, 300, 3000 and 30000)
- Imagine we ran 5000 studies with each sample size option
  - How widely would the numbers vary when you have 30 obs vs 30,000 obs?
```{r}
means30<-rep(NA,5000) # empty container variable
means300<-rep(NA,5000) # empty container variable
means3000<-rep(NA,5000) # empty container variable
means30000<-rep(NA,5000) # empty container variable
for (i in 1:5000) {
  means30[i]<-mean(rbinom(30,1,.8)) # sampling 30 people X repeat 5000 times
  means300[i]<-mean(rbinom(300,1,.8)) # sampling 300 people X repeat 5000 times
  means3000[i]<-mean(rbinom(3000,1,.8)) # sampling 3000 people X repeat 5000 times
  means30000[i]<-mean(rbinom(30000,1,.8)) # sampling 30000 people X repeat 5000 times
}
st.erros<-c(sd(means30),sd(means300),sd(means3000),sd(means30000))
st.erros
```

## Sampling distributions
```{r echo=F, warning=F, message=F, fig.height=4}
means30<-rep(NA,5000) # empty container variable
means300<-rep(NA,5000) # empty container variable
means3000<-rep(NA,5000) # empty container variable
means30000<-rep(NA,5000) # empty container variable
for (i in 1:5000) {
  means30[i]<-mean(rbinom(30,1,.8)) # sampling 10 people X repeat 5000 times
  means300[i]<-mean(rbinom(300,1,.8)) # sampling 100 people X repeat 5000 times
  means3000[i]<-mean(rbinom(3000,1,.8)) # sampling 1000 people X repeat 5000 times
  means30000[i]<-mean(rbinom(30000,1,.8)) # sampling 10000 people X repeat 5000 times
}
library(patchwork)
g1<-ggplot(as.data.frame(means30),aes(x=means30))+geom_density(adjust = 5)+xlim(0.3,1.01)+
  theme_bw()+labs(title="sample n = 30", x = "sample seans")+geom_vline(xintercept=0.8,linetype="dashed",color="red4")
g2<-ggplot(as.data.frame(means300),aes(x=means300))+geom_density(adjust = 5)+xlim(0.3,1.01)+
  theme_bw()+labs(title="sample n = 300", x = "sample seans")+geom_vline(xintercept=0.8,linetype="dashed",color="red4")
g3<-ggplot(as.data.frame(means3000),aes(x=means3000))+geom_density(adjust = 5)+xlim(0.3,1.01)+
  theme_bw()+labs(title="sample n = 3000", x = "sample seans")+geom_vline(xintercept=0.8,linetype="dashed",color="red4")
g4<-ggplot(as.data.frame(means30000),aes(x=means30000))+geom_density(adjust = 5)+xlim(0.3,1.01)+
  theme_bw()+labs(title="sample n = 30000", x = "sample seans")+geom_vline(xintercept=0.8,linetype="dashed",color="red4")
(g1+g2)/(g3+g4)
```

- Mean of point estimates ($\hat\theta$) $\rightsquigarrow$ converge toward the true parameter ($\theta$) in repeated samples
- Distribution of point estimates $\rightsquigarrow$ normal (bell-shaped)
- Larger sample $\rightsquigarrow$ narrower margin of error

## Zooming in on the large sampling distribution
```{r echo=F, warning=F, message=F, fig.height=3,  fig.height=3}
g4<-ggplot(as.data.frame(means30000),aes(x=means30000))+geom_density(adjust = 5)+xlim(0.79,.81)+
  theme_bw()+labs(title="sample n = 30000", x = "sample seans")+geom_vline(xintercept=0.8,linetype="dashed",color="red4")
g4
range(means30000)
sd(means30000)
```
    
- At worst, your estimate will be off by about 0.08
- On average, your point estimate will be off by 0.02

## Chance error vs bias

- Standard errors only account for sampling variability 
- They're correct only if your sample is randomly drawn from the population
- True random sampling is nearly impossible in many cases (political polls)
- If your sample wasn't randomly drawn, your estimates may be biased
- Unlike chance errors, such biases are very difficult to account for
  - Key problem with the polls in 2016, and 2020


## Estimating standard errors in R
- run a regression model
- pass the model through the summary() function
```{r}
m.climate<-lm(climate~1,anes)
summary(m.climate)
```

## Estimating standard errors in R
- Making sense of the key results
  - Our best guess about Americans' belief in climate change is that 82% believe it
  - If we ran the survey repeatedly with new samples, the sample means will deviate from the unknown true parameter by 0.59 percentage point, on average


## Confidence interval
- Confidence interval: A range of plausible guesses about the parameter 
- Strategy
  - find a range of plausible chance errors (i.e., **margin of error**) 
  - and add them to the point estimate ($\hat\theta$)
- How to define plausible chance errors?
  - Sampling distribution is normal
  - One thing about normal distribution: 95% of cases are within $\pm 2\times SD$ 
    - To be exact, it's $\pm 1.96\times SD$
    - $\rightsquigarrow$ 95% of chance errors are within $\pm 2\times SE$ 
  - Let's call 95% of chance errors "plausible" $\rightsquigarrow$ conventional margin of error
    - You can choose other confidence levels (e.g., 90%, 99%) but 95% is most common
- 95% Confidence Interval: $\hat\theta \pm 1.96 \times SE$

## Calculating CIs in R
- you can use the formula $\hat\theta \pm 1.96 \times SE$
```{r}
ub<-0.8206406+1.96*0.00591 # upper bound
lb<-0.8206406-1.96*0.00591 # lower bound
c(lb,ub)
```

- But no need. Just pass the model through the **confint()** function
```{r}
confint(m.climate)
```

## Interpreting confidence interval
- "There is a 95% chance that true parameter is within that range [80.9%, 82.1%]"
  - True or False?

## Interpreting confidence interval
- "There is a 95% chance that true parameter is within that range [80.9%, 82.1%]"
  - common misunderstanding!

- It's a really tricky concept to wrap your head around
- You should understand this in the context of **repeated** samples
- As you keep running the same study...
  - the CIs you calculate will contain the true parameter 95% of the time
   
- A value not within your CI (e.g., 80% believe in CC)
  - "implausible" in the sense that...
  - if that was indeed the true value (it could be), 
    - $\rightsquigarrow$ your data is one of the 5% unlucky draws that happened to exclude the true value


## Interpreting confidence interval
```{r}
confint(m.climate)
```
- Our 95% CIs suggest that plausible proportions of American believing in climate change range from 80.9 to 83.2%.
  - That is, we deem values like 80.8% or 83.3% implausible in the sense that if those were true, it would've been unlikely (i.e., less a 5% chance) to get the result that we just got

## Standard error/CI of slope coefficients
- We have focused on a mean parameter
- The concepts of SEs and CIs are applicable to slope parameters
- Example: partisan difference in climate belief
  - Regress climate belief on binary PID
```{r warning = F}
anes$dem[anes$V161155==1|anes$V161157==3]<-1 # recreating the variable from last class
anes$dem[anes$V161155==2|anes$V161157==1]<-0
```

## Standard error/CI of slope coefficients
```{r results = 'hold'}
m.climate2<-lm(climate~dem,anes)
summary(m.climate2)
confint(m.climate2)
```


## Making sense of the output
- Our best guess about Americans' partisan gap in climate change is 22 percentage points.
- If we ran the survey repeatedly with new samples, the point estimates will deviate from the true parameter by 1.2 percentage point, on average.
- The 95% confidence interval is between 0.197 and 0.245.
   
- The ratio between the size estimated partisan gap (22.1), and the average size of error (1.2) $\rightsquigarrow$ looks large enough to say...
  - We probably wouldn't see these numbers if we were living in a world where Democrats and Republicans actually held the same belief about climate change ($\beta=0$)
  - In other words, $\beta=0$ is way, way outside of the plausible range of partisan gap that the data suggest [0.197,0.245]
  - Is there a formal way to communicate these thoughts?


## t-value and p-value
```{r}
summary(m.climate2)
```



## t-value and p-value
- t-value: Ratio between point estimate and standard error
- p-value: Probability of observing the evidence when the true parameter is zero
- t < -1.96 or t > 1.96 (so roughly below -2 or above 2) $\rightsquigarrow$ p < 0.05
- p < 0.05 is the most widely used standard for being "statistically significant"



## t-value and p-value
```{r}
options(scipen=999)
summary(m.climate2)
```


## Hypothesis testing
- Our hypothesis ($H_1$): Democrats and Republicans differ in belief about climate change
- Null hypothesis ($H_0$): No difference between Democrats and Republicans
- p < 0.0000000000000002 
  - $\rightsquigarrow$ Our findings are **extremely** unlikely to be observed if the null hypothesis was correct (i.e., true value of $\beta$ was 0).
  - We reject the null hypothesis in favor of the alternative hypothesis (our hypothesis)

## In-class exercise
- Using the comm 290 data, test the hypothesis that number of prior stat-related courses is positively related to number of familiar statistical terms

## In-class exercise
- Using the comm 290 data, test the hypothesis that number of prior stat-related courses is positively related to number of familiar statistical terms
```{r}
# m.comm290<-lm(terms~courses,comm290)
# summary(m.comm290)
# confint(m.comm290)
```

## Be cautious when interpreting null findings
- We got p = 0.854 (a null finding) 
  - The 95% CI includes 0
    - $\beta=0$ is within the range of plausible values
  - The relationship was not statistically significant
  - We fail to reject the null hypothesis $H_0:\beta=0$. So our hypothesis wasn't supported.
  - Would you believe that taking stat courses has nothing to do with stat knowledge?   
   
- Important: A null finding doesn't mean our hypothesis was incorrect. 
  - Failure to find a relationship $\neq$ Absence of a relationship
  - Unless you had a very large sample, and an impeccable study design, you shouldn't conclude that there's no relationship between X and Y.

## Precise null

- You ran many, many studies with large samples (millions when combined)
  - Your best guess is sill $\beta=0$ and your CI is really narrow because you had large samples
    - Even a small effect size (say 1 percentage point) is implausible
      - $\rightsquigarrow$ you have a "precise" null finding
      - Precision: how small is your standard error?   
   
- Though you can't prove the null, if your point estimate is zero and it is precise, that can be considered strong evidence against the claim that there is a relationship between X and Y.
   
- Examples of a precise null
  - Effect of the MMR vaccines on autism


## In-class exercise
- Using the face data, test the hypothesis that candidates with higher facial competence score won larger shares of votes 
  - Compute SE, t- and p-values
  - Compute 95% CIs


## In-class exercise
- Using the ANES data, test the hypothesis that Democrats and Republicans differ in their attitude toward vaccination (V162147x)
  - Compute SE, t- and p-values
  - Compute 95% CIs

# CLASS 22

## Housekeeping
- Penn Student Survey
  - Due by 11/20 (short)
  - Ask your friends to take it!
  
  
## Today's Agenda

- Last few classes
  - Regression with one predictor and one outcome
  - Uncertainty
- Today
  - Exercise: run a bunch of regressions
  - Multiple regression: Regression with two or more predictors
  
  

## Libraries and Datasets

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(haven)
library(labelled)
library(emmeans)
anes<-read_dta("anes.dta")
social<-read_csv("social.csv")
climate<-read_csv("climate.csv")
yg<-read_csv("yg.sav")
toxic<-read_csv("toxic.csv") # new
```


## In-class exercise: vaccination
- Using the ANES data, test the hypothesis that Democrats and Republicans (`V161158x`) differ in their attitude toward vaccination `V162147x`
  - Compute SE, t- and p-values
  - Compute 95% CIs

## In-class exercise: turnout
- Using the ANES data, test the hypothesis that age (`V161267x`) is positively associated with turnout (`V162031x`)
  - Compute SE, t- and p-values
  - Compute 95% CIs


## In-class exercise: COVID
- Using the covid data, test the hypothesis that mask-wearing is positively associated with covid cases
  - Compute SE, t- and p-values
  - Compute 95% CIs

- Also test the hypothesis that mask-wearing is positively associated with covid-related deaths
  - Compute SE, t- and p-values
  - Compute 95% CIs

## In-class exercise: Privacy
- Using the `yg` data, test the hypothesis that online privacy concern (`pii_concern_1`) is negatively associated with Facebook use (`fb_freq`)

## In-class exercise: Incivility on social media
- Is incivility contagious on social media? 
```{r}
head(toxic)
```
- Each observation: Facebook post from news outlets
- `feature_toxic`: toxicity of top comments (the comments you normally see when you read an article on FB)
- `toxic`: average toxicity of other comments
   
- Test the hypothesis that the toxicity of top comments on each Facebook post is positively associated with other comments on the same post



# What is Multiple Regression

## Regression with multiple predictors
- We have run regression to predict a Y variable with one X variable
  - What if we want to predict Y as a function of many variables?
  - We run a regression model with multiple predictors $X_{1i}, X_{2i}, X_{3i}, ...$
    - Intercept: Average of Y when $X_{1i}=X_{2i}=X_{3i}=0$
    - Coefficient on $X_1$: Change in the outcome variable when $X_1$ increases by one unit with other predictors ($X_2, X_3, ...$) held constant.
    - Coefficient on $X_2$: Change in the outcome variable when $X_2$ increases by one unit with other predictors ($X_1, X_3, ...$) held constant.
  
     
     ...
       



## Three most common cases
- Why might we do this?
  - Your independent variable is a non-binary categorical variable
    - Race {white, black, Asian, Hispanic, etc}
    - Three or more conditions in an experiment
  - Improve precision
    - More predictors 
    - $\rightsquigarrow$ smaller prediction errors 
    - $\rightsquigarrow$ smaller SE 
    - $\rightsquigarrow$ more precise estimates 
  - Rule out confounding factors
    - $\beta_1$ is the effect of $X_{1}$ holding all other variables constant

# Case 1: Categorical predictor

## Categorical predictor
- We have dealt with either continuous or binary predictors
- What if you have a non-binary categorical variable (e.g., race, 3 or more experimental conditions)
- Strategy for including in categorical a regression: create a series of binary variables (AKA "dummies")

## Recap: social pressure experiment

- Experimental study where each household for 2006 MI primary was randomly assigned to one of 4 conditions:
  - Control: no mailer
  - Civic Duty: mailer saying voting is your civic duty.
  - Hawthorne: a â€œweâ€™re watching youâ€ message.
  - Neighbors: naming-and-shaming social pressure mailer.
- Outcome: whether household members voted or not.


```{r echo=F}
category<-tibble(
  unit = 1:4,
  messages = c("Control","Civic Duty","Hawthorne","Neighbors"),
  Control = c(1,0,0,0),
  Civic = c(0,1,0,0),
  Hawthorne = c(0,0,1,0),
  Neighbors = c(0,0,0,1)
)
category
```

- Then include **all but one** of these categorical variables:


## Reference Category

- The omitted category is called the "reference" category, and it serves as the baseline against which to compare the other categories
- Who should be the reference group?
  - Experimental conditions: control group should be the reference category
  - other cases (e.g., race): depends on which comparison you want to emphasize
    - white vs. other races
    - black vs. other races (depends on your research interests)
    
## Interpreting coefficients on categorical predictors


- $\hat\alpha$: average outcome in the omitted group (control).
- Other coefficients: difference-in-means between that group and the omitted group.
  - $\hat\beta_1$: average difference in outcome between control and civic duty
  - $\hat\beta_2$: average difference in outcome between control and hawthorne
  - $\hat\beta_3$: average difference in outcome between control and neighbors



## Run regression on social pressure experiment
- Syntax: `lm(dv~iv1+iv2+iv3..., data)`
```{r}
# Create 4 dummy variables for each condition
social$control<-ifelse(social$messages=="Control",1,0)
social$civic<-ifelse(social$messages=="Civic Duty",1,0)
social$hawthorne<-ifelse(social$messages=="Hawthorne",1,0)
social$neighbors<-ifelse(social$messages=="Neighbors",1,0)
# run the model
m.social<-lm(primary2006~civic+hawthorne+neighbors,social)
```

## Results
```{r}
summary(m.social)
```

## Results
```{r}
confint(m.social)
```
## Interpreting the results

- (Intercept): average turnout when all independent vars = 0   
  - $\rightsquigarrow$ ~30% turnout rate in the â€œControlâ€ condition
- neighbors: difference in turnout rates between â€œCivic Dutyâ€ condition and
â€œControlâ€ condition.   
  - $\rightsquigarrow$ social pressure mailer leads to 8pp increase in turnout rates.
  - $\rightsquigarrow$ plausible estimates range from 7.6 to 8.6. 
  - $\rightsquigarrow$ Super unlikely observation if the null was correct that the effect of the neighbors treatment was zero.
  - $\rightsquigarrow$ We reject the null, and find support for the alternative hypothesis

## Factor variables in `lm()`

- You actually don't have to create dummies
- Including a character/factor variable in lm() will automatically create binary variables and exclude one group:

```{r}
lm(primary2006 ~ messages, data = social)
```

- Omitted group is â€œCivic Dutyâ€ $\rightsquigarrow$ not ideal!
  - chosen alphabetically 
  - it should be the control

## Changing reference category

- earlier in the semester, we re-wrote the levels of all categories in `factor`
- Perhaps an easier way:
  - syntax: `fct_relevel(data$var, "ref")`

```{r}
social$messages<-fct_relevel(social$messages,"Control")  # changing the reference to control
m.social2<-lm(primary2006~messages,social)
m.social2
```

## Calculating marginal means

- `emmeans` provides the means and uncertainty stats

```{r}
emmeans(m.social2,~messages)
```


## Measure the difference in the effects of two treatments

- What if you wanted to know if the difference between the Hawthorne and Neighbors conditions in their effectiveness?
- Re-shuffle the order to make Hawthorne (or Neighbors) the reference 

```{r}
social$messages <- fct_relevel(social$messages, ref = "Hawthorne")
m.social3<-lm(primary2006~messages,social)
```

## Results
```{r}
summary(m.social3)
```


## In-class exercise: climate change experiment (HW2)

- measure the effects of the two treatments (relative to control) and test the hypothesis that the treatment messages increase people's belief that climate change is due to human activities
```{r}
climate$change <- climate$cause - climate$cause_base 
# calculating belief change score for the outcome variable
```


## In-class exercise 

- Test the hypothesis that the causal treatment is *more* effective than the consensus treatment


# CLASS 23


## Logistics
- Take the Penn student survey and submit your completion code on Canvas by this Friday
- Final Experiment Design
  - We will randomize invasiveness of survey questions
  - Help us come up with those questions (i.e., those people may feel reluctant to answer)
  - Link: https://docs.google.com/document/d/1uivK6PrqFola8fBUJbthw5DELZjTD34urljGSb9FRsQ/edit
  - I'm counting number of contributions ($\rightsquigarrow$ participation score)
- HW5 is out
  - Wait until next week for Q4 and Q5

  
## Today's Agenda

- Multiple regression: Regression with two or more predictors
  - When you have a non-binary categorical variable $\checkmark$
  - When you want to account for confounding variables
  - When you want to boost precision
  

## Libraries and Datasets

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(haven)
library(labelled)
library(emmeans)
anes<-read_dta("anes.dta")
social<-read_csv("social.csv")
climate<-read_csv("climate.csv")
aca<-read_dta("aca.dta") # new
```


# Case 1: Categorical predictor


## Recap: social pressure experiment

- Experimental study where each household for 2006 MI primary was randomly assigned to one of 4 conditions:
  - Control: no mailer
  - Civic Duty: mailer saying voting is your civic duty.
  - Hawthorne: a â€œweâ€™re watching youâ€ message.
  - Neighbors: naming-and-shaming social pressure mailer.
- Outcome: whether household members voted or not.


## Run regression on social pressure experiment
- Syntax: `lm(dv~iv1+iv2+iv3..., data)`
```{r}
# Create 4 dummy variables for each condition
social$control<-ifelse(social$messages=="Control",1,0)
social$civic<-ifelse(social$messages=="Civic Duty",1,0)
social$hawthorne<-ifelse(social$messages=="Hawthorne",1,0)
social$neighbors<-ifelse(social$messages=="Neighbors",1,0)
# run the model
m.social<-lm(primary2006~civic+hawthorne+neighbors,social)
```


## Factor variables in `lm()`

- You actually don't have to create dummies
- Including a character/factor variable in `lm()` will automatically create binary variables and exclude one group:

```{r}
lm(primary2006 ~ messages, data = social)
```

- Omitted group is â€œCivic Dutyâ€ $\rightsquigarrow$ not ideal!
  - chosen alphabetically 
  - it should be the control

## Changing reference category

- earlier in the semester, we re-wrote the levels of all categories in `factor`
- Perhaps an easier way:
  - syntax: `fct_relevel(data$var, "ref")`

```{r}
social$messages<-fct_relevel(social$messages,"Control")  # changing the reference to control
m.social2<-lm(primary2006~messages,social)
m.social2
```

## Calculating marginal means

- `emmeans` provides the means and uncertainty stats

```{r}
emmeans(m.social2,~messages)
```


## Measure the difference in the effects of two treatments

- What if you wanted to know if the difference between the Hawthorne and Neighbors conditions in their effectiveness?
- Re-shuffle the order to make Hawthorne (or Neighbors) the reference 

```{r}
social$messages <- fct_relevel(social$messages, ref = "Hawthorne")
m.social3<-lm(primary2006~messages,social)
```

## Results
```{r}
summary(m.social3)
```


## In-class exercise: climate change experiment (HW2)

- measure the effects of the two treatments (relative to control) and test the hypothesis that the treatment messages increase people's belief that climate change is due to human activities
```{r}
climate$change <- climate$cause - climate$cause_base 
# calculating belief change score for the outcome variable
```




## In-class exercise 

- Test the hypothesis that the causal treatment is *more* effective than the consensus treatment


## In-class exercise: Relationship between race and support for Trump
```{r}
anes$race <- case_when(anes$V161310x==1~"white",
                       anes$V161310x==2~"black",
                       anes$V161310x==3~"asian",
                       anes$V161310x==4~"native",
                       anes$V161310x==5~"hispanic",
                       anes$V161310x==6~"other")
anes$trump<-case_when(anes$V161087>0~anes$V161087)
```
- Test the hypothesis that white Americans are more favorable toward Trump than any other racial group

## In-class exercise: Relationship between race and support for Trump

- Test the hypothesis that black Americans are less favorable toward Trump than any other racial group


# Case 2: Deal with confounders

## Observational (non-experimental) studies

- Example: effect of watching Hannity on climate belief

- Your independent variable wasn't randomized
  - Problem: people watching Hannity and those not watching have many pre-existing differences
  - You can't know your "effect" is really the consequence of watching Hannity or the effects of confounding factors
  - Confounder: a variable that affects both the independent and dependent variables
  - The causal relationship is not identified because of potential confounders
    - $\rightsquigarrow$ causal identification problem
   
- One thing you may try is to "control for" the most obvious confounders 
  - run a multiple regression model include the following as additional predictors
    - Partisanship
    - Favorability toward scientists 

- Among multiple predictors in a model like this
  - One is your "independent variable" of interest (exposure to Hannity)
  - Others are "control variables" that you include to hold constant (e.g., partisanship)

## Running the model
```{r}
# creating key variables; from previous codes
anes$climate<-case_when(anes$V161221>0~ 2-anes$V161221)
anes$hannity<-case_when(anes$V161370>=0 ~ anes$V161370)
anes$scientist<-case_when(anes$V162112>=0&anes$V162112<=100 ~ anes$V162112/100) #0-1 scale
anes$pid<- case_when(anes$V161158x>0 ~ (anes$V161158x-1)/6) #0-1 scale, where 0 = strong dem
# baseline model
m.hannity1<-lm(climate~hannity,anes)
```

## Results
```{r}
summary(m.hannity1)
```

## Adding a covariate
- Let's first add partisanship
```{r}
m.hannity2<-lm(climate~hannity+pid,anes)
summary(m.hannity2)
```

## Interpreting the results

- (Intercept): average belief in climate change when all independent vars = 0   
  - $\rightsquigarrow$ ~95% of Strong Dems not watching Hannity believe in climate change
- `hannity:` mean difference in climate change belief between Hannity viewers vs non-viewers **holding partisanship fixed**
- `pid`: average change in climate change belief when pid changes by one unit (strong dem to strong rep) **holding Hannity viewership fixed**

## Adding another covariate
- Let's first add partisanship and scientist favorability
```{r}
m.hannity3<-lm(climate~hannity+pid+scientist,anes)
summary(m.hannity3)
```

## Controls don't establish causality

- Controls can be useful when you want to rule out certain alternative explanations
- But you don't have a causal effect unless you have controlled for all confounders
- Hannity example
  - after controlling for two confounders the "effect" decreased from 28 pts to 18 pts
  - $\rightsquigarrow$ how much will it go down as we keep adding controls?
  - $\rightsquigarrow$ will it go down to zero if we added all confounders?
  - We don't know!
- Robustness check: is your finding sensitive to covariates? If yes, your finding isn't robust!

## in-class exercise  (anes)
- Re-examine the relationship watching Hannity and belief in climate change
  - model 1: control for favorability toward trump in addition to partisanship and scientist favorability

# Case 3: Improve Precision

## Multiple regression can be useful for experimental data too

- Two main purposes of using multiple regressions (other than having a categorical variable)
  - Deal with confounders: irrelevant for experiments because randomization takes care of confounders
  - Boost precision: very relevant

- Choosing a covariate when analyzing experimental data
  - Don't throw in everything
  - Two criteria
    - Strong predictor of the outcome
      - $\rightsquigarrow$ substantially reduces chance errors
    - Pre-treatment
      - DO NOT include a covariate that was measured **after** the treatment, and it's something that can be reasonably affected by the treatment.

## Best strategy: lagged dependent variable
- Measure your outcome variable twice (before and after the treatment)
- Control for the value of your outcome measured before the treatment (i.e., lagged DV)
- Often substantially improves precision (could be as effective as doubling the sample size)

## Partisan learning experiment
- Do people revise their opinions about partisan issues after learning new information?   
   
    
- Survey experiment conducted in 2016
  - People filled out a baseline survey where the baseline outcomes was measured
  - People were invited to a follow-up where they were randomly assigned to one of five conditions
    - control
    - strong con argument: why ACA makes health care more expensive (+ strong evidence)
    - weak con argument: why ACA makes health care more expensive (+ weak evidence)
    - weak pro argument: why ACA makes health care cheaper (+ weak evidence)
    - strong pro argument: why ACA makes health care cheaper (+ strong evidence)
  - Two outcomes
    - Belief about the ACA's effect on cost (0-1, where 1 = definitely cheaper)
    - Attitude toward the ACA (0-1, where  1 = strong support)
```{r}
names(aca)
head(aca$cond)
```


## Fitting regressions with and without covariates
```{r}
m.aca1<-lm(cost~cond,aca)
m.aca1 # R treated cond as a continous variable!
m.aca1<-lm(cost~factor(cond),aca) # use factor() function to tell R that this is a factor variable
m.aca2<-lm(cost~factor(cond)+w1cost+w1aca,aca) # adding lagged outcomes to the model
```

## Results without the covariates
```{r}
summary(m.aca1)
```

## Results with the covariates
```{r}
summary(m.aca2)
```

## Differences across models
- Standard errors on the key coefficients decrease from ~0.0195 to ~0.0135
  - 70% of the original SEs $\approx$ gain in precision when your sample sizes doubles (more $)
- Point estimates are similar between the models
  - Expected when working with experimental data 
  - If your estimates jump up and down, something probably went wrong
  - $\rightsquigarrow$ Failed the robustness check!



## in-class exercise 3 (aca)
- Examine the treatment effects on attitude toward the ACA
- Run two regression models:
  - model 1: don't add covariates
  - model 2: control for wave 1 outcomes



# CLASS 24


## Logistics
- HW5 is due in a week
  - I'm available for office hours this week
  - Penn student survey data is available on Canvas (thank you!)
- Final project
  - prep in progress (full design will be shared shortly)
  - will be conducted on 11/30 (Mon) and the data will be uploaded on 12/1
  - optional: write a memo for feedback
  - optional: sign up for office hours
  - paper deadline: 12/20

  
## Today's Agenda

- Using a change score to test a hypothesis
- Multiple regression: Regression with two or more predictors
  - When you want to account for confounding variables
- Heterogeneous effects
  - You should know it to do Question 4 of Homework 5
  - We may go over 4:15 (feel free to leave and catch up later)

  
  
## Libraries and Datasets

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(haven)
library(labelled)
library(emmeans)
library(margins) # new
anes<-read_dta("anes.dta")
anes2008<-read_dta("anes2008.dta")
aca<-read_dta("aca.dta")
mmr<-read.csv("mmr.csv") 
```

## Change score
- Has Y changed since X?
  - Several groups had this kind of research question for the Penn student survey
  - You can test the hypothesis Y changed overtime by calculating its change score and running the following regression model
  - Syntax: `lm(change~1,data)`
    - will give you the mean of `change` and its t- and p-values
    - Null hypothesis: Y did not change overtime $\rightsquigarrow$ `change` = 0
  
## Change score
- Example: Did people's misperception about Obama's religion change between September and November 2008?
```{r}
anes2008$w9muslim<-case_when(anes2008$w9v3==1~0,anes2008$w9v3==3~1) # belief in Sep
anes2008$w11muslim<-case_when(anes2008$w11wv3==1~0,anes2008$w11wv3==3~1) # belief in Nov
anes2008$change<-anes2008$w11muslim-anes2008$w9muslim
m1<-lm(change~1,anes2008)
summary(m1)
```
- We can't reject the null hypothesis that people's misperception remained unchanged 
 
## in-class exercise (last class)
- Examine the treatment effects on attitude toward the ACA
- Run two regression models:
  - model 1: don't add covariates
  - model 2: control for wave 1 outcomes

 
# Multiple regression to account for confounders

## Observational (non-experimental) studies

- Example: effect of watching Hannity on climate belief

- Your independent variable wasn't randomized
  - Problem: people watching Hannity and those not watching have many pre-existing differences
  - You can't know your "effect" is really the consequence of watching Hannity or the effects of confounding factors
  - Confounder: a variable that affects both the independent and dependent variables
  - The causal relationship is not identified because of potential confounders
    - $\rightsquigarrow$ causal identification problem
   
- One thing you may try is to "control for" the most obvious confounders 
  - run a multiple regression model include the following as additional predictors
    - Partisanship
    - Favorability toward scientists 

- Among multiple predictors in a model like this
  - One is your "independent variable" of interest (exposure to Hannity)
  - Others are "control variables" that you include to hold constant (e.g., partisanship)

## Running the model
```{r}
# creating key variables; from previous codes
anes$climate<-case_when(anes$V161221>0~ 2-anes$V161221)
anes$hannity<-case_when(anes$V161370>=0 ~ anes$V161370)
anes$scientist<-case_when(anes$V162112>=0&anes$V162112<=100 ~ anes$V162112/100) #0-1 scale
anes$pid<- case_when(anes$V161158x>0 ~ (anes$V161158x-1)/6) #0-1 scale, where 0 = strong dem
# baseline model
m.hannity1<-lm(climate~hannity,anes)
```

## Results
```{r}
summary(m.hannity1)
```

## Adding a covariate
- Let's first add partisanship
```{r}
m.hannity2<-lm(climate~hannity+pid,anes)
summary(m.hannity2)
```

## Interpreting the results

- (Intercept): average belief in climate change when all independent vars = 0   
  - $\rightsquigarrow$ ~95% of Strong Dems not watching Hannity believe in climate change
- `hannity:` mean difference in climate change belief between Hannity viewers vs non-viewers **holding partisanship fixed**
- `pid`: average change in climate change belief when pid changes by one unit (strong dem to strong rep) **holding Hannity viewership fixed**

## Inclass exericse
- Add partisanship and scientist favorability

## Controls don't establish causality

- Controls can be useful when you want to rule out certain alternative explanations
- But you don't have a causal effect unless you have controlled for all confounders
- Hannity example
  - after controlling for two confounders the "effect" decreased from 28 pts to 18 pts
  - $\rightsquigarrow$ how much will it go down as we keep adding controls?
  - $\rightsquigarrow$ will it go down to zero if we added all confounders?
  - We don't know!
- Robustness check: is your finding sensitive to covariates? If yes, your finding isn't robust!




# Heterogeneous effects


## What are heterogeneous effects?

- Heterogeneous treatment effects: the treatment effect varies across groups. 
  - Covid19 vaccine is more effective for young people than for old people 
  - Effect of some new information is positive for Reps and negative for Dems

- When your treatment effects differ by another factor, that factor is called a "moderating variable" or "moderator"

- The effect of such a variable is called a "moderating effect" or "interaction effect"
  - "We find that the effect of X is moderated by Z..."
  - "There was a statistically significant interaction between X and Z..."

- Two approaches
  - Subsetting
  - Interaction terms in a regression
  
  
## ACA experiment


- Telling people Obamacare makes healthcare more expensive decreased support on average
- Did it work only among Reps, or Dems, or both?
- PID is a moderator

```{r}
# for simplicity, we're going to focus on the strong pro vs control conditions
aca<-subset(aca,cond==0|cond==1)
# fitting three regression models
ate.a<-lm(aca~cond,aca)
ate.r<-lm(aca~cond,subset(aca,dem==0))
ate.d<-lm(aca~cond,subset(aca,dem==1))
```


## Results
```{r}
summary(ate.a)
```

## Results
```{r}
summary(ate.d)
```

## Results
```{r}
summary(ate.r)
```

## Difference in effects
- How much does the estimated effect differ between groups?
```{r}
coef(ate.d)[2]-coef(ate.r)[2]
# Effect was strong for Dems than Reps by 1.4 ppts
```
- Any easier way to do this?
- Can we quantify the level of uncertainty of this difference?

## Interaction terms
- You can allow for different slopes/coefficients/effects of a variable by including an interaction term:
  - Interaction term: multiplication of your independent and moderating variables
    - the `cond` variable multiplied by the `dem` variable
    - Equals 1 if received the treatment (`cond==1`) and identified as Democratic (`dem==1`)
    - Equals 0 otherwise 


## Interactions in R
```{r}
# Approach 1
aca$condxdem<-aca$cond*aca$dem # we're literally multiplying the variables
m.aca3<-lm(aca~cond+dem+condxdem,aca)
# Approach 2 (better)
m.aca4<-lm(aca~cond*dem,aca) # easier way that does the same thing as m.aca3 
```

## Results
```{r}
summary(m.aca4)
```


## Interpreting coefficients

- The model

- $support_i = \alpha+\beta_1\times cond_i+ \beta_2 \times Dem_i + \beta_3 \times (cond_i \times Dem_i)+\epsilon_i$

- $\hat\alpha=0.26$ (`Intercept`): average support among Republicans in control group
- $\hat\beta_1=0.08$ (`cond`): treatment effect when `dem == 0` (i.e., among Republicans)
  - $\rightsquigarrow$ p < 0.05 (significant)
- $\hat\beta_2=0.48$ (`dem)`: difference in support between Republicans and Democrats in control group (we don't really care)
- $\hat\beta_3=-0.01$ (`cond:dem`): difference in treatment effects between Republicans and Democrats 
  - $\rightsquigarrow$ the effect of anti-ACA information was more negative for Democrats
  - $\rightsquigarrow$ but p > 0.05 (not significant)
  - $\rightsquigarrow$ we don't find evidence that treatment had different effects on Dem vs Rep 


## Marginal means

- Recap: mean of Y at fixed values of X (i.e., group means) are called marginal means
- To obtain marginal means, use the `emmeans()` function 
```{r}
emmeans(m.aca4,~cond|dem) # emmeans(model, ~ IV|Moderator)
```

## Marginal means
- In fact we've been doing this many times in this class
  - We just didn't call it a heterogeneous effect (so you already know this concept!)
```{r warning=F}
aca$group[aca$dem==0&aca$cond==0]<-1
aca$group[aca$dem==0&aca$cond==1]<-2
aca$group[aca$dem==1&aca$cond==0]<-3
aca$group[aca$dem==1&aca$cond==1]<-4
tapply(aca$aca,aca$group,mean,na.rm=T)
```

## In-class exercise: MMR Vaccine (Recap)

- Question: How to reduce vaccine misperceptions and increase vaccination rates?
- A nationally representative experiment with about 1700 parents conducted in 2011
- Parents were randomly assigned to receive 1 of 4 treatment messages or a "placebo" message
  1. Placebo: costs and benefits of bird feeding (Unrelated)
  2. Correction: No evidence that vaccination causes autism
  3. Disease Image: Pictures of a child who has each of the diseases 
  4. Disease Narrative: A narrative of an infant boy hospitalized because of measles
  5. Disease Risk: Explain the dangers of the diseases prevented by MMR vaccine

## In-class exercise: MMR Vaccine (Recap)

- cond: experimental conditions
- prior: attitude toward vaccination measured **BEFORE** treatment
- autism: 1 = MMR vaccine doesn't cause autism (0 = else)
- intent: 1 = Very likely to give MMR vaccine to another child (0 = else)

```{r}
mmr$unfav[mmr$prior=="unfavorable"]<-1 # for simplicity let's turn prior into a binary var
mmr$unfav[mmr$prior=="favorable"|mmr$prior=="neutral"]<-0
```


## In-class exercise: MMR Vaccine (Recap)
- Q1. Estimate the average treatment effects

## In-class exercise: MMR Vaccine (Recap)
- Q2. Estimate whether the treatment effect varies by unfav
- Q3. Who drove the effect observed in Q1?

## In-class exercise: MMR Vaccine (Recap)
- Q4. Estimate the marginal means and their uncertainty stats


# Class 25 - Dec 01 2020 - 12-01-2020

## Housekeeping

- Final paper guideline is up
- Online privacy experiment will launch tomorrow
  - Data will be posted before Thursday's class
  - Survey instrument on Canvas (`privacy.docx`)

## Today's Agenda

- Online privacy experiment overview
- Final paper Q&A
- Creating regression tables

## Libraries and Datasets

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(haven)
library(labelled)
library(stargazer) # creates regression tables, new
aca<-read_dta("aca.dta")
social<-read_csv("social.csv")
```

# Climate Change Experiment

## Let's take it!

- https://upenn.co1.qualtrics.com/jfe/form/SV_6tx8KSPbTMCPw5T
- Survey is identical except the future research part

## Study overview 

- Participants
  - Workers at Amazon's Mechanical Turk
  - 1,500 participants

- Survey experiment with 2 X 2 X 2 (8) conditions (Independent variable)
  - Whether asked to to provide social media accounts in the new study
  - Whether asked about personal life in the new study
  - Wage (\$0.5 vs \$25)

- Outcome: only two of them 
  - Willingness to participate in the "future" study (yes or no)
  - Desired wage ($)

## Procedure

1. Consent
2. Demographics (gender, age, race, income, education)
3. Privacy concerns
4. Social media
5. Political questions
6. Treatment + outcome


## Potential research questions and key comparisons

- Effect of one of the three treatments:
  - Not asking for social media accounts (baseline) vs Asking for social media accounts
  - Not asking about personal life  (baseline) vs Asking about personal life
  - Offered 0.5  (baseline) vs Offered 25  

- Interaction among the treatments (heterogeneous effect):
  - e.g., Effect of asking for social media accounts: larger when offered 0.5 than when offered 25
    - People will be less willing to take part in a study that asks for social media info **but that depends on** how much money is offered
    - moderating variable: monetary incentive

- Interaction between a treatment and a non-random covariate (heterogeneous effect)
  - e.g., Effect of asking for social media accounts: larger among highly educated people
    - People will be less willing to take part in a study that asks for social media info **but that depends on** education level
    - moderating variable: education


# Final Paper Q&A

# Regression Tables

## After you run models, you report the findings in a table

- Typically, people report several models in one table
- Related models that differ in terms of...
  - Outcome variables
  - With or without controls
  - With or without interaction terms
- Manually transcribing your R outputs to a standard regression table format will be extremely tedious
- the `stargazer` library makes this much easier
- Syntax: `stargazer(model)`
  - options
    - `type`: "text", "html", or "latex" $\rightsquigarrow$ output type
    - `covariate.labels = c ("iv1", "iv2", "iv3)` $\rightsquigarrow$ names of IVs
    - `dep.var.labels = c("dv1", "dv2)` $\rightsquigarrow$ names of DVs
    - `keep.stat=("n")` $\rightsquigarrow$ drop every summary stat except "n"
    - `style=("ajps)` $\rightsquigarrow$ format the table to the style of the American Journal of Political Science
    
## Example (ACA)

```{r}
# for simplicity, we're going to drop three treatment conditions
aca<-subset(aca,cond==0|cond==1)
# 8 models
m1<-lm(aca~cond,aca) # DV = aca
m2<-lm(aca~cond+w1aca+w1cost,aca) # controlling for baseline covariates
m3<-lm(aca~cond*dem,aca) # interaction term between condition and Dem dummy
m4<-lm(aca~cond*dem+w1aca+w1cost,aca) # controls + interaction term
m5<-lm(cost~cond,aca)
m6<-lm(cost~cond+w1aca+w1cost,aca)
m7<-lm(cost~cond*dem,aca)
m8<-lm(cost~cond*dem+w1aca+w1cost,aca)
```

## Reporting the 8 models with stargazer
- default will produce latex code 
```{r}
stargazer(m1,m2,m3,m4,m5,m6,m7,m8)
```

## Reporting the 8 models with stargazer
- style = "text" argument will produce the table in plain text
```{r}
stargazer(m1,m2,m3,m4,m5,m6,m7,m8, type = "text")
```

## Reporting the 8 models with stargazer
- style = "html" argument will produce the table in html code
```{r}
stargazer(m1,m2,m3,m4,m5,m6,m7,m8, type = "html")
```


## Reporting the 8 models with stargazer
- `out = "table.html"` argument will produce a html file named "table.html" in your WD
- You can copy and paste the table in word doc (for final paper)
```{r}
stargazer(m1,m2,m3,m4,m5,m6,m7,m8, out = "table.html")
```
- But let's use the "text" style for now...

## Changing the significance level cutoffs
- Little stars indicate whether a coefficient is statistically significant
- Default cutoffs are .1, .05, .01
- More widely accepted cutoffs are .05,
  - p < .1 is sometimes called "marginally significant" 
- Use the `star.cutoffs` argument
```{r}
stargazer(m1,m2,m3,m4,m5,m6,m7,m8,type = "text",
          star.cutoffs = c(.05, .01, .001))
```

## Changing the names of variables
- `covariate.labels` option for IVS
- `dep.var.labels` option for IVS

```{r}
stargazer(m1,m2,m3,m4,m5,m6,m7,m8,type = "text",
          star.cutoffs = c(.05, .01, .001),
          covariate.labels = c("Con Info", "Baseline Attitude", "Baseline Belief", "Democrat", "Con Info X Democrat"),
          dep.var.labels = c("Attitude toward the ACA", "Belief that ACA saves costs"))
```

## Dropping Summary stats
- R2, Adjusted R2, Residual Std. Error, F Statistic
  - Measures of how well the model fit the data 
    - e.g., R-square: the amount of the variance of the DV, explained by the IVs
  - Not as important as the coefficients (why we skipped them)
  - People often omit them
- `keep.stat="n"` will drop everything but `n`, # of observations (which you should report)
```{r}
stargazer(m1,m2,m3,m4,m5,m6,m7,m8,type = "text",
          star.cutoffs = c(.05, .01, .001),
          covariate.labels = c("Con Info", "Baseline Attitude", "Baseline Belief", "Democrat", "Con Info X Democrat"),
          dep.var.labels = c("Attitude toward the ACA", "Belief that ACA saves costs"),
          keep.stat="n")
```

## Style of a particular journal
- You can match the format of your table to the formatting of top journals in social sciences
- I am using the formatting of the American Journal of Political Science
```{r}
# Use this as your template in your final paper unless you have a good reason!
stargazer(m1,m2,m3,m4,m5,m6,m7,m8,type = "text",
          star.cutoffs = c(.05, .01, .001),
          covariate.labels = c("Con Info", "Baseline Attitude", "Baseline Belief", "Democrat", "Con Info X Democrat"),
          dep.var.labels = c("Attitude toward the ACA", "Belief that ACA saves costs"),
          keep.stat="n",
          style = "ajps")
```

## In-class exercise
- Run the following models using the social pressure experiment data
  - m1: regress 2006 turnout on messages
  - m2: add age (2006-yearofbirth) as a control to m1
  - m3: add the interaction between messages and 2004 turnout to m1
  - m4: add age (2006-yearofbirth) as a control to m3

## In-class exercise
- Create a table that reports the results of m1 to m4, using stargazer

# Class 26 12 - 03 -2020

## Housekeeping

- Final project data is now up
- Final paper update memo due in 1 week (optional)

## Today's Agenda

- Final paper & data
- Heterogeneous effects
- Visualizing linear/nonlinear trends

## Libraries and Datasets

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(haven)
library(labelled)
library(emmeans) 
library(margins) # new
aca<-read_dta("aca.dta")
gallup<-read_csv("gallup.csv")
social<-read_csv("social.csv")
```


## Final Paper Structure
- Short research paper based on the experiment (2,500 words, including everything)
  - Title
  - Abstract (150 words)
  - Introduction (1-2 paragraphs)
    - Introduce the research topic
    - Present research question
    - Briefly mention your research approach
    - Optional: Explain why it matters
  - Hypotheses
    - Present **4** hypotheses
    - Optional: explain 
  - Methods section
    - Sample (1-2 sentence)
    - Experimental design and procedure (1-2 paragraphs)
    - Measurement of key variables (1-2 paragraphs)


## Final Paper Structure (continued)
  - Results (4 paragraphs)
    - Findings on the 4 hypotheses
    - 2 Tables and 2 Figures
  - Conclusion (2 paragraphs)
    - Brief summary of the findings
    - Optional: Limitation
  - Reference (if any)

## Some key variables
- treatments
  - `account`: respondents will be asked to install a software and provide social media accounts
  - `personal`: respondents will be asked about personal lives
  - `money`: respondents will be paid 0.5 vs 25
- outcomes
  - `q33`: yes or no
  - `q34`: wage
- other variables
  - demographics
  - privacy concerns
  - social media use

## Treatment (bolded parts are randomized)

We are currently preparing new research on social media use patterns. Participants in this future study will be asked more detailed information about `their personal lives such as their relationships, physical and mental health`, in addition to similar questions as the current survey. `They will be also asked to install a software in their computers and provide their social media accounts. The software will merge the data from the surveys and information from social media accounts to send customized messages and recommendations in their social media feeds.` Participants in the new study will be paid `$25.`


## outcome

- Would you want us to use your MTurk ID to send you an invitation to this new study?
- If you were to take part in the study, what do you think would be a fair wage? (0.25 to 100)

## Test the hypothesis 
- On average, participants will be less willing to take part in a study when the new study asks for social media account information.

## Recap
- Heterogeneous treatment effects: when a treatment has different effects across groups
- Two approaches
  - You can subset the data and calculate separate treatment effects
  - You can add an interaction term

## Subsetting

```{r}
aca<-subset(aca,cond==0|cond==1)
m.aca2<-lm(aca~cond,subset(aca,dem==0))
m.aca3<-lm(aca~cond,subset(aca,dem==1))
```

## Effects among Republicans

```{r}
summary(m.aca2)
```

## Effects among Democrats
```{r}
summary(m.aca3)
```


## Marginal Effects
- Average effect of treatment when dem = 0 
- Average effect of treatment when dem = 1 
  - $\rightsquigarrow$ these are called "marginal effects" (i.e., effects at the margins)


## Interaction term

```{r}
aca<-subset(aca,cond==0|cond==1)
m.aca4<-lm(aca~cond*dem,aca) #IV*MOD
summary(m.aca4)
```
- Interaction term: difference between marginal effects


## In-class Exercise: privacy experiment


- Test the hypothesis that the effect of asking for social media account information will be smaller when people are offered a higher wage.
  - `money` (0 = offered 0.5; 1 = offered 25)

## Continuous moderator
- Can your moderator be a continuous variable?
  - Yes
- Gallup: examine how gender gap varies by age

```{r}
names(gallup)
table(gallup$age)
```

## Continuous moderator
- We don't want to subset by all those groups!
- But we can still use the interaction term approach

```{r}
m.gallup<-lm(goodlooking~female*age,gallup)
```

## Results

```{r}
summary(m.gallup)
```

## Interpreting the results

- Intercept ($\hat\alpha=0.05$): average satisfaction among for *0-year-old boys*
  - Not the real mean obviously, but a projected mean based on the estimated trends
- female ($\hat\beta_1=-0.39$): average gender gap in satisfaction for 0-year-olds
- age ($\hat\beta_2=0.01$): slope of regression line for age among men
- female:age ($\hat\beta_3=0.003$): change in the gender gap for a 1-year increase in age 
  - Gender gap for $k$ year-olds: $\hat\beta_2+\hat\beta_3 \times k = -0.39 +0.003\times k$
  - Gender gap for 20-year-olds: $-0.39 +0.003\times 20 = -.33$
  - Quiz: What is the predicted gender gap for 80-year-olds?

## Interpreting the results

- Intercept ($\hat\alpha=0.05$): average satisfaction among for *0-year-old boys*
  - Not the real mean obviously, but a projected mean based on the estimated trends
- female ($\hat\beta_1=-0.39$): average gender gap in satisfaction for 0-year-olds
- age ($\hat\beta_2=0.01$): slope of regression line for age among men
- cond:pid7 ($\hat\beta_3=0.003$): change in the gender gap for a 1-year increase in age 
  - Gender gap for $k$ year-olds: $\hat\beta_2+\hat\beta_3 \times k = -0.39 +0.003\times k$
  - Gender gap for 20-year-olds: $-0.39 +0.003\times 20 = -.33$
  - Gender gap for 80-year-olds: $-0.39 +0.003\times 80 = -.15$
  


## plotting the trends
- Moderating effects of a continuous variable are often easier to understand with trend plots
```{r warning=F, message=F, fig.show='hide'}
gallup$gender<-ifelse(gallup$female==0,"male", "female")
ggplot(gallup, aes(x=age, y=goodlooking, color = gender, fill = gender))+ 
# fill changes color of CIs
  geom_smooth(method = "lm")+
  theme_bw()
```

## plotting the trends
```{r warning=F, message=F, echo=F}
gallup$gender<-ifelse(gallup$female==0,"male", "female")
ggplot(gallup, aes(x=age, y=goodlooking, color = gender, fill = gender))+ 
# fill changes color of CIs
  geom_smooth(method = "lm")+
  theme_bw()
```


## plotting the trends
- Changing reference category to male
- Getting rid of NAs using subset() and is.na()
```{r warning=F, message=F, fig.show='hide'}
gallup$gender<-fct_relevel(gallup$gender,"male")
ggplot(subset(gallup,!is.na(female)), aes(x=age, y=goodlooking, color = gender, fill = gender))+
# fill changes color of CIs
  geom_smooth(method = "lm")+
  theme_bw()
```

## plotting the trends

```{r warning=F, message=F, echo=F}
gallup$gender<-fct_relevel(gallup$gender,"male")
ggplot(subset(gallup,!is.na(female)), aes(x=age, y=goodlooking, color = gender, fill = gender))+
# fill changes color of CIs
  geom_smooth(method = "lm")+
  theme_bw()
```


## plotting the trends
- We have been forcing the trends to be linear (i.e., slope doesn't change across x values)
  - But nothing changes in exactly linear trends
- dropping the 'method="lm"' argument will give you non-linear trends
```{r warning=F, message=F, fig.show='hide'}
ggplot(subset(gallup,!is.na(female)), aes(x=age, y=goodlooking, color = gender, fill = gender))+
  geom_smooth()+
  theme_bw()
```


## plotting the trends
```{r warning=F, message=F, echo=F}
ggplot(subset(gallup,!is.na(female)), aes(x=age, y=goodlooking, color = gender, fill = gender))+
  geom_smooth()+
  theme_bw()
```



## plotting the trends
- Let's add linear lines
```{r warning=F, message=F, fig.show='hide'}
ggplot(subset(gallup,!is.na(female)), aes(x=age, y=goodlooking, color = gender, fill = gender))+
# fill changes color of CIs
  geom_smooth()+geom_smooth(method="lm",linetype="dashed", se=FALSE)+
  theme_bw()
```

## plotting the trends
```{r warning=F, message=F, echo=F}
ggplot(subset(gallup,!is.na(female)), aes(x=age, y=goodlooking, color = gender, fill = gender))+
# fill changes color of CIs
  geom_smooth()+geom_smooth(method="lm",linetype="dashed", se=FALSE)+
  theme_bw()
```

## plotting the trends
- Let's change the axis labels
- Use the labs() argument
```{r warning=F, message=F, fig.show='hide'}
ggplot(subset(gallup,!is.na(female)), aes(x=age, y=goodlooking, color = gender, fill = gender))+
# fill changes color of CIs
  geom_smooth()+
  labs(x = "Age", y = "Satisfaction with Appearance")+
  theme_bw()
```


## In-class exercise
- ACA example: examine heterogeneous treatment effects on attitude by baseline attitude
- Calculate the marginal effects at the following baseline values
  :0, 0.25, .5, .75, 1




